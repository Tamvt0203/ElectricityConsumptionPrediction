{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', index_col='date')\n",
    "df_test = pd.read_csv('test.csv',index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chia tập validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validate = df_train.iloc[-100:, :]\n",
    "df_train = df_train.iloc[:-100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>new_total_usage</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>True</td>\n",
       "      <td>21.89</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02</th>\n",
       "      <td>False</td>\n",
       "      <td>21.96</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>True</td>\n",
       "      <td>21.99</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>False</td>\n",
       "      <td>22.57</td>\n",
       "      <td>4037.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>False</td>\n",
       "      <td>22.97</td>\n",
       "      <td>4191.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  new_total_usage  weekday_0  weekday_1  \\\n",
       "date                                                                         \n",
       "2022-01-01        True        21.89           1496.0      False      False   \n",
       "2022-01-02       False        21.96           1177.0      False      False   \n",
       "2022-01-03        True        21.99           1463.0       True      False   \n",
       "2022-01-04       False        22.57           4037.0      False       True   \n",
       "2022-01-05       False        22.97           4191.0      False      False   \n",
       "\n",
       "            weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "date                                                               \n",
       "2022-01-01      False      False      False       True      False  \n",
       "2022-01-02      False      False      False      False       True  \n",
       "2022-01-03      False      False      False      False      False  \n",
       "2022-01-04      False      False      False      False      False  \n",
       "2022-01-05       True      False      False      False      False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chuẩn bị dữ liệu để train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuyển kiểu dữ liệu True/False thành 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_int(df, weekday_cols_prefix='weekday_', holiday_col='is_holiday', num_weekdays=6):\n",
    "    \"\"\"\n",
    "    Chuyển đổi các cột 'weekday' và 'is_holiday' của DataFrame sang kiểu dữ liệu int.\n",
    "\n",
    "    Tham số:\n",
    "    - df: DataFrame cần chuyển đổi.\n",
    "    - weekday_cols_prefix: Tiền tố của các cột 'weekday'. Mặc định là 'weekday_'.\n",
    "    - holiday_col: Tên cột ngày lễ cần chuyển đổi. Mặc định là 'is_holiday'.\n",
    "    - num_weekdays: Số lượng cột 'weekday' cần chuyển đổi (mặc định là 6).\n",
    "    \n",
    "    Trả về:\n",
    "    - DataFrame với các cột đã được chuyển sang kiểu int.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chuyển đổi các cột weekday_{i} thành int\n",
    "    for i in range(0, num_weekdays + 1):  # Lặp từ 1 tới num_weekdays\n",
    "        df[f'{weekday_cols_prefix}{i}'] = df[f'{weekday_cols_prefix}{i}'].astype(int)\n",
    "    \n",
    "    # Chuyển đổi cột is_holiday thành int\n",
    "    df[holiday_col] = df[holiday_col].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_columns_to_int(df=df_train)\n",
    "df_validate = convert_columns_to_int(df=df_validate)\n",
    "df_test = convert_columns_to_int(df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale dữ liệu số về 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['temperature', 'new_total_usage']\n",
    "scaler = MinMaxScaler()\n",
    "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
    "df_validate[numerical_cols] = scaler.transform(df_validate[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>new_total_usage</th>\n",
       "      <th>weekday_0</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>0.346911</td>\n",
       "      <td>0.100282</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>0</td>\n",
       "      <td>0.332654</td>\n",
       "      <td>0.313559</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300747</td>\n",
       "      <td>0.498588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>0</td>\n",
       "      <td>0.272234</td>\n",
       "      <td>0.497175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0.316361</td>\n",
       "      <td>0.498588</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  new_total_usage  weekday_0  weekday_1  \\\n",
       "date                                                                         \n",
       "2024-01-01           1     0.346911         0.100282          1          0   \n",
       "2024-01-02           0     0.332654         0.313559          0          1   \n",
       "2024-01-03           0     0.300747         0.498588          0          0   \n",
       "2024-01-04           0     0.272234         0.497175          0          0   \n",
       "2024-01-05           0     0.316361         0.498588          0          0   \n",
       "\n",
       "            weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "date                                                               \n",
       "2024-01-01          0          0          0          0          0  \n",
       "2024-01-02          0          0          0          0          0  \n",
       "2024-01-03          1          0          0          0          0  \n",
       "2024-01-04          0          1          0          0          0  \n",
       "2024-01-05          0          0          1          0          0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_past = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo tập X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích: mỗi dòng dữ liệu ở tập X sẽ bao gồm n_past dòng dữ liệu ở quá khứ (bao gồm cả 8 features và 1 target value), tập y sẽ là target value ở dòng dữ liệu n_past+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            dataX.append(dataset.iloc[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset.iloc[i,2])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX,trainY=createXY(df_train,n_past=n_past)\n",
    "testX,testY=createXY(df_test,n_past=n_past)\n",
    "validX, validY = createXY(df_validate, n_past=n_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "630"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 30, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 30, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.25458248, 0.16384181, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         1.000000\n",
       "temperature        0.254582\n",
       "new_total_usage    0.163842\n",
       "weekday_0          0.000000\n",
       "weekday_1          0.000000\n",
       "weekday_2          0.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          1.000000\n",
       "weekday_6          0.000000\n",
       "Name: 2022-01-01, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.2742702 , 0.12711864, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         0.000000\n",
       "temperature        0.274270\n",
       "new_total_usage    0.127119\n",
       "weekday_0          0.000000\n",
       "weekday_1          0.000000\n",
       "weekday_2          0.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          0.000000\n",
       "weekday_6          1.000000\n",
       "Name: 2022-01-30, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[29, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12853107344630846"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         1.000000\n",
       "temperature        0.213170\n",
       "new_total_usage    0.128531\n",
       "weekday_0          1.000000\n",
       "weekday_1          0.000000\n",
       "weekday_2          0.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          0.000000\n",
       "weekday_6          0.000000\n",
       "Name: 2022-01-31, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[30, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the build_model function\n",
    "def build_model(optimizer='adam'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(50, return_sequences=True, input_shape=(n_past, n_cols)))\n",
    "    grid_model.add(LSTM(50))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1))\n",
    "    grid_model.compile(loss='mse', optimizer=optimizer)\n",
    "    return grid_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_prediction(y_pred, validY, scaler, numerical_cols, title='Predict Electricity Consumption'):\n",
    "    \"\"\"\n",
    "    Hàm dự đoán và vẽ biểu đồ cho dữ liệu giá cổ phiếu dựa trên mô hình và scaler.\n",
    "\n",
    "    Tham số:\n",
    "    - y_pred: Dự đoán của mô hình, đầu vào cần được reshape\n",
    "    - validY: Giá trị thực của dữ liệu kiểm tra, đầu vào cần được reshape\n",
    "    - scaler: Bộ scaler đã được dùng để chuẩn hóa dữ liệu\n",
    "    - numerical_cols: Số lượng cột dữ liệu (các thuộc tính số cần dự đoán)\n",
    "    - title: Tiêu đề của biểu đồ (tùy chọn)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape y_pred và validY để chuẩn bị cho việc đảo ngược chuẩn hóa\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    validY = validY.reshape(-1, 1)\n",
    "    \n",
    "    # Tạo các bản sao của y_pred và validY với số cột bằng số lượng thuộc tính số\n",
    "    y_pred_copies_array = np.repeat(y_pred, len(numerical_cols), axis=-1)\n",
    "    y_copies_array = np.repeat(validY, len(numerical_cols), axis=-1)\n",
    "    \n",
    "    # Inverse transform để đưa dữ liệu về giá trị ban đầu\n",
    "    y_pred_before_scale = scaler.inverse_transform(y_pred_copies_array)[:, -1]\n",
    "    y_before_scale = scaler.inverse_transform(y_copies_array)[:, -1]\n",
    "    \n",
    "    # Vẽ biểu đồ\n",
    "    plt.plot(y_before_scale, color='red', label='Real Electricity Consumption')\n",
    "    plt.plot(y_pred_before_scale, color='blue', label='Predicted Electricity Consumption')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Electricity Consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(optimizer='Adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved_models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the checkpoint callback to save only 5 most recent models\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(save_dir, 'model_epoch_{epoch:02d}_loss_{loss:.4f}.keras'), \n",
    "    monitor='loss',\n",
    "    save_best_only=False,  # Save the model every epoch\n",
    "    mode='auto',\n",
    "    verbose=1,\n",
    "    save_weights_only=False  # Save the entire model (architecture + weights)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(directory):\n",
    "    model_files = sorted(\n",
    "        [f for f in os.listdir(directory) if f.endswith('.keras')],\n",
    "        key=lambda x: os.path.getmtime(os.path.join(directory, x))\n",
    "    )\n",
    "    \n",
    "    if model_files:\n",
    "        latest_model = model_files[-1]\n",
    "        print(f\"Loading model: {latest_model}\")\n",
    "        return load_model(os.path.join(directory, latest_model))\n",
    "    else:\n",
    "        print(\"No model found, training from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for epoch set: 1 to 10\n",
      "No model found, training from scratch.\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2853\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.2892.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.2855 - val_loss: 0.1558\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2736\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.2866.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2743 - val_loss: 0.1527\n",
      "Epoch 3/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2856\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.2813.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2851 - val_loss: 0.1496\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2719\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.2766.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.2723 - val_loss: 0.1465\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2548\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.2725.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2557 - val_loss: 0.1434\n",
      "Epoch 6/10\n",
      "\u001b[1m34/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2645\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.2655.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.2647 - val_loss: 0.1404\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.2601\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.2589.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2601 - val_loss: 0.1372\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2603\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.2541.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2598 - val_loss: 0.1342\n",
      "Epoch 9/10\n",
      "\u001b[1m34/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2366\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.2504.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2385 - val_loss: 0.1311\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2456\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.2430.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2455 - val_loss: 0.1280\n",
      "Epoch 1: Training Loss = 0.2892040014266968\n",
      "Epoch 2: Training Loss = 0.2866086959838867\n",
      "Epoch 3: Training Loss = 0.2813454568386078\n",
      "Epoch 4: Training Loss = 0.27658507227897644\n",
      "Epoch 5: Training Loss = 0.27247264981269836\n",
      "Epoch 6: Training Loss = 0.265485942363739\n",
      "Epoch 7: Training Loss = 0.258943110704422\n",
      "Epoch 8: Training Loss = 0.2540915310382843\n",
      "Epoch 9: Training Loss = 0.2504011392593384\n",
      "Epoch 10: Training Loss = 0.2430468201637268\n",
      "Starting training for epoch set: 11 to 20\n",
      "Loading model: model_epoch_10_loss_0.2430.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2406\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.2396.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.2406 - val_loss: 0.1250\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2252\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.2323.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2257 - val_loss: 0.1219\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2238\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.2280.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2240 - val_loss: 0.1189\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2215\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.2228.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2215 - val_loss: 0.1159\n",
      "Epoch 5/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2194\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.2225.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2197 - val_loss: 0.1129\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.2074\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.2124.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2076 - val_loss: 0.1100\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2261\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.2065.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2256 - val_loss: 0.1071\n",
      "Epoch 8/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1947\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.2012.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1954 - val_loss: 0.1042\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1960\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1954.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1960 - val_loss: 0.1014\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.2013\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1900.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.2008 - val_loss: 0.0986\n",
      "Epoch 11: Training Loss = 0.2396388202905655\n",
      "Epoch 12: Training Loss = 0.23230069875717163\n",
      "Epoch 13: Training Loss = 0.22802461683750153\n",
      "Epoch 14: Training Loss = 0.2227647453546524\n",
      "Epoch 15: Training Loss = 0.2224717140197754\n",
      "Epoch 16: Training Loss = 0.21236036717891693\n",
      "Epoch 17: Training Loss = 0.20651377737522125\n",
      "Epoch 18: Training Loss = 0.20116034150123596\n",
      "Epoch 19: Training Loss = 0.1954355388879776\n",
      "Epoch 20: Training Loss = 0.1899692416191101\n",
      "Starting training for epoch set: 21 to 30\n",
      "Loading model: model_epoch_10_loss_0.1900.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1790\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1865.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.1795 - val_loss: 0.0958\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1633\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.1844.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1649 - val_loss: 0.0931\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1701\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.1754.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1702 - val_loss: 0.0904\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1685\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.1694.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1686 - val_loss: 0.0878\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.1615\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.1674.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.1620 - val_loss: 0.0853\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1753\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.1619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1749 - val_loss: 0.0828\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1557\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.1592.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1560 - val_loss: 0.0804\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1438\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.1528.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1445 - val_loss: 0.0781\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1514\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1489.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1513 - val_loss: 0.0758\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1432\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1461.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1433 - val_loss: 0.0736\n",
      "Epoch 21: Training Loss = 0.1864572912454605\n",
      "Epoch 22: Training Loss = 0.18444207310676575\n",
      "Epoch 23: Training Loss = 0.1753600835800171\n",
      "Epoch 24: Training Loss = 0.16942991316318512\n",
      "Epoch 25: Training Loss = 0.1673784852027893\n",
      "Epoch 26: Training Loss = 0.16192929446697235\n",
      "Epoch 27: Training Loss = 0.15923675894737244\n",
      "Epoch 28: Training Loss = 0.1528470665216446\n",
      "Epoch 29: Training Loss = 0.14893585443496704\n",
      "Epoch 30: Training Loss = 0.14613774418830872\n",
      "Starting training for epoch set: 31 to 40\n",
      "Loading model: model_epoch_10_loss_0.1461.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1402\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1400.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.1402 - val_loss: 0.0715\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1314\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.1354.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1318 - val_loss: 0.0695\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1365\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.1320.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1361 - val_loss: 0.0676\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1237\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.1260.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1238 - val_loss: 0.0658\n",
      "Epoch 5/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1215\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.1224.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1215 - val_loss: 0.0641\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1154\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.1193.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1159 - val_loss: 0.0624\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1109\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.1147.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1112 - val_loss: 0.0609\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1197\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.1119.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1195 - val_loss: 0.0595\n",
      "Epoch 9/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1050\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1099.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1054 - val_loss: 0.0582\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1141\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1042.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1133 - val_loss: 0.0569\n",
      "Epoch 31: Training Loss = 0.13996635377407074\n",
      "Epoch 32: Training Loss = 0.13541753590106964\n",
      "Epoch 33: Training Loss = 0.13200253248214722\n",
      "Epoch 34: Training Loss = 0.12604276835918427\n",
      "Epoch 35: Training Loss = 0.12241696566343307\n",
      "Epoch 36: Training Loss = 0.11932860314846039\n",
      "Epoch 37: Training Loss = 0.11466429382562637\n",
      "Epoch 38: Training Loss = 0.11191200464963913\n",
      "Epoch 39: Training Loss = 0.10994873195886612\n",
      "Epoch 40: Training Loss = 0.10421764105558395\n",
      "Starting training for epoch set: 41 to 50\n",
      "Loading model: model_epoch_10_loss_0.1042.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1049\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1038.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 0.1048 - val_loss: 0.0558\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0994\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0977.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0994 - val_loss: 0.0548\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0966\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0967.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0966 - val_loss: 0.0539\n",
      "Epoch 4/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0957\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0913.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0953 - val_loss: 0.0531\n",
      "Epoch 5/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0873\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0895.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0875 - val_loss: 0.0523\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0964\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0911.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0959 - val_loss: 0.0517\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0908\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0862.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0906 - val_loss: 0.0512\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0814\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0848.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0817 - val_loss: 0.0507\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0832\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0860.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0833 - val_loss: 0.0504\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0859\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0819.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0856 - val_loss: 0.0501\n",
      "Epoch 41: Training Loss = 0.1038220152258873\n",
      "Epoch 42: Training Loss = 0.09773987531661987\n",
      "Epoch 43: Training Loss = 0.09674591571092606\n",
      "Epoch 44: Training Loss = 0.09134194999933243\n",
      "Epoch 45: Training Loss = 0.08947717398405075\n",
      "Epoch 46: Training Loss = 0.09114064276218414\n",
      "Epoch 47: Training Loss = 0.08616792410612106\n",
      "Epoch 48: Training Loss = 0.08475819230079651\n",
      "Epoch 49: Training Loss = 0.08598481863737106\n",
      "Epoch 50: Training Loss = 0.08194208145141602\n",
      "Starting training for epoch set: 51 to 60\n",
      "Loading model: model_epoch_10_loss_0.0819.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0780\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0788.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0781 - val_loss: 0.0498\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0799\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0780.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0798 - val_loss: 0.0497\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0795\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0764.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0794 - val_loss: 0.0496\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0797\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0780.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0797 - val_loss: 0.0495\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0703\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0731.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0704 - val_loss: 0.0495\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0743\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0731.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0742 - val_loss: 0.0496\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0682\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0719.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0684 - val_loss: 0.0497\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0709\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0689.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0709 - val_loss: 0.0498\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0707\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0701.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0706 - val_loss: 0.0500\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0687\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0695.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0688 - val_loss: 0.0502\n",
      "Epoch 51: Training Loss = 0.07881037145853043\n",
      "Epoch 52: Training Loss = 0.07797461003065109\n",
      "Epoch 53: Training Loss = 0.07641091197729111\n",
      "Epoch 54: Training Loss = 0.07800599932670593\n",
      "Epoch 55: Training Loss = 0.07309892773628235\n",
      "Epoch 56: Training Loss = 0.07313596457242966\n",
      "Epoch 57: Training Loss = 0.07192318886518478\n",
      "Epoch 58: Training Loss = 0.06890510767698288\n",
      "Epoch 59: Training Loss = 0.07012949883937836\n",
      "Epoch 60: Training Loss = 0.06947021931409836\n",
      "Starting training for epoch set: 61 to 70\n",
      "Loading model: model_epoch_10_loss_0.0695.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0703\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0698.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0702 - val_loss: 0.0503\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0679\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0673.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0679 - val_loss: 0.0506\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0690\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0672.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0689 - val_loss: 0.0508\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0699\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0695.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0699 - val_loss: 0.0511\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0721\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0691.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0719 - val_loss: 0.0513\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0658\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0677.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0659 - val_loss: 0.0516\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0644\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0648.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0644 - val_loss: 0.0519\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0717\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0692.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0716 - val_loss: 0.0522\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0613\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0669.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0615 - val_loss: 0.0525\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0617\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0618 - val_loss: 0.0528\n",
      "Epoch 61: Training Loss = 0.06981804221868515\n",
      "Epoch 62: Training Loss = 0.06726228445768356\n",
      "Epoch 63: Training Loss = 0.06724525988101959\n",
      "Epoch 64: Training Loss = 0.0695166364312172\n",
      "Epoch 65: Training Loss = 0.06910930573940277\n",
      "Epoch 66: Training Loss = 0.06772603839635849\n",
      "Epoch 67: Training Loss = 0.06477773934602737\n",
      "Epoch 68: Training Loss = 0.06921350210905075\n",
      "Epoch 69: Training Loss = 0.06690774112939835\n",
      "Epoch 70: Training Loss = 0.06403154134750366\n",
      "Starting training for epoch set: 71 to 80\n",
      "Loading model: model_epoch_10_loss_0.0640.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0624\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0663.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0625 - val_loss: 0.0531\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0644\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0652.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0644 - val_loss: 0.0534\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0652\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0636.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0652 - val_loss: 0.0536\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0675\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0639.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0674 - val_loss: 0.0539\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0661\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0650.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0661 - val_loss: 0.0542\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0691\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0654.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0689 - val_loss: 0.0545\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0605\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0631.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0606 - val_loss: 0.0547\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0635\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0634.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0635 - val_loss: 0.0550\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0635\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0634 - val_loss: 0.0552\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0629\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0629 - val_loss: 0.0554\n",
      "Epoch 71: Training Loss = 0.06628766655921936\n",
      "Epoch 72: Training Loss = 0.0652000829577446\n",
      "Epoch 73: Training Loss = 0.06359986960887909\n",
      "Epoch 74: Training Loss = 0.06390602141618729\n",
      "Epoch 75: Training Loss = 0.06504350900650024\n",
      "Epoch 76: Training Loss = 0.06543009728193283\n",
      "Epoch 77: Training Loss = 0.06308679282665253\n",
      "Epoch 78: Training Loss = 0.06335292756557465\n",
      "Epoch 79: Training Loss = 0.06103524565696716\n",
      "Epoch 80: Training Loss = 0.06304849684238434\n",
      "Starting training for epoch set: 81 to 90\n",
      "Loading model: model_epoch_10_loss_0.0630.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0609\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0648.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0611 - val_loss: 0.0557\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0692\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0645.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0688 - val_loss: 0.0558\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0629\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0648.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0630 - val_loss: 0.0560\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0622\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0622 - val_loss: 0.0562\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0645\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0645 - val_loss: 0.0564\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0652\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0658.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0652 - val_loss: 0.0566\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0642\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0640 - val_loss: 0.0567\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0622\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0636.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0623 - val_loss: 0.0568\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0627\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0600.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0626 - val_loss: 0.0570\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0645\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0643.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0645 - val_loss: 0.0570\n",
      "Epoch 81: Training Loss = 0.06484025716781616\n",
      "Epoch 82: Training Loss = 0.06446654349565506\n",
      "Epoch 83: Training Loss = 0.06479322910308838\n",
      "Epoch 84: Training Loss = 0.06218787655234337\n",
      "Epoch 85: Training Loss = 0.06403153389692307\n",
      "Epoch 86: Training Loss = 0.06581921875476837\n",
      "Epoch 87: Training Loss = 0.061638571321964264\n",
      "Epoch 88: Training Loss = 0.06363946944475174\n",
      "Epoch 89: Training Loss = 0.05995189771056175\n",
      "Epoch 90: Training Loss = 0.06434118002653122\n",
      "Starting training for epoch set: 91 to 100\n",
      "Loading model: model_epoch_10_loss_0.0643.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0599\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0657.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0607 - val_loss: 0.0571\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0640\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0638.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0640 - val_loss: 0.0572\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0644\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0643 - val_loss: 0.0574\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0625\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0625 - val_loss: 0.0574\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0628\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0628 - val_loss: 0.0575\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0607\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0608 - val_loss: 0.0575\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0631\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0632 - val_loss: 0.0576\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0629\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0634.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0629 - val_loss: 0.0577\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0642\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0639 - val_loss: 0.0578\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0630\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0630 - val_loss: 0.0578\n",
      "Epoch 91: Training Loss = 0.06572660803794861\n",
      "Epoch 92: Training Loss = 0.06379222869873047\n",
      "Epoch 93: Training Loss = 0.06283546984195709\n",
      "Epoch 94: Training Loss = 0.06296781450510025\n",
      "Epoch 95: Training Loss = 0.062475331127643585\n",
      "Epoch 96: Training Loss = 0.06254889070987701\n",
      "Epoch 97: Training Loss = 0.06398870050907135\n",
      "Epoch 98: Training Loss = 0.06339388340711594\n",
      "Epoch 99: Training Loss = 0.06059320271015167\n",
      "Epoch 100: Training Loss = 0.06245330721139908\n",
      "Starting training for epoch set: 101 to 110\n",
      "Loading model: model_epoch_10_loss_0.0625.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0610\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0610 - val_loss: 0.0579\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0576\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0578 - val_loss: 0.0580\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0678\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0649.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0675 - val_loss: 0.0580\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0625\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0624 - val_loss: 0.0580\n",
      "Epoch 5/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0623\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0648.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0626 - val_loss: 0.0581\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0636\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0635 - val_loss: 0.0581\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0645\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0644 - val_loss: 0.0582\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0625\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0625 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0615\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0615 - val_loss: 0.0582\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0623\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0623 - val_loss: 0.0581\n",
      "Epoch 101: Training Loss = 0.06073674559593201\n",
      "Epoch 102: Training Loss = 0.06165289506316185\n",
      "Epoch 103: Training Loss = 0.06489913910627365\n",
      "Epoch 104: Training Loss = 0.061330974102020264\n",
      "Epoch 105: Training Loss = 0.06482198089361191\n",
      "Epoch 106: Training Loss = 0.06332042068243027\n",
      "Epoch 107: Training Loss = 0.062480900436639786\n",
      "Epoch 108: Training Loss = 0.06405430287122726\n",
      "Epoch 109: Training Loss = 0.061768047511577606\n",
      "Epoch 110: Training Loss = 0.06220187619328499\n",
      "Starting training for epoch set: 111 to 120\n",
      "Loading model: model_epoch_10_loss_0.0622.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0593\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0627.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0594 - val_loss: 0.0581\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0619\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0655.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0621 - val_loss: 0.0580\n",
      "Epoch 3/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0611\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0613 - val_loss: 0.0580\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0639\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0626.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0638 - val_loss: 0.0580\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0660\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0658 - val_loss: 0.0580\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0591\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0636.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0593 - val_loss: 0.0580\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0624\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0601.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0623 - val_loss: 0.0581\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0666\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0665 - val_loss: 0.0581\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0613\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0614 - val_loss: 0.0582\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0627\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0626 - val_loss: 0.0582\n",
      "Epoch 111: Training Loss = 0.06265470385551453\n",
      "Epoch 112: Training Loss = 0.065498486161232\n",
      "Epoch 113: Training Loss = 0.0628380998969078\n",
      "Epoch 114: Training Loss = 0.06264414638280869\n",
      "Epoch 115: Training Loss = 0.06096258759498596\n",
      "Epoch 116: Training Loss = 0.06364772468805313\n",
      "Epoch 117: Training Loss = 0.0600527748465538\n",
      "Epoch 118: Training Loss = 0.06367722153663635\n",
      "Epoch 119: Training Loss = 0.06188857927918434\n",
      "Epoch 120: Training Loss = 0.06149958819150925\n",
      "Starting training for epoch set: 121 to 130\n",
      "Loading model: model_epoch_10_loss_0.0615.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0553\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0595.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0556 - val_loss: 0.0583\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0650\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0651.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0650 - val_loss: 0.0582\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0573\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0587.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0574 - val_loss: 0.0583\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0625\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0625 - val_loss: 0.0583\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0631\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0630 - val_loss: 0.0582\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0650\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0649 - val_loss: 0.0583\n",
      "Epoch 7/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0571\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0593.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0573 - val_loss: 0.0583\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0672\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0655.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0671 - val_loss: 0.0583\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0640\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0640 - val_loss: 0.0583\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0668\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0667 - val_loss: 0.0583\n",
      "Epoch 121: Training Loss = 0.05947946012020111\n",
      "Epoch 122: Training Loss = 0.06506746262311935\n",
      "Epoch 123: Training Loss = 0.05872156098484993\n",
      "Epoch 124: Training Loss = 0.06280656903982162\n",
      "Epoch 125: Training Loss = 0.06149612367153168\n",
      "Epoch 126: Training Loss = 0.062069524079561234\n",
      "Epoch 127: Training Loss = 0.05934687331318855\n",
      "Epoch 128: Training Loss = 0.06546236574649811\n",
      "Epoch 129: Training Loss = 0.06374886631965637\n",
      "Epoch 130: Training Loss = 0.06410330533981323\n",
      "Starting training for epoch set: 131 to 140\n",
      "Loading model: model_epoch_10_loss_0.0641.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0673\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0646.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0672 - val_loss: 0.0583\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0699\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0694 - val_loss: 0.0583\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0655\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0654 - val_loss: 0.0583\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0622\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0621 - val_loss: 0.0583\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0622\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0623 - val_loss: 0.0583\n",
      "Epoch 6/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0609\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0610 - val_loss: 0.0583\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0618\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0619 - val_loss: 0.0582\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0617\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0631.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0618 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0625\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0625 - val_loss: 0.0581\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0581\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0582 - val_loss: 0.0582\n",
      "Epoch 131: Training Loss = 0.06456489861011505\n",
      "Epoch 132: Training Loss = 0.06411627680063248\n",
      "Epoch 133: Training Loss = 0.062353286892175674\n",
      "Epoch 134: Training Loss = 0.06059090420603752\n",
      "Epoch 135: Training Loss = 0.06330981850624084\n",
      "Epoch 136: Training Loss = 0.06213867664337158\n",
      "Epoch 137: Training Loss = 0.06285181641578674\n",
      "Epoch 138: Training Loss = 0.06307023018598557\n",
      "Epoch 139: Training Loss = 0.06168358400464058\n",
      "Epoch 140: Training Loss = 0.06109358370304108\n",
      "Starting training for epoch set: 141 to 150\n",
      "Loading model: model_epoch_10_loss_0.0611.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0644\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0627.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0643 - val_loss: 0.0582\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0635\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0635.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0635 - val_loss: 0.0582\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0652\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0651.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0652 - val_loss: 0.0582\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0611\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0611 - val_loss: 0.0582\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0629\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0627 - val_loss: 0.0582\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0668\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0643.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0667 - val_loss: 0.0582\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0590\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0601.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0590 - val_loss: 0.0582\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0628\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0628 - val_loss: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0604\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0605 - val_loss: 0.0582\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0581\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0586 - val_loss: 0.0582\n",
      "Epoch 141: Training Loss = 0.06269805133342743\n",
      "Epoch 142: Training Loss = 0.0634615421295166\n",
      "Epoch 143: Training Loss = 0.06513183563947678\n",
      "Epoch 144: Training Loss = 0.06279370933771133\n",
      "Epoch 145: Training Loss = 0.060955289751291275\n",
      "Epoch 146: Training Loss = 0.06427165865898132\n",
      "Epoch 147: Training Loss = 0.06006481871008873\n",
      "Epoch 148: Training Loss = 0.062364108860492706\n",
      "Epoch 149: Training Loss = 0.06113436445593834\n",
      "Epoch 150: Training Loss = 0.06396772712469101\n",
      "Starting training for epoch set: 151 to 160\n",
      "Loading model: model_epoch_10_loss_0.0640.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0627\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0632.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0627 - val_loss: 0.0581\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0664\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0654.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0663 - val_loss: 0.0581\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0628\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0628 - val_loss: 0.0581\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0603\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0604 - val_loss: 0.0581\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0662\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0660 - val_loss: 0.0581\n",
      "Epoch 6/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0643\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0641 - val_loss: 0.0581\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0661\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0660 - val_loss: 0.0580\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0601\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0626.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0602 - val_loss: 0.0580\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0610\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0623.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0611 - val_loss: 0.0580\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0632\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0632 - val_loss: 0.0580\n",
      "Epoch 151: Training Loss = 0.06318735331296921\n",
      "Epoch 152: Training Loss = 0.06538312137126923\n",
      "Epoch 153: Training Loss = 0.06241189315915108\n",
      "Epoch 154: Training Loss = 0.06201373413205147\n",
      "Epoch 155: Training Loss = 0.06331057846546173\n",
      "Epoch 156: Training Loss = 0.06241566315293312\n",
      "Epoch 157: Training Loss = 0.06402084231376648\n",
      "Epoch 158: Training Loss = 0.06261027604341507\n",
      "Epoch 159: Training Loss = 0.062294602394104004\n",
      "Epoch 160: Training Loss = 0.06374495476484299\n",
      "Starting training for epoch set: 161 to 170\n",
      "Loading model: model_epoch_10_loss_0.0637.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0605\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 0.0606 - val_loss: 0.0580\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0593\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0601.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0593 - val_loss: 0.0580\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0648\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0648 - val_loss: 0.0580\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0627\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0627 - val_loss: 0.0580\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0624\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0601.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0624 - val_loss: 0.0580\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0638\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0638.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0638 - val_loss: 0.0580\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0657\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0654 - val_loss: 0.0580\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0627\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0626 - val_loss: 0.0580\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0608\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0609 - val_loss: 0.0580\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0586\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0588 - val_loss: 0.0581\n",
      "Epoch 161: Training Loss = 0.06178712844848633\n",
      "Epoch 162: Training Loss = 0.06014220044016838\n",
      "Epoch 163: Training Loss = 0.06374645978212357\n",
      "Epoch 164: Training Loss = 0.0639595240354538\n",
      "Epoch 165: Training Loss = 0.06012716889381409\n",
      "Epoch 166: Training Loss = 0.0637965053319931\n",
      "Epoch 167: Training Loss = 0.06162036955356598\n",
      "Epoch 168: Training Loss = 0.06105520948767662\n",
      "Epoch 169: Training Loss = 0.06199268251657486\n",
      "Epoch 170: Training Loss = 0.06207515671849251\n",
      "Starting training for epoch set: 171 to 180\n",
      "Loading model: model_epoch_10_loss_0.0621.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0574\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0595.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0575 - val_loss: 0.0580\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0642\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0632.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0642 - val_loss: 0.0580\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0604\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0605 - val_loss: 0.0581\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0610\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0643.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0611 - val_loss: 0.0580\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0614\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0614 - val_loss: 0.0580\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0647\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0646 - val_loss: 0.0580\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0596\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0597 - val_loss: 0.0580\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0656\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0654 - val_loss: 0.0580\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0598\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0599 - val_loss: 0.0580\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0598\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0599 - val_loss: 0.0580\n",
      "Epoch 171: Training Loss = 0.059496644884347916\n",
      "Epoch 172: Training Loss = 0.06324822455644608\n",
      "Epoch 173: Training Loss = 0.06130973994731903\n",
      "Epoch 174: Training Loss = 0.06431525200605392\n",
      "Epoch 175: Training Loss = 0.06118324398994446\n",
      "Epoch 176: Training Loss = 0.062026042491197586\n",
      "Epoch 177: Training Loss = 0.06326709687709808\n",
      "Epoch 178: Training Loss = 0.061282169073820114\n",
      "Epoch 179: Training Loss = 0.06187310442328453\n",
      "Epoch 180: Training Loss = 0.061384957283735275\n",
      "Starting training for epoch set: 181 to 190\n",
      "Loading model: model_epoch_10_loss_0.0614.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0568\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 44ms/step - loss: 0.0572 - val_loss: 0.0580\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0662\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0659.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0662 - val_loss: 0.0579\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0568\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0570 - val_loss: 0.0579\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0617\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0617 - val_loss: 0.0579\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0610\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0636.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0610 - val_loss: 0.0579\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0628\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0623.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0628 - val_loss: 0.0578\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0644\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0643 - val_loss: 0.0578\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0634\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0634 - val_loss: 0.0578\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0599\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0638.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0601 - val_loss: 0.0578\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0599\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0600 - val_loss: 0.0578\n",
      "Epoch 181: Training Loss = 0.06172594800591469\n",
      "Epoch 182: Training Loss = 0.0658755972981453\n",
      "Epoch 183: Training Loss = 0.06159055605530739\n",
      "Epoch 184: Training Loss = 0.06214408949017525\n",
      "Epoch 185: Training Loss = 0.06355047971010208\n",
      "Epoch 186: Training Loss = 0.06228310242295265\n",
      "Epoch 187: Training Loss = 0.06191190704703331\n",
      "Epoch 188: Training Loss = 0.06275998800992966\n",
      "Epoch 189: Training Loss = 0.06383571773767471\n",
      "Epoch 190: Training Loss = 0.0628892183303833\n",
      "Starting training for epoch set: 191 to 200\n",
      "Loading model: model_epoch_10_loss_0.0629.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0633\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.0633 - val_loss: 0.0578\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0585\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0585 - val_loss: 0.0578\n",
      "Epoch 3/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0649\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0648 - val_loss: 0.0577\n",
      "Epoch 4/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0591\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0593 - val_loss: 0.0577\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0610\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0610 - val_loss: 0.0577\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0657\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0656 - val_loss: 0.0576\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0613\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0613 - val_loss: 0.0576\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0637\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0636 - val_loss: 0.0576\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0562\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0590.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0564 - val_loss: 0.0575\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0661\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0658 - val_loss: 0.0576\n",
      "Epoch 191: Training Loss = 0.06414791196584702\n",
      "Epoch 192: Training Loss = 0.061175644397735596\n",
      "Epoch 193: Training Loss = 0.06406490504741669\n",
      "Epoch 194: Training Loss = 0.0607142336666584\n",
      "Epoch 195: Training Loss = 0.061207231134176254\n",
      "Epoch 196: Training Loss = 0.06288907676935196\n",
      "Epoch 197: Training Loss = 0.06275482475757599\n",
      "Epoch 198: Training Loss = 0.06142519786953926\n",
      "Epoch 199: Training Loss = 0.05901457369327545\n",
      "Epoch 200: Training Loss = 0.06122056394815445\n",
      "Starting training for epoch set: 201 to 210\n",
      "Loading model: model_epoch_10_loss_0.0612.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0645\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0598.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.0643 - val_loss: 0.0576\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0625\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0632.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0625 - val_loss: 0.0576\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0566\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0595.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0567 - val_loss: 0.0576\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0596\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0597 - val_loss: 0.0576\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0579\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0580 - val_loss: 0.0576\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0612\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0613 - val_loss: 0.0577\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0620\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0619 - val_loss: 0.0577\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0547\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0600.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0551 - val_loss: 0.0577\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0625\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0627.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0625 - val_loss: 0.0577\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0587\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0588 - val_loss: 0.0576\n",
      "Epoch 201: Training Loss = 0.05976806581020355\n",
      "Epoch 202: Training Loss = 0.06322600692510605\n",
      "Epoch 203: Training Loss = 0.05954834073781967\n",
      "Epoch 204: Training Loss = 0.06065814942121506\n",
      "Epoch 205: Training Loss = 0.06224169582128525\n",
      "Epoch 206: Training Loss = 0.06235037371516228\n",
      "Epoch 207: Training Loss = 0.0610419400036335\n",
      "Epoch 208: Training Loss = 0.06002429872751236\n",
      "Epoch 209: Training Loss = 0.06271810829639435\n",
      "Epoch 210: Training Loss = 0.06168084591627121\n",
      "Starting training for epoch set: 211 to 220\n",
      "Loading model: model_epoch_10_loss_0.0617.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0634\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0640.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 0.0635 - val_loss: 0.0577\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0597\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0598 - val_loss: 0.0576\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0610\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0600.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0609 - val_loss: 0.0577\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0577\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0579 - val_loss: 0.0577\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0621\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0621 - val_loss: 0.0577\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0649\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0646 - val_loss: 0.0577\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0634\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0633 - val_loss: 0.0577\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0595\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0596 - val_loss: 0.0577\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0595\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0596 - val_loss: 0.0577\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0591\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0598.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0591 - val_loss: 0.0577\n",
      "Epoch 211: Training Loss = 0.06403223425149918\n",
      "Epoch 212: Training Loss = 0.060710322111845016\n",
      "Epoch 213: Training Loss = 0.06001192703843117\n",
      "Epoch 214: Training Loss = 0.06121138855814934\n",
      "Epoch 215: Training Loss = 0.061912644654512405\n",
      "Epoch 216: Training Loss = 0.06198743358254433\n",
      "Epoch 217: Training Loss = 0.06129799410700798\n",
      "Epoch 218: Training Loss = 0.06101199612021446\n",
      "Epoch 219: Training Loss = 0.06203868240118027\n",
      "Epoch 220: Training Loss = 0.05976703017950058\n",
      "Starting training for epoch set: 221 to 230\n",
      "Loading model: model_epoch_10_loss_0.0598.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0576\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0608.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0578 - val_loss: 0.0578\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0569\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0596.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0570 - val_loss: 0.0578\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0670\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0667 - val_loss: 0.0577\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0603\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0603 - val_loss: 0.0578\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0584\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0586 - val_loss: 0.0577\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0654\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0631.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0652 - val_loss: 0.0577\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0612\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0612 - val_loss: 0.0576\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0653\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0635.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0652 - val_loss: 0.0576\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0633\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0633 - val_loss: 0.0576\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0604\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0588.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0603 - val_loss: 0.0576\n",
      "Epoch 221: Training Loss = 0.06084900721907616\n",
      "Epoch 222: Training Loss = 0.05956730619072914\n",
      "Epoch 223: Training Loss = 0.06194880232214928\n",
      "Epoch 224: Training Loss = 0.06143823638558388\n",
      "Epoch 225: Training Loss = 0.06121833249926567\n",
      "Epoch 226: Training Loss = 0.06309990584850311\n",
      "Epoch 227: Training Loss = 0.0614517405629158\n",
      "Epoch 228: Training Loss = 0.063482366502285\n",
      "Epoch 229: Training Loss = 0.06245400011539459\n",
      "Epoch 230: Training Loss = 0.05876663699746132\n",
      "Starting training for epoch set: 231 to 240\n",
      "Loading model: model_epoch_10_loss_0.0588.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0639\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0636 - val_loss: 0.0576\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0572\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0598.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0574 - val_loss: 0.0576\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0621\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0621 - val_loss: 0.0576\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0615\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0615 - val_loss: 0.0575\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0634\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0633 - val_loss: 0.0575\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0604\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0605 - val_loss: 0.0575\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0625\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0638.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0626 - val_loss: 0.0576\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0641\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0626.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0641 - val_loss: 0.0576\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0666\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0664 - val_loss: 0.0576\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0617\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0617 - val_loss: 0.0575\n",
      "Epoch 231: Training Loss = 0.061600323766469955\n",
      "Epoch 232: Training Loss = 0.05981390178203583\n",
      "Epoch 233: Training Loss = 0.06186710298061371\n",
      "Epoch 234: Training Loss = 0.060489535331726074\n",
      "Epoch 235: Training Loss = 0.06282997131347656\n",
      "Epoch 236: Training Loss = 0.061203498393297195\n",
      "Epoch 237: Training Loss = 0.06375353783369064\n",
      "Epoch 238: Training Loss = 0.06258664280176163\n",
      "Epoch 239: Training Loss = 0.06290555000305176\n",
      "Epoch 240: Training Loss = 0.062066782265901566\n",
      "Starting training for epoch set: 241 to 250\n",
      "Loading model: model_epoch_10_loss_0.0621.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0670\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0645.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - loss: 0.0667 - val_loss: 0.0575\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0595\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0597 - val_loss: 0.0575\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0649\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0648 - val_loss: 0.0575\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0643\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0642 - val_loss: 0.0576\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0617\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0616 - val_loss: 0.0576\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0613\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0634.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0615 - val_loss: 0.0576\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0574\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0593.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0575 - val_loss: 0.0575\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0608\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0609 - val_loss: 0.0575\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0604\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0603.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0604 - val_loss: 0.0574\n",
      "Epoch 10/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0632\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0632 - val_loss: 0.0574\n",
      "Epoch 241: Training Loss = 0.06449215859174728\n",
      "Epoch 242: Training Loss = 0.06140352785587311\n",
      "Epoch 243: Training Loss = 0.06328128278255463\n",
      "Epoch 244: Training Loss = 0.06147926300764084\n",
      "Epoch 245: Training Loss = 0.06053953245282173\n",
      "Epoch 246: Training Loss = 0.06344778090715408\n",
      "Epoch 247: Training Loss = 0.05925534665584564\n",
      "Epoch 248: Training Loss = 0.06195924058556557\n",
      "Epoch 249: Training Loss = 0.06029694154858589\n",
      "Epoch 250: Training Loss = 0.062440477311611176\n",
      "Starting training for epoch set: 251 to 260\n",
      "Loading model: model_epoch_10_loss_0.0624.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0635\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0633 - val_loss: 0.0574\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0614\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0614 - val_loss: 0.0574\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0635\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0635 - val_loss: 0.0573\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0618\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0617 - val_loss: 0.0573\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0602\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0602 - val_loss: 0.0573\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0593\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0593 - val_loss: 0.0573\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0614\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0590.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0613 - val_loss: 0.0573\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0595\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0595 - val_loss: 0.0573\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0610\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0610 - val_loss: 0.0573\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0627\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0628 - val_loss: 0.0572\n",
      "Epoch 251: Training Loss = 0.06120980530977249\n",
      "Epoch 252: Training Loss = 0.06179230287671089\n",
      "Epoch 253: Training Loss = 0.06301254034042358\n",
      "Epoch 254: Training Loss = 0.060987021774053574\n",
      "Epoch 255: Training Loss = 0.06181162968277931\n",
      "Epoch 256: Training Loss = 0.06065169721841812\n",
      "Epoch 257: Training Loss = 0.058966949582099915\n",
      "Epoch 258: Training Loss = 0.05987173691391945\n",
      "Epoch 259: Training Loss = 0.06141262128949165\n",
      "Epoch 260: Training Loss = 0.0637459084391594\n",
      "Starting training for epoch set: 261 to 270\n",
      "Loading model: model_epoch_10_loss_0.0637.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0639\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0644.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0640 - val_loss: 0.0571\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0637\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0637 - val_loss: 0.0571\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0619\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0619 - val_loss: 0.0571\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0595\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0596 - val_loss: 0.0571\n",
      "Epoch 5/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0682\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0677 - val_loss: 0.0571\n",
      "Epoch 6/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0655\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0623.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0653 - val_loss: 0.0571\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0603\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0604 - val_loss: 0.0572\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0609\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0609 - val_loss: 0.0571\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0596\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0597 - val_loss: 0.0572\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0599\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0600 - val_loss: 0.0572\n",
      "Epoch 261: Training Loss = 0.06439083069562912\n",
      "Epoch 262: Training Loss = 0.06279554218053818\n",
      "Epoch 263: Training Loss = 0.062429435551166534\n",
      "Epoch 264: Training Loss = 0.061843566596508026\n",
      "Epoch 265: Training Loss = 0.0610017403960228\n",
      "Epoch 266: Training Loss = 0.06232220306992531\n",
      "Epoch 267: Training Loss = 0.0628887340426445\n",
      "Epoch 268: Training Loss = 0.06301476061344147\n",
      "Epoch 269: Training Loss = 0.0612252801656723\n",
      "Epoch 270: Training Loss = 0.06135426089167595\n",
      "Starting training for epoch set: 271 to 280\n",
      "Loading model: model_epoch_10_loss_0.0614.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0634\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0631.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0634 - val_loss: 0.0573\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0604\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0635.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0606 - val_loss: 0.0572\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0644\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0594.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0641 - val_loss: 0.0572\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0599\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0600 - val_loss: 0.0572\n",
      "Epoch 5/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0625\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0623 - val_loss: 0.0572\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0600\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0600 - val_loss: 0.0571\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0675\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0671 - val_loss: 0.0571\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0583\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0584 - val_loss: 0.0572\n",
      "Epoch 9/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0684\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0677 - val_loss: 0.0572\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0640\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0639 - val_loss: 0.0572\n",
      "Epoch 271: Training Loss = 0.06311266869306564\n",
      "Epoch 272: Training Loss = 0.06352245807647705\n",
      "Epoch 273: Training Loss = 0.059390291571617126\n",
      "Epoch 274: Training Loss = 0.06105199083685875\n",
      "Epoch 275: Training Loss = 0.06055700406432152\n",
      "Epoch 276: Training Loss = 0.06128112971782684\n",
      "Epoch 277: Training Loss = 0.06328406929969788\n",
      "Epoch 278: Training Loss = 0.0622149221599102\n",
      "Epoch 279: Training Loss = 0.06176989898085594\n",
      "Epoch 280: Training Loss = 0.06052621826529503\n",
      "Starting training for epoch set: 281 to 290\n",
      "Loading model: model_epoch_10_loss_0.0605.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0635\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0633 - val_loss: 0.0573\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0570\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0571 - val_loss: 0.0572\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0643\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0642 - val_loss: 0.0572\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0647\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0646 - val_loss: 0.0573\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0577\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0578 - val_loss: 0.0572\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0578\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0580 - val_loss: 0.0571\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0591\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0593 - val_loss: 0.0571\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0644\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0635.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0644 - val_loss: 0.0571\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0654\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0652 - val_loss: 0.0570\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0559\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0563 - val_loss: 0.0570\n",
      "Epoch 281: Training Loss = 0.06117413938045502\n",
      "Epoch 282: Training Loss = 0.061046142131090164\n",
      "Epoch 283: Training Loss = 0.06221022829413414\n",
      "Epoch 284: Training Loss = 0.06110609695315361\n",
      "Epoch 285: Training Loss = 0.05990031734108925\n",
      "Epoch 286: Training Loss = 0.06186052784323692\n",
      "Epoch 287: Training Loss = 0.06136123090982437\n",
      "Epoch 288: Training Loss = 0.06346272677183151\n",
      "Epoch 289: Training Loss = 0.062224261462688446\n",
      "Epoch 290: Training Loss = 0.0605965293943882\n",
      "Starting training for epoch set: 291 to 300\n",
      "Loading model: model_epoch_10_loss_0.0606.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0570\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0575 - val_loss: 0.0570\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0613\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0589.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0612 - val_loss: 0.0570\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0598\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0599 - val_loss: 0.0570\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0635\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0634 - val_loss: 0.0570\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0566\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0592.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0567 - val_loss: 0.0570\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0625\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0625 - val_loss: 0.0571\n",
      "Epoch 7/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0631\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0629 - val_loss: 0.0570\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0613\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0613 - val_loss: 0.0570\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0643\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0642 - val_loss: 0.0570\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0649\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0647 - val_loss: 0.0571\n",
      "Epoch 291: Training Loss = 0.0621369294822216\n",
      "Epoch 292: Training Loss = 0.058949802070856094\n",
      "Epoch 293: Training Loss = 0.06097794696688652\n",
      "Epoch 294: Training Loss = 0.061811529099941254\n",
      "Epoch 295: Training Loss = 0.05918771028518677\n",
      "Epoch 296: Training Loss = 0.06073331832885742\n",
      "Epoch 297: Training Loss = 0.06170019134879112\n",
      "Epoch 298: Training Loss = 0.0611429400742054\n",
      "Epoch 299: Training Loss = 0.061349447816610336\n",
      "Epoch 300: Training Loss = 0.0614086277782917\n",
      "Starting training for epoch set: 301 to 310\n",
      "Loading model: model_epoch_10_loss_0.0614.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0638\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0636 - val_loss: 0.0570\n",
      "Epoch 2/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0552\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0598.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0557 - val_loss: 0.0570\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0605\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0605 - val_loss: 0.0570\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0600\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0600 - val_loss: 0.0571\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0615\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0614 - val_loss: 0.0571\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0601\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0601 - val_loss: 0.0571\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0556\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0609.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0558 - val_loss: 0.0570\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0607\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0609 - val_loss: 0.0570\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0609\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0609 - val_loss: 0.0570\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0594\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0595 - val_loss: 0.0570\n",
      "Epoch 301: Training Loss = 0.06098527833819389\n",
      "Epoch 302: Training Loss = 0.059776902198791504\n",
      "Epoch 303: Training Loss = 0.06147262081503868\n",
      "Epoch 304: Training Loss = 0.060648053884506226\n",
      "Epoch 305: Training Loss = 0.060537632554769516\n",
      "Epoch 306: Training Loss = 0.06179149076342583\n",
      "Epoch 307: Training Loss = 0.06092015653848648\n",
      "Epoch 308: Training Loss = 0.06239217147231102\n",
      "Epoch 309: Training Loss = 0.06164546310901642\n",
      "Epoch 310: Training Loss = 0.06100337952375412\n",
      "Starting training for epoch set: 311 to 320\n",
      "Loading model: model_epoch_10_loss_0.0610.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0620\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0621 - val_loss: 0.0570\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0647\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0646 - val_loss: 0.0569\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0575\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0624.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0579 - val_loss: 0.0570\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0643\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0637.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0643 - val_loss: 0.0570\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0618\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0617 - val_loss: 0.0570\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0637\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0636 - val_loss: 0.0570\n",
      "Epoch 7/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0598\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0601 - val_loss: 0.0569\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0604\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0601.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0604 - val_loss: 0.0569\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0580\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0597.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0581 - val_loss: 0.0569\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0611\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0585.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0610 - val_loss: 0.0569\n",
      "Epoch 311: Training Loss = 0.06253310292959213\n",
      "Epoch 312: Training Loss = 0.06187247112393379\n",
      "Epoch 313: Training Loss = 0.06239927187561989\n",
      "Epoch 314: Training Loss = 0.06373237818479538\n",
      "Epoch 315: Training Loss = 0.06104882061481476\n",
      "Epoch 316: Training Loss = 0.062113359570503235\n",
      "Epoch 317: Training Loss = 0.06252176314592361\n",
      "Epoch 318: Training Loss = 0.06013581529259682\n",
      "Epoch 319: Training Loss = 0.05970543995499611\n",
      "Epoch 320: Training Loss = 0.05852826312184334\n",
      "Starting training for epoch set: 321 to 330\n",
      "Loading model: model_epoch_10_loss_0.0585.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0625\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: 0.0624 - val_loss: 0.0570\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0598\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0599 - val_loss: 0.0569\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0619\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0619 - val_loss: 0.0569\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0629\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0628 - val_loss: 0.0570\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0661\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0641.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0660 - val_loss: 0.0569\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0619\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0618 - val_loss: 0.0570\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0617\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0618 - val_loss: 0.0569\n",
      "Epoch 8/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0618\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0628.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0619 - val_loss: 0.0569\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0604\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0604 - val_loss: 0.0569\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0584\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0585 - val_loss: 0.0569\n",
      "Epoch 321: Training Loss = 0.060665760189294815\n",
      "Epoch 322: Training Loss = 0.062498193234205246\n",
      "Epoch 323: Training Loss = 0.06216907501220703\n",
      "Epoch 324: Training Loss = 0.0622372180223465\n",
      "Epoch 325: Training Loss = 0.06405195593833923\n",
      "Epoch 326: Training Loss = 0.05985867232084274\n",
      "Epoch 327: Training Loss = 0.06250834465026855\n",
      "Epoch 328: Training Loss = 0.06279157102108002\n",
      "Epoch 329: Training Loss = 0.060459334403276443\n",
      "Epoch 330: Training Loss = 0.05985742062330246\n",
      "Starting training for epoch set: 331 to 340\n",
      "Loading model: model_epoch_10_loss_0.0599.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0620\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0621 - val_loss: 0.0569\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0614\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0615 - val_loss: 0.0569\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0637\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0636 - val_loss: 0.0568\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0608\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0603.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0607 - val_loss: 0.0568\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0617\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0627.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0617 - val_loss: 0.0568\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0572\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0588.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0572 - val_loss: 0.0567\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0659\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0632.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0658 - val_loss: 0.0567\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0648\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0631.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0647 - val_loss: 0.0567\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0616\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0629.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0616 - val_loss: 0.0566\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0549\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0595.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0553 - val_loss: 0.0566\n",
      "Epoch 331: Training Loss = 0.06219080463051796\n",
      "Epoch 332: Training Loss = 0.0629710778594017\n",
      "Epoch 333: Training Loss = 0.061529871076345444\n",
      "Epoch 334: Training Loss = 0.06029769778251648\n",
      "Epoch 335: Training Loss = 0.06270520389080048\n",
      "Epoch 336: Training Loss = 0.058847662061452866\n",
      "Epoch 337: Training Loss = 0.06317047029733658\n",
      "Epoch 338: Training Loss = 0.06313998252153397\n",
      "Epoch 339: Training Loss = 0.06289993971586227\n",
      "Epoch 340: Training Loss = 0.05948731675744057\n",
      "Starting training for epoch set: 341 to 350\n",
      "Loading model: model_epoch_10_loss_0.0595.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0591\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0618.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0594 - val_loss: 0.0566\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0604\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0605 - val_loss: 0.0565\n",
      "Epoch 3/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0626\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0624 - val_loss: 0.0565\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0607\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0607 - val_loss: 0.0565\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0619\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0622.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0619 - val_loss: 0.0565\n",
      "Epoch 6/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0593\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0609.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0594 - val_loss: 0.0565\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0631\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0630 - val_loss: 0.0565\n",
      "Epoch 8/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0638\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0635 - val_loss: 0.0565\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0604\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0587.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0603 - val_loss: 0.0566\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0602\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0587.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0601 - val_loss: 0.0566\n",
      "Epoch 341: Training Loss = 0.061759401112794876\n",
      "Epoch 342: Training Loss = 0.06110060214996338\n",
      "Epoch 343: Training Loss = 0.060637619346380234\n",
      "Epoch 344: Training Loss = 0.06109819561243057\n",
      "Epoch 345: Training Loss = 0.06221664324402809\n",
      "Epoch 346: Training Loss = 0.06085800752043724\n",
      "Epoch 347: Training Loss = 0.06104251369833946\n",
      "Epoch 348: Training Loss = 0.06148989871144295\n",
      "Epoch 349: Training Loss = 0.05865359678864479\n",
      "Epoch 350: Training Loss = 0.05870433896780014\n",
      "Starting training for epoch set: 351 to 360\n",
      "Loading model: model_epoch_10_loss_0.0587.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0628\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0627 - val_loss: 0.0566\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0600\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0601 - val_loss: 0.0566\n",
      "Epoch 3/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0632\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0591.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0630 - val_loss: 0.0566\n",
      "Epoch 4/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0636\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0634 - val_loss: 0.0566\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0607\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0587.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0607 - val_loss: 0.0567\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0569\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0570 - val_loss: 0.0566\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0651\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0621.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0649 - val_loss: 0.0566\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0639\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0638 - val_loss: 0.0567\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0645\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0643 - val_loss: 0.0567\n",
      "Epoch 10/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0630\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0629 - val_loss: 0.0567\n",
      "Epoch 351: Training Loss = 0.06119820848107338\n",
      "Epoch 352: Training Loss = 0.06119406968355179\n",
      "Epoch 353: Training Loss = 0.05909615382552147\n",
      "Epoch 354: Training Loss = 0.06140603497624397\n",
      "Epoch 355: Training Loss = 0.05870433151721954\n",
      "Epoch 356: Training Loss = 0.06104040890932083\n",
      "Epoch 357: Training Loss = 0.06214752793312073\n",
      "Epoch 358: Training Loss = 0.059905171394348145\n",
      "Epoch 359: Training Loss = 0.06171991303563118\n",
      "Epoch 360: Training Loss = 0.06203368678689003\n",
      "Starting training for epoch set: 361 to 370\n",
      "Loading model: model_epoch_10_loss_0.0620.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0630\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0619.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0628 - val_loss: 0.0566\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0597\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0595.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0597 - val_loss: 0.0566\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0628\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0630.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0628 - val_loss: 0.0566\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0603\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0620.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0603 - val_loss: 0.0566\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0597\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0596.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0597 - val_loss: 0.0566\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0590\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0591 - val_loss: 0.0566\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0598\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0602.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0598 - val_loss: 0.0566\n",
      "Epoch 8/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0620\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0605.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0620 - val_loss: 0.0566\n",
      "Epoch 9/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0561\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0603.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0565 - val_loss: 0.0566\n",
      "Epoch 10/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0577\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0580 - val_loss: 0.0566\n",
      "Epoch 361: Training Loss = 0.061949871480464935\n",
      "Epoch 362: Training Loss = 0.0594710148870945\n",
      "Epoch 363: Training Loss = 0.06302046775817871\n",
      "Epoch 364: Training Loss = 0.06199325621128082\n",
      "Epoch 365: Training Loss = 0.05964195355772972\n",
      "Epoch 366: Training Loss = 0.0607462003827095\n",
      "Epoch 367: Training Loss = 0.06017931550741196\n",
      "Epoch 368: Training Loss = 0.0605301670730114\n",
      "Epoch 369: Training Loss = 0.06034254655241966\n",
      "Epoch 370: Training Loss = 0.06154538318514824\n",
      "Starting training for epoch set: 371 to 380\n",
      "Loading model: model_epoch_10_loss_0.0615.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0615\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0606.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0614 - val_loss: 0.0566\n",
      "Epoch 2/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0568\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0572 - val_loss: 0.0566\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0601\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0609.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0602 - val_loss: 0.0566\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0585\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0590.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0586 - val_loss: 0.0566\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0601\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0591.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0600 - val_loss: 0.0566\n",
      "Epoch 6/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0607\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0610.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0607 - val_loss: 0.0567\n",
      "Epoch 7/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0612\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0617.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0612 - val_loss: 0.0566\n",
      "Epoch 8/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0602\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0625.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0604 - val_loss: 0.0566\n",
      "Epoch 9/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0601\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0616.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0602 - val_loss: 0.0565\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0680\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0633.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0678 - val_loss: 0.0565\n",
      "Epoch 371: Training Loss = 0.06061919406056404\n",
      "Epoch 372: Training Loss = 0.05992823466658592\n",
      "Epoch 373: Training Loss = 0.06093533709645271\n",
      "Epoch 374: Training Loss = 0.05901041626930237\n",
      "Epoch 375: Training Loss = 0.059134114533662796\n",
      "Epoch 376: Training Loss = 0.06098107621073723\n",
      "Epoch 377: Training Loss = 0.06173113361001015\n",
      "Epoch 378: Training Loss = 0.0625390037894249\n",
      "Epoch 379: Training Loss = 0.061616215854883194\n",
      "Epoch 380: Training Loss = 0.06328098475933075\n",
      "Starting training for epoch set: 381 to 390\n",
      "Loading model: model_epoch_10_loss_0.0633.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0598\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0612.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0598 - val_loss: 0.0565\n",
      "Epoch 2/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0628\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0609.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0627 - val_loss: 0.0565\n",
      "Epoch 3/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0585\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0589.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0585 - val_loss: 0.0565\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0584\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0596.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0585 - val_loss: 0.0565\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0552\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0583.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0553 - val_loss: 0.0565\n",
      "Epoch 6/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0607\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0600.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0607 - val_loss: 0.0565\n",
      "Epoch 7/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0622\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0613.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0622 - val_loss: 0.0565\n",
      "Epoch 8/10\n",
      "\u001b[1m35/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0580\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0592.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0581 - val_loss: 0.0565\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0633\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0614.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0633 - val_loss: 0.0565\n",
      "Epoch 10/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0608\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0611.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0608 - val_loss: 0.0565\n",
      "Epoch 381: Training Loss = 0.061239782720804214\n",
      "Epoch 382: Training Loss = 0.06089092418551445\n",
      "Epoch 383: Training Loss = 0.058919284492731094\n",
      "Epoch 384: Training Loss = 0.05959177762269974\n",
      "Epoch 385: Training Loss = 0.058328405022621155\n",
      "Epoch 386: Training Loss = 0.05996059253811836\n",
      "Epoch 387: Training Loss = 0.06127307191491127\n",
      "Epoch 388: Training Loss = 0.059242554008960724\n",
      "Epoch 389: Training Loss = 0.061361972242593765\n",
      "Epoch 390: Training Loss = 0.061082903295755386\n",
      "Starting training for epoch set: 391 to 400\n",
      "Loading model: model_epoch_10_loss_0.0611.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0585\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 0.0586 - val_loss: 0.0565\n",
      "Epoch 2/10\n",
      "\u001b[1m36/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0575\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0599.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0577 - val_loss: 0.0565\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0633\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0615.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0632 - val_loss: 0.0565\n",
      "Epoch 4/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0648\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0627.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0647 - val_loss: 0.0565\n",
      "Epoch 5/10\n",
      "\u001b[1m37/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0607\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0607.keras\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 0.0607 - val_loss: 0.0565\n",
      "Epoch 6/10\n",
      "\u001b[1m16/38\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0622"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdadelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model for a set number of epochs (e.g., 10)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the loss at the end of each epoch set\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 0\n",
    "while True:\n",
    "    print(f\"Starting training for epoch set: {epoch_count + 1} to {epoch_count + 10}\")\n",
    "    model = load_latest_checkpoint(save_dir)\n",
    "    if model is None:\n",
    "    # Build a new model if no checkpoint is found\n",
    "        model = build_model(optimizer='Adadelta')\n",
    "    # Train the model for a set number of epochs (e.g., 10)\n",
    "    history = model.fit(\n",
    "        trainX, trainY, \n",
    "        batch_size=16, \n",
    "        epochs=10, \n",
    "        verbose=1, \n",
    "        validation_data=(validX, validY),\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    # Print the loss at the end of each epoch set\n",
    "    for epoch, loss in enumerate(history.history['loss'], start=1):\n",
    "        print(f\"Epoch {epoch_count + epoch}: Training Loss = {loss}\")\n",
    "\n",
    "    epoch_count += 10\n",
    "\n",
    "    # Keep only the 5 most recent model files\n",
    "    model_files = sorted(os.listdir(save_dir), key=lambda x: os.path.getmtime(os.path.join(save_dir, x)))\n",
    "    if len(model_files) > 5:\n",
    "        for old_file in model_files[:-5]:\n",
    "            os.remove(os.path.join(save_dir, old_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: model_epoch_05_loss_0.0607.keras\n"
     ]
    }
   ],
   "source": [
    "model_latest = load_latest_checkpoint(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_latest = model_latest.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAADZcUlEQVR4nOxdd3wT5R9+ku5doIMCpWVT9ihbZG+QpYIiQygylK2MHypLQFGWIijKEBEEFVCGDNlbtixBoFBG2dBSupP7/fH2kktySe4ud8klvM/nc5+0N957b77PPd+lYRiGAQUFBQUFBQWFh0Dr6g5QUFBQUFBQUMgJSm4oKCgoKCgoPAqU3FBQUFBQUFB4FCi5oaCgoKCgoPAoUHJDQUFBQUFB4VGg5IaCgoKCgoLCo0DJDQUFBQUFBYVHgZIbCgoKCgoKCo8CJTcUFBQUFBQUHgVKbig8CsuXL4dGozFM3t7eKFGiBN5++23cvn3bKX2Ij49Hv379DP/v2bMHGo0Ge/bsEdXOoUOHMHnyZDx9+lTQ+pMnTzY5dvPp+vXrhnU1Gg0mT54sqj9iMGPGDGzYsEHUNuy14/ZTCNjj5mLhwoVYvny5qHaEIj09HdOnT0diYiJCQ0Ph5+eH+Ph49O/fHydPnlRkn+6OVatWYd68ebzLlL4XKV5MeLu6AxQUSmDZsmWoWLEisrKysG/fPsycORN79+7F2bNnERQU5NS+1KpVC4cPH0alSpVEbXfo0CFMmTIF/fr1Q3h4uODttm7dirCwMIv5MTExovbvCGbMmIFXX30VXbp0EbxNhw4dcPjwYdH9TEpKQtu2bU3mLVy4EBERESYkUw5cvXoVrVu3xv379zF48GBMmTIFwcHBuH79OtauXYvatWvj6dOnvOf/RcaqVatw7tw5jBw50mLZ4cOHUaJECed3isKjQckNhUeiSpUqSExMBAA0a9YMOp0O06ZNw4YNG9CrVy/ebTIzMxEYGCh7X0JDQ1G/fn3Z27WG2rVrIyIiwmn7cxRZWVnw9/dHZGQkIiMjRW9fokQJpwyOOp0OXbt2xcOHD3H48GFUqVLFsKxJkybo27cv/vzzT/j4+CjeF0+CM58NihcH1CxF8UKAfYHeuHEDANCvXz8EBwfj7NmzaN26NUJCQtCiRQsAQG5uLj755BNUrFgRfn5+iIyMxNtvv40HDx6YtJmXl4exY8eiaNGiCAwMxEsvvYS///7bYt/WzFJHjx5Fp06dUKRIEfj7+6NMmTKGL9vJkyfjgw8+AACUKlXKYFoSa9oSg7t372LQoEEoUaIEfH19UapUKUyZMgX5+fkm6+Xk5GDq1KlISEiAv78/ihQpgmbNmuHQoUMAiJnh+fPn+OGHHwz9btq0KQCj6Wn79u3o378/IiMjERgYiJycHKtmqa1bt6JFixYICwtDYGAgEhISMHPmTMNyc7NUfHw8zp8/j7179xr2Hx8fj4yMDISHh2PQoEEWx379+nV4eXnh888/t3p+NmzYgLNnz2LChAkmxIaLdu3amRDkAwcOoEWLFggJCUFgYCAaNmyIzZs3m2zDHvfu3bsxZMgQREREoEiRIujWrRvu3Lljsu6uXbvQtGlTFClSBAEBAShZsiS6d++OzMxMANbvtevXr0Oj0ZiY6thn4N9//0WbNm0QFBSEmJgYfPrppwCAI0eO4KWXXkJQUBDKly+PH374gbffO3bswNtvv43ChQsjKCgInTp1wrVr1wzrNW3aFJs3b8aNGzdMzKQs+MxS586dQ+fOnVGoUCH4+/ujRo0aFvtnj3X16tWYOHEiihUrhtDQULRs2RKXLl3ivT4ULw6ockPxQuDKlSsAYKIM5Obm4pVXXsGgQYMwfvx45OfnQ6/Xo3Pnzti/fz/Gjh2Lhg0b4saNG5g0aRKaNm2K48ePIyAgAAAwcOBArFixAu+//z5atWqFc+fOoVu3bnj27Jnd/mzbtg2dOnVCQkIC5syZg5IlS+L69evYvn07AGJqefz4Mb766iusW7fOYKoRYtrS6XQWhESj0cDLy8vqNnfv3kXdunWh1Wrx8ccfo0yZMjh8+DA++eQTXL9+HcuWLQMA5Ofno127dti/fz9GjhyJ5s2bIz8/H0eOHEFKSgoaNmyIw4cPo3nz5mjWrBk++ugjAES94qJ///7o0KEDfvzxRzx//tyq2rFkyRIMHDgQTZo0wTfffIOoqChcvnwZ586ds3os69evx6uvvoqwsDAsXLgQAODn54fg4GD0798fixcvxqxZs0xMRwsXLoSvry/69+9vtV322gg1te3duxetWrVCtWrVsGTJEvj5+WHhwoXo1KkTVq9ejR49episn5SUhA4dOmDVqlW4efMmPvjgA7z11lvYtWsXAEJQOnTogMaNG2Pp0qUIDw/H7du3sXXrVuTm5kpSHfPy8tCtWzcMHjwYH3zwAVatWoUJEyYgPT0dv/32G8aNG4cSJUrgq6++Qr9+/VClShXUrl3bpI0BAwagVatWhn5/+OGHaNq0Kf755x+Eh4dj4cKFeOedd3D16lWsX7/ebp8uXbqEhg0bIioqCl9++SWKFCmClStXol+/frh37x7Gjh1rsv7//vc/NGrUCN9//z3S09Mxbtw4dOrUCRcvXrR5z1N4OBgKCg/CsmXLGADMkSNHmLy8PObZs2fMpk2bmMjISCYkJIS5e/cuwzAM07dvXwYAs3TpUpPtV69ezQBgfvvtN5P5x44dYwAwCxcuZBiGYS5evMgAYEaNGmWy3k8//cQAYPr27WuYt3v3bgYAs3v3bsO8MmXKMGXKlGGysrKsHsvnn3/OAGCSk5MFHfukSZMYALxTmTJlTNYFwEyaNMnw/6BBg5jg4GDmxo0bJut98cUXDADm/PnzDMMwzIoVKxgAzHfffWezL0FBQSbngAV7ffr06WN1GXu8z549Y0JDQ5mXXnqJ0ev1do+bi8qVKzNNmjSxWPfq1auMVqtl5s6da5iXlZXFFClShHn77bdtHlPbtm0ZAEx2drbN9VjUr1+fiYqKYp49e2aYl5+fz1SpUoUpUaKE4ZjY4x46dKjJ9rNmzWIAMKmpqQzDMMyvv/7KAGBOnz5tdZ989xrDMExycjIDgFm2bJlhHvsMcO/1vLw8JjIykgHAnDx50jD/0aNHjJeXFzN69GjDPLbfXbt2NdnXwYMHGQDMJ598YpjXoUMHJi4ujrfP5vdiz549GT8/PyYlJcVkvXbt2jGBgYHM06dPTY61ffv2JuutXbuWAcAcPnyYd38ULwaoWYrCI1G/fn34+PggJCQEHTt2RNGiRfHnn38iOjraZL3u3bub/L9p0yaEh4ejU6dOyM/PN0w1atRA0aJFDXL/7t27AcDCf+f111+Ht7dtQfTy5cu4evUqBgwYAH9/fweP1BJ//fUXjh07ZjLZi1zatGkTmjVrhmLFipkcd7t27QAQFQIA/vzzT/j7+9tUOITA/Lzz4dChQ0hPT8fQoUMtoqGkonTp0ujYsSMWLlwIhmEAEGfXR48e4b333pNlHwDw/PlzHD16FK+++iqCg4MN8728vNC7d2/cunXLwnTyyiuvmPxfrVo1AEZTao0aNeDr64t33nkHP/zwg4npRyo0Gg3at29v+N/b2xtly5ZFTEwMatasaZhfuHBhREVFGfrChfkz0LBhQ8TFxRmeEbHYtWsXWrRogdjYWJP5/fr1Q2ZmJg4fPmwy3955o3gxQc1SFB6JFStWICEhAd7e3oiOjuaNwAkMDLQwl9y7dw9Pnz6Fr68vb7sPHz4EADx69AgAULRoUZPl3t7eKFKkiM2+sb47SjnBVq9eXbRD8b1797Bx40ar5iH2uB88eIBixYpBq3Xsu0hIRJRS52nEiBFo0aIFduzYgdatW+Prr79GgwYNUKtWLZvblSxZEgCQnJyMihUr2lz3yZMnYBiG9ziLFSsGwHgPsTC/b/z8/AAQh2sAKFOmDP766y/MmjUL7777Lp4/f47SpUtj+PDhGDFihM3+WENgYKAFwfb19UXhwoUt1vX19UV2drbFfPNngJ1nfnxC8ejRI1nPG8WLCUpuKDwSCQkJhmgpa+BTA1hnzq1bt/JuExISAsD4Qr179y6KFy9uWJ6fn2/3pc76/dy6dcvmes5EREQEqlWrhunTp/MuZweWyMhIHDhwAHq93iGCI0SJUeo8NW/eHFWqVMGCBQsQHByMkydPYuXKlXa3a9OmDRYvXowNGzZg/PjxNtctVKgQtFotUlNTLZaxTsJSItoaN26Mxo0bQ6fT4fjx4/jqq68wcuRIREdHo2fPngaikpOTY7IdS06VwN27d3nnlS1bVlJ7RYoUkf28Ubx4oGYpCgoOOnbsiEePHkGn0yExMdFiqlChAgAYon9++uknk+3Xrl1r4cxrjvLly6NMmTJYunSpxSDEhTO/QDt27Ihz586hTJkyvMfNkpt27dohOzvbboI8Pz8/h/vdsGFDhIWF4ZtvvjGYkITC3v6HDx+OzZs3Y8KECYiOjsZrr71mt83OnTujatWqmDlzplWH5m3btiEzMxNBQUGoV68e1q1bZ9IPvV6PlStXokSJEihfvryoY+LCy8sL9erVw9dffw0AhuSB8fHxAIB//vnHZP0//vhD8r7swfwZOHToEG7cuGF4RgBx90OLFi2wa9cui0ixFStWIDAwkIaOUwgCVW4oKDjo2bMnfvrpJ7Rv3x4jRoxA3bp14ePjg1u3bmH37t3o3LkzunbtioSEBLz11luYN28efHx80LJlS5w7dw5ffPGFhamLD19//TU6deqE+vXrY9SoUShZsiRSUlKwbds2w2BRtWpVAMD8+fPRt29f+Pj4oEKFCgb1yBpOnDjBm0SuUqVKVvs2depU7NixAw0bNsTw4cNRoUIFZGdn4/r169iyZQu++eYblChRAm+88QaWLVuGwYMH49KlS2jWrBn0ej2OHj2KhIQE9OzZ09D3PXv2YOPGjYiJiUFISIiBGApFcHAwZs+ejaSkJLRs2RIDBw5EdHQ0rly5gjNnzmDBggVWt61atSp+/vlnrFmzBqVLl4a/v7/hfALAW2+9hQkTJmDfvn348MMPrZohufDy8sL69evRunVrNGjQAEOGDEGzZs0QFBSEGzdu4Ndff8XGjRvx5MkTAMDMmTPRqlUrNGvWDO+//z58fX2xcOFCnDt3DqtXrxbtR/TNN99g165d6NChA0qWLIns7GwsXboUANCyZUsAxBzUsmVLzJw5E4UKFUJcXBx27tyJdevWidqXGBw/fhxJSUl47bXXcPPmTUycOBHFixfH0KFDDetUrVoV69atw6JFi1C7dm1otVqryuqkSZMMPmAff/wxChcujJ9++gmbN2+2iHKjoLAKFzs0U1DICjaC49ixYzbX69u3LxMUFMS7LC8vj/niiy+Y6tWrM/7+/kxwcDBTsWJFZtCgQcx///1nWC8nJ4cZM2YMExUVxfj7+zP169dnDh8+zMTFxdmNlmIYhjl8+DDTrl07JiwsjPHz82PKlCljEX01YcIEplixYoxWq+Vtgwtb0VIAmB07dhjWhVmECsMwzIMHD5jhw4czpUqVYnx8fJjChQsztWvXZiZOnMhkZGQY1svKymI+/vhjply5coyvry9TpEgRpnnz5syhQ4cM65w+fZpp1KgRExgYyAAwRC7Zuj7m0VIstmzZwjRp0oQJCgpiAgMDmUqVKjGfffaZxXFzcf36daZ169ZMSEgIA4A3Uqdfv36Mt7c3c+vWLavnlA9Pnz5lpk2bxtSqVYsJDg5mfHx8mJIlSzJvvfUWc/DgQZN19+/fzzRv3pwJCgpiAgICmPr16zMbN27kPW7zc2J+3xw+fJjp2rUrExcXx/j5+TFFihRhmjRpwvzxxx8m26WmpjKvvvoqU7hwYSYsLIx56623mOPHj/NGS/E9A02aNGEqV65sMT8uLo7p0KGDRb+3b9/O9O7dmwkPD2cCAgKY9u3bmzwnDMMwjx8/Zl599VUmPDyc0Wg0JteL7148e/Ys06lTJyYsLIzx9fVlqlevbtJ37vn55ZdfTObzRYZRvHjQMIxIvZeCgoLCzZGbm4v4+Hi89NJLWLt2rau745ZYvnw53n77bRw7dsyufxsFhbNBzVIUFBQvDB48eIBLly5h2bJluHfvnl3HYAoKCvcEJTcUFBQvDDZv3oy3334bMTExWLhwod3wbwoKCvcENUtRUFBQUFBQeBRoKDgFBQUFBQWFR4GSGwoKCgoKCgqPAiU3FBQUFBQUFB6FF86hWK/X486dOwgJCZGtGB8FBQUFBQWFsmAYBs+ePRNU3+6FIzd37tyxqDZLQUFBQUFB4R64efOm3YK6Lic3CxcuxOeff47U1FRUrlwZ8+bNQ+PGjXnX7devH3744QeL+ZUqVcL58+cF7Y9NXX/z5k1BafIpKCgoKCgoXI/09HTExsbaLUEDuJjcrFmzBiNHjsTChQvRqFEjfPvtt2jXrh0uXLiAkiVLWqw/f/58fPrpp4b/8/PzUb16dUFF71iwpqjQ0FBKbigoKCgoKNwMQlxKXJrnpl69eqhVqxYWLVpkmJeQkIAuXbpg5syZdrffsGEDunXrhuTkZMTFxQnaZ3p6OsLCwpCWlkbJDQUFBQUFhZtAzPjtsmip3NxcnDhxAq1btzaZ37p1axw6dEhQG0uWLEHLli1tEpucnBykp6ebTBQUFBQUFBSeC5eRm4cPH0Kn0yE6OtpkfnR0NO7evWt3+9TUVPz5559ISkqyud7MmTMRFhZmmKgzMQUFBQUFhWfD5XluzG1nDMMIsqctX74c4eHh6NKli831JkyYgLS0NMN08+ZNR7pLQUFBQUFBoXK4zKE4IiICXl5eFirN/fv3LdQcczAMg6VLl6J3797w9fW1ua6fnx/8/Pwc7i8FBQUFBQWFe8Blyo2vry9q166NHTt2mMzfsWMHGjZsaHPbvXv34sqVKxgwYICSXaSgoKCgoKBwQ7g0FHz06NHo3bs3EhMT0aBBAyxevBgpKSkYPHgwAGJSun37NlasWGGy3ZIlS1CvXj1UqVLFFd2moKCgoKCgUDFcSm569OiBR48eYerUqUhNTUWVKlWwZcsWQ/RTamoqUlJSTLZJS0vDb7/9hvnz57uiyxQUFBQUFBQqh0vz3LgCNM8NBQUFBQWF+8Et8txQUFBQUFBQUCgBSm4oKCgoKCgoPAqU3FBQUFBQUFB4FCi5UTMyM13dAwoKCgoKCrcDJTdqRHY2MGQIEBwMLF7s6t5QUFBQUFC4FSi5URuuXQMaNQK++QZgGODIEVf3iIKCgoKCwq1AyY2asH49UKsWcPKkcV5Wluv6Q0FBQUFB4Yag5EYNyM0FRo8GunUD0tKAhg2BDz8kyyi5oaCgoKCgEAVKblyNmzeBpk2BuXPJ/2PGAHv2AOXKkf8pufEs5OcD6emu7gUFBQWFR4OSG1dCrwfatgUOHwbCwoANG4AvvgB8fICAALIOJTeehddeA2JigFu3XN0TCgoKCo8FJTeuRGYmcOEC+fvoUaBzZ+MySm48E6dOkeu+e7ere0JBQUHhsaDkxpXgEhfWDMWCkhvPRG4u+eU6jVNQUFBQyApKblyJ7Gzy6+sLaM0uBSU3nom8PPJLyQ0FBQWFYqDkxpVgiYu/v+UySm48E6xyc+oU8bmioPAUTJkCNGsG5OS4uicUFJTcuBSscsMSGS4oufFMsMrNs2fA1auu7QsFhZxYvJhEev7zj6t7QkFByY1LYUu5CQw0XYfCM8AqNwBw4oTr+kFBITdYxYYl8I4iP59kaaegkABKblwJIcpNTg41X7gS+fnArl3A8+eOt6XXAzqd8X/qd0PhSWDJDZfAS0VyMhAZCYwY4XhbFC8kKLlxJVhyY8vnhrsehfOxciXQogXwySeOt2X+RUvJDYUnQU7l5pdfgKdPgb17HW+L4oUEJTeuBGtysqXcACQvCoVrkJJCfm/ccLwt8y/akyep7E7hGdDrjaRGDnKzfbt8bVG8kKDkxpWwpdx4eZFMxYDr/G7OngWWLXuxzWLs16gc14D7ovb2Bp48Aa5fd7xdCgpXg3tvO2qWyswEDhyQpy2KFxaU3LgStpQb7nxXkBuGIYU8+/cHli51/v7VAjnJDfui1miAatXI39Q0ReEJ4IZ/O6q27N8vr/8OxQsJSm5cCVvKDeBacnPqFHDlCvl72rQXN3eFEsqNjw9Quzb5m5IbCk+AnORmxw752qJ4YUHJjSuhZuXm11+Nf6ekAEuWOL8PaoASyo2vL1CrFvmbkhsKTwCX3DiqtnDJDVVuKCSCkhtXQq3KDcOQaAUAaN6c/E6f/mLm3FFKuWHJzYkT1KmYwv3BJSGOqC1375omAaTkhkIiKLlxJWwl8QNcR27++YeYpPz9gbVrgdhY4M4d4NtvndsPufHgATBjBnmBCoVSyk3VqsRp/MED4PZtx9umoHAl5DJL/fUX+Y2KcrwtihcalNy4EraS+HHnO5vcsCaptm2BIkWAjz4i/8+cKU8yO1fh44+BiROBr78Wvg370pYjHJ+r3AQEAJUqkf+paYpCKLKz1Zn3Si6zFBsC3r69sS2qbFJIACU3roQalRuuSeq118hvv35A6dLA/fvAggXO64uc0OmA9evJ30+fCt9OKeUGoE7FFOKQkQGUKQPUr+/qnlhCDuWGYYz+Nh06GOdxs3pTUAgEJTeuhBqVm/PngUuXyADcsSOZ5+NDVA8AmDULSE93Xn/kwpEjwL175G8xX75K+dwA1KmYQhx27SLm4TNn1Jd7Sg6fm3PniMk4IIBUF3e0PYoXGpTcuBL2HIpdUTyTNUm1aQOEhhrn9+oFlC8PPH4MzJ/vvP7IBVa1AcSFtbPrylHjy1y5oeSGQgy2bTP+rbYBXw7lhlVtXn4ZCAkxzqdOxRQSQMmNK6HGUHCW3Lz6qul8b29g8mTy9+zZJLuuu4BhgHXrjP9LITeA474O7EuaVW6qVycJ/W7fNqpKFBTWsHWr8W+1Dfhy+Nyw5KZ1a+Mz4kh7FC80KLlxJdQWCn7xIjFL+fgAr7xiubxHD6ByZSAtDZgzxzl9kgP//EOqDLOQSm4cvQ7sFy2r3AQHAxUqkL+pekNhC1euANeuGf9X24DvqHKTnW0sktmqFSH93t7S26N44UHJjSshVLlxVuFMVrVp1QoID7dcrtUCU6eSv+fNAx4+dE6/HAWr2rAvSyk+N4Dj5MbcLAVQp2IKYeCqNoDnkZtDh8jzVbQoUKUKmceqN2o7Vgq3ACU3roTalBtrJikuunYFatYkkRuff+6cfjkK1t+mdWvy62rlhiu5U78bCiEwJzdqUzO4BEQKGWFDwFnVBjB+BKjtWCncApTcuBJq8rm5fJmYb7y9gc6dra+n0QAffkj+5pZoUCuuXCHVzb28jKTNVeSGT7nhZiqmoOBDTg6we7fpPLWpGY4qN1x/Gxbsc6K2Y6VwC1By40qoSblhiUqLFkDhwrbXbdKE/F67Rvxv1AxWtWnaFIiJIX9LJTeOmgf5lJsaNcjvjRvAo0eOtU/hmThwgNx7RYsan021DfiOkJsHD0ihXgBo2dI4n5qlKBwAJTeuhJqUGyEmKRZFipCSDIBpHRg1giU33boBfn7kbzX53ISHk8RsgPEFT0HBBRsC3qaN8R5W24DvSLTUzp0korFaNULgWFCzFIUDoOTGlVCLcnP1KhlYvbyALl2EbcMqDqdPK9QpGZCaChw+TP7u3Nk4MKjJ5wagfjfuDoYBVqxQ7llg/W3atFGvmuFIEj/WJNWqlel8apaicACU3LgSaim/wKo2zZoBERHCtnEHcrNhA/mtVw8oXlw8udHpTFO/K6HcADRiyt1x9izQty8wYID8bd+5Q9rXaMjgr1Y1Q6pZimGMzsRcfxtAvUSOwi1AyY0roZbyC2JMUixq1iS/aiY3XJMUYCSRQsmN+XpKKzfUqdg9wfpK3b8vf9usSSoxkXx4qFXNkGqWunQJuHWLfHg0bmy6TK1EjsIt4HJys3DhQpQqVQr+/v6oXbs29u/fb3P9nJwcTJw4EXFxcfDz80OZMmWwdOlSJ/VWRuh0xpeAK5Wb69eB48dJDhuhJinAqNycO6fOl8+TJ8YIk65dya9Ynxu5yY015YYlileuqN9Bm8IS7HVV4jllyU3btuTXHciNmPcBa5Jq3NjyI0+tx0rhFnApuVmzZg1GjhyJiRMn4tSpU2jcuDHatWuHlJQUq9u8/vrr2LlzJ5YsWYJLly5h9erVqFixohN7LRO4LwNXKjesT0rdukB0tPDt4uNJ7ancXJLZWG3YtAnIzycZlcuVI/PEmqWcpdxERAAlS5K/1ayEUfBDKXKj0xlNNm3akF+1DvhSfW6s+dsA1CxF4RBcSm7mzJmDAQMGICkpCQkJCZg3bx5iY2OxaNEi3vW3bt2KvXv3YsuWLWjZsiXi4+NRt25dNGzY0Mk9lwFc9cCVhTPZ8GahvjYsNBp1+92Ym6QA00gThrHfhrOUG4A6FbszuORGyH0lFMePEwUyLIz4jQHqJTdSzVJsWRT2/ueCmqUoHIDLyE1ubi5OnDiB1mZOZK1bt8ahQ4d4t/njjz+QmJiIWbNmoXjx4ihfvjzef/99ZNkYdHJycpCenm4yqQJsn729jWUBzOEM5cZexJYtqJXcZGYaI0xYkxRgeoxCXsBKkRtz5QYwOhVTvxv3A3tdGUZe0sHewy1bGt8RalUzpJqlbJnmXU3k9u4l+bHOn3fN/ikcgsvIzcOHD6HT6RBtZgqJjo7G3bt3ebe5du0aDhw4gHPnzmH9+vWYN28efv31V7z77rtW9zNz5kyEhYUZplg2P4urIYRUOKO2lCPkRq1Oxdu2ESISH28kYIBRuQGE+d2Ykxu5kvjxKTdsAc3r1x3bB4XzwR185fwQMfe3AdSrZjhKbvieCVcTuZUrCcFZtsw1+6dwCC53KNawdUQKwDCMxTwWer0eGo0GP/30E+rWrYv27dtjzpw5WL58uVX1ZsKECUhLSzNMN2/elP0YJMFeAj/usuxseeVuLuRSbpTqnxSwhTK7djXWqQFMX6BC/G6caZZiz7/aBi0K+1CC3Dx5Ahw9Sv5m/W0A16sZ1iC1tpStZ8LVRI59N54965r9UzgEl5GbiIgIeHl5Wag09+/ft1BzWMTExKB48eIICwszzEtISADDMLh16xbvNn5+fggNDTWZVAExyg13fbnBDuBSyE2lSuTr6skTwIYTuFORn0+ciQFTkxRAiA77wnQFubHmUAyod9CisA8lyM1ffwF6PXnGuGqzWu8TR5UbNT4T7H7PnXPN/ikcgsvIja+vL2rXro0drLd8AXbs2GHVQbhRo0a4c+cOMjIyDPMuX74MrVaLEiVKKNpf2SFGueGuLzccUW58fcnLF1CPaerBA+DpU0Jk+O4jMblunKncuPpFTiEdSpAbblZiLtR6n0glN7ZMta42S7H7vXMHePzYNX2gkAyXmqVGjx6N77//HkuXLsXFixcxatQopKSkYPDgwQCISalPnz6G9d98800UKVIEb7/9Ni5cuIB9+/bhgw8+QP/+/RFgiySoEUJIhY8PKYkAqJPcAOpzKmaJb3Cw8dxxISbXDVVuKIRAbnLDMPz+NoB67xOp0VJqNktxj4OqN24Hl5KbHj16YN68eZg6dSpq1KiBffv2YcuWLYiLiwMApKammuS8CQ4Oxo4dO/D06VMkJiaiV69e6NSpE7788ktXHYJ02Cu9wELpiCl2kOc624qBmskNH8TkuqHKDYUQcAdfOZ7T8+eB27fJu8E8a6+r1QxrkJrnRs0Oxdznn/rduB2sxCA7D0OHDsXQoUN5ly1fvtxiXsWKFS1MWW4Je6UXWAQEkAFbrcqN2iKm1ExuqHLjmZBbuWFVm6ZNrWftVZvjuRSzFLd2G1VuKGSGy6OlXlioTbmRSm6qVye/168TXxdXwx65UbvPjdoGLQr7kJvc/PMP+TVXbQD1kmApZinuva5Gws/dL1Vu3A6U3LgKYpQbQL3kJjyc5JMB1KHeKKHcyJUp2pZy42oJnkI65CY37H0XEmK5zNUDvjVIUW6466nRLGWu3Kgp3QWFXVBy4yoIJRVqJzeAuvxuhJIbMQ7F4eHk19EkftTnxjMhN7lxx/vEnNwIIQLcY7Cl3KjBLJWWRqqXU7gNKLlxFYSEgnOXU3IjDEooN4UKkV9n+dzQL0T3glLkxp0UPvP+5OcL38bLiz+y0dVEzny/1O/GrUDJjasglFQoXTxTDnKjJqdiJcgNq9w4w+eGYYxOlhTuAbnJja3cL65WM6zB/HkS0j9bzwPgeiLHHlPp0uSX+t24FSi5cRU8Ubm5cMH1X5RKOBQ7k9xw16NwD1CzlDLkxtVEju0fW9SWKjduBUpuXAWxPjdKFc+Ug9zExhLTTV4eITiuhJI+N47W+BJilgLUN3BR2MaLTm70ekszlJD+2TK/Aa4/Vna/tWqRX6rcuBUouZELOTnAwYPArl3C1vck5UajMao3p0453CWHoKRZCnCsxpeQhGWA+kwOFLbxopMbvr4IuYdtmd8A15ulzJWbixeF+RJRqAKU3MiFBw+Al16yTJduDWqJlnKkcCYXanEqVoLccAq1OnQdbCk3Gg3gXZBTU00DF4V9yJ2h2N3IDfdZ0mjIryeZpSpUIL6POTnAlSuu6QuFaFByIxeCgshvXp6wh9GTlBvAfciNFJ+boCAj8XDkOgh9matp4KKwD2cqN65WM/jAfZbY96AYs5QanweGMe43IACoXJn8Tf1u3AaU3MgF9qEGgOfP7a+vFuVGLnLDjZhyZSizEj43fn7yXAdbyg1AyY27whVmKTWZLtnnxMfH+Hy5e7SUeYLBqlXJ39Tvxm1AyY1c8PU1PoxCyI0ayi/o9cYXh9TCmSwqViTnID2dlGJwFZQwS/n5GUPyHXHsVvOXKoV0ODPPjRrvEe47hO2zu5uluOfX1xeoUoX8TcmN24CSGznBqjfsAGsLaii/wB3gHVVufHyMLwBXmqbYc8+Xuh6QTm4cvQ4MY3RGpMqNZ+FFdyjmPidi+mcvWsqVyo05uWGVG2qWchtQciMnWHLjLsoN1zTjKLkB1BExpYTPjRzkxl4dHe58NQ1cFPbhiiR+arpHuM+JGOXGXrSUGpQbrZZkT2Y/3K5cUS4tB4WsoORGToghN2pQbtg+aLVGh1lHoAanYrX63NirgAyo05+Cwj6ockN+uaZ5Oc1SrlRu2D5ERwMREUSBvXjR+f2hEA1KbuQEO6C6i0Mxtw9sCKcjcHUZBoZRzufG0etgLnPzQY2RMBT28aJHS3F9bqSYpdT4PHCffYC8H6nfjVuBkhs5IcbnRg2h4HJFSrGoVo383rwJPHokT5tikJ1NnKQBdZMbayqZGr/KKexDTnLDMO5XW0qqWcodHIq5faN+N24FSm7khBSzlCsLZ8pNbkJDgTJlyN+uUG+4pJI9b+Zwtc+Nj491lYySG/eEnOTGnm+WGu8RpcmNGsxSAFVu3AyU3MgJKQ7FnqTcAMYKunfuyNemULDkJjCQOAHywVU+N/Ze5Nxlahq4KOxDzgzF9syXarxHuD43YvpnL++TGqKlqHLjtqDkRk4I9blhGHUUzlSC3LDnQIhpTm48e2baBz44apaSeh3svcgBdQ5cFPYhp3LjzuTG081SbJbiO3eAx4+d3ycKUaDkRk4I9bnhDqxqyHOjBLkRol7JDXvOxIDjSfyockPBBTdNP0AGYp1OentsWxoNv/qoxntE6SR+rlRuuMlNQ0OBuDjyN1VvVA9KbuSEULOUmPwy7maWEuNULTeEkBs1+NxYgxqdRSlsQ6ezLDeiZP0x9v7R6YzO866Go0n81HisXFMbF7QMg9uAkhs5IZTcsC8/rdb2YAeYDqpy12xS0iylduVGjT43agzzpbANvmslR/0xe2oGd11XQ+k8N0LbkxPW+sY6FVPlRvWg5EZOiFVuhOSXYQdVc/lbDrD9cLSuFBeu9LmR0yzFMK5Tbii5cR9wr5W24HXqjMrx5vt2JRz1ubGX1JK7rrNg7TpQ5cZtQMmNnBA6sAstvQCY+uTIbZp6Ec1SQskN9+VMo6Xkxf79QIUKwLZtru6J4+Beq9BQ8vuikRupSfzsqVRc0qMWcsNVbuRW0ilkBSU3ckKscmPPmRggDxer7rgDuXEXs5Q9csNdTpUbebFiBXD5MvDHH67uiePgqg9y+MfZIzdeXkaFSC33iVLRUtxjVYtZqmJFkoQzLQ24dcu5faIQBUpu5IRYnxshpEKjUc6p2NNCwcU6FNv68pKb3FDlxoh//iG/Qvye1A7udXUGuQHU55ullM8Nd5mzj9W8/AK3P+XLk7+p342qQcmNnFBCueGu5w7kRkwiQ7khRrnR64H8fOvrsS83Ly8yUeVGHuh0xkFBSMSa2uEKcqO2qDqloqUA1xE5W32jfjduAUpu5IRQ1UIsqXAncqN25Yb7JWZrcDX/cnM0iZ87DlpK4OpV4zn0BOWG6zfibHKjFhKsVJ4b7jK1mKUAWobBTUDJjZwQa5byZOXG08iNM5L4qc3coARYkxRAlRtb7bmTwqdUtBR3mRqVG2qWUjUouZETUkLBhUCp4pkvokOxl5exKrcU5YaapRwDl9x4gnLjSrOUWu4TR2tLuatyc/GiehIpUliAkhs5wSU3tpxVPVm5UbtZChCWyE9ucuOOg5YSoOTGNsQM+Gq5T5SKluIuU5NyExVFfnNyPEN99FBQciMn2EFVp7N900v1uZG7eKbSDsXO/qoRS26ocuN8nDlj/NsTBgYaLaWsz42rjtVatJT5PE8g6B4KSm7kBDuwA7bNMmpRbpQsnAkoUw/LFtRMbtT8leospKUB168b//eEgYFGSykbLaVGsxT3A8UTCLqHgpIbOeHlZRwMbZEbT46WCggwJh10tmmK3V9IiO31hBTPtEVupGQmpcqNpQOmJ5EbZyXx4y5Ty33yopmlNBrhyUApXAZKbuSGEKditSg3SpAbrdboAO1sp2Jn+Nxwl4mBmr9SnQXWJMX1WXB3UIdix5P4uVu0FEDJjRuAkhu5IcShVm3KjZyFMwHXORU7wywFSLsOQpQbtflSyA3WmbhOHfLrSuVGpwMeP3a8HUpulKstxV2mJrMUIEz9pXApKLmRG2KUG7WQGzmVG8A1uW5yc40vQCXIjY8PMTsC0hy73XHQkhssualbl/y6cmDo2ROIjQUOH3asHUpuPNMsZcuhmDvfE0yrHgpKbuSGEHLjyeUXANfkuuESKa5jNx+k+NwAjl2HF93nRq83ZnRlyY0rB4ZTpwhJfftt+UK35YhqVLOpxhqUJDfULEUhES4nNwsXLkSpUqXg7++P2rVrY//+/VbX3bNnDzQajcX077//OrHHdiCG3HiqcuMKsxS7L+4L1hqk+NwAjiVTVPNXqjOQnEyuka8vUK0amWeveKmSYK/9pUvAlCnS23FlnhuhpprLl4ELF6T3yR6kJvFTsx8aJTduD5eSmzVr1mDkyJGYOHEiTp06hcaNG6Ndu3ZISUmxud2lS5eQmppqmMqVK+ekHgvAi+5QDLjGLCXU3waQZpYCqHLjCFiTVOXKptfIVcfKJbaffw4cOyatHbWbpXQ6oEEDklV36lTyv9zwRLMUJTduD5eSmzlz5mDAgAFISkpCQkIC5s2bh9jYWCxatMjmdlFRUShatKhh8mJ9IdQAd3Qo9iSzlFrJjZpf5M4AS26qVVNHEjR2vw0bEpPZ2287HgWnRnKTmUkcpxkGmDQJaN8eePBAev/4IDWJn5qd7KlDsdtDErnR6/W4fPkyDhw4gH379plMQpGbm4sTJ06gdevWJvNbt26NQ4cO2dy2Zs2aiImJQYsWLbB7924ph6AcqHLjWrOUEHLjCp8bNUvwzgBLbqpXNz0HrhgcGMZ4Db/5BoiMBM6fB6ZPF9+W2skNlzwGBADbtwM1awIHD0rvozletCR+AFVu3ADeYjc4cuQI3nzzTdy4cQOMmb1co9FAJ1D2fPjwIXQ6HaKjo03mR0dH4+7du7zbxMTEYPHixahduzZycnLw448/okWLFtizZw9efvll3m1ycnKQw7kB09PTBfVPMpTwuVGicCbDKG+WUrtyI9bnRmmzlNocReUEm+OmWjVjErScHNcoN/n5xtIgJUoAX38NvP46MHMm0K0bUKOG8LbUTm7Y+9jHB/j7b+C114B//wWaNAE++wwYPdqYdFMqpOS50emMJjI1OhTTaCm3h2jlZvDgwUhMTMS5c+fw+PFjPHnyxDA9lpA3QmP2YDEMYzGPRYUKFTBw4EDUqlULDRo0wMKFC9GhQwd88cUXVtufOXMmwsLCDFNsbKzoPoqCksqNnLWluC+LF025oWYp5yIjA7h6lfzNOhOz95wrBgfuPv39yYDfvTshPW+/LU4lcEWGYjGmH+59XKUK8S164w1CLN5/H+jaFXDkg49LUsSYpbjL1fhMUOXG7SGa3Pz333+YMWMGEhISEB4ebkIcwsLCBLcTEREBLy8vC5Xm/v37FmqOLdSvXx///fef1eUTJkxAWlqaYbp586bgtiXBXXxuuA8ldSi2BHUolg9s2YWYGGICAlzrs2BObgCi3hQuDJw+DcyaJbwttSs35u+a4GDgp5+IOc7XF/j9d2DuXMf7C4gzS3GXU7MUhQIQTW7q1auHK1euOLxjX19f1K5dGzt27DCZv2PHDjRs2FBwO6dOnUJMTIzV5X5+fggNDTWZFIW7+NxwX/C2Xi5S4OkOxTSJnzhwTVIsXCnrczNzsypxdDTw5Zfk76lTiQ+OEChFbuQiwXz3sUYDDBoEfPgh+f/GDfH9NG+f3YdQ5YbbdzWaaqlDsdtDtM/NsGHDMGbMGNy9exdVq1aFj9mNWY37ArOD0aNHo3fv3khMTESDBg2wePFipKSkYPDgwQCI6nL79m2sWLECADBv3jzEx8ejcuXKyM3NxcqVK/Hbb7/ht99+E3sYysFd8txw++Cozd0crjBLPXtmum9bEGIScaVyk5dHfKLkvi6uAjdSioUazFLmz9+bbwI//wxs2gRMnAhs2GC/LXdTbrhgC8w6MkBzt/XxEW+W8vIi9eisgSo3FBIhmtx0794dANC/f3/DPI1GY/CVEepQDAA9evTAo0ePMHXqVKSmpqJKlSrYsmUL4uLiAACpqakmOW9yc3Px/vvv4/bt2wgICEDlypWxefNmtG/fXuxhyA6GAZo3BxIDX0IvVEf1jOewOjSpofyCUnWlAM81SzkriR9AXuZyK2quAh+5ceXgYG3A12iAIUMIuRFqvubLUJydLZ2cikniJ1W5YSGHesZ1JtZoxJul7N3jri6/YI/cUIdi1UI0uUlOTpa1A0OHDsXQoUN5ly1fvtzk/7Fjx2Ls2LGy7l8uHDsG7NkD7EElfIHTqHToGt6aST4GC7gaATdKSQ1mKbn9bQDPN0sprdyw63sCuWEY0zBwFq5Ubmx9XHAJihDwKTfs9kKfb2vtWYNcyo0cBJOb4wYQb5ayd4+72ixlL1qKKjeqhWhyE2cyUlOwqF4dWL8e+GnOPWzcH4YLWaXxv/8B//sf8NJLQK9ewCuvAMUi8oxhqGpQbpQkN56m3CgdLcUlPrm59mtkuQNu3CDROD4+QIUKxvlqcCiWY8C3Rm6yspQjN1KjpcwhxwDNV2CW7Zst9UqsckPNUhQiISmJ39WrVzFs2DC0bNkSrVq1wvDhw3GVDfV8QeHnB3TpAvzy2TXcQzSWRIxD8+bk2T5wgKjdxYsDNRK9MB4zsQdNkOctUrnR6eR7yJUkN2rPc6PWwpnenG8NT3EqZlWbhATTgUINDsV8975YRYk7CHIrx0v9EHE35cbcfMO9v/PzrW+nZrOUTmf8AKUOxW4L0eRm27ZtqFSpEv7++29Uq1YNVapUwdGjR1G5cmWLyKcXEkFBCEM6+nv9gJ07gZQUUrqmbl1CdM6c9cJnGI9m2IMixf3QtSuwapXxWeKF+RehDQiuQ0iVG+c6FAt5mYvxWXA2li4lvihiwedvA6jDoZhPWXFEueG2qRZyY0u5kWOANm/f3LRqDUKiwrjLnfk8CAlTp8qN6iHaLDV+/HiMGjUKn376qcX8cePGoVWrVrJ1zi1hNrCXKEFyZb3/Pinpsn3VQ2wd+Se2oQ0ePIvChg0kKOOLL4A5c4CmTXna5BCQ3LQs/LE9FMePAw8fmk4PHgBPnhDhpHBhoFAh8stOUVFAqVJA6dJAqRsalIAXvJVUbtROblyh3Aj5Us3NVRe5uXcPGDAACAsDnj4Vty2fvw2gXrOUI8oNQO6RjAz1kBshyo0jBNOazw1gm9yIeR7stSU3xJAb6lCsWogmNxcvXsTatWst5vfv3x/z5s2To0/uDXZgz8y0sDlHRgK92j5CL/SBPqwQTv71GH/8AcyfD5w6BTRrRkxbn38OlC3LaVOjQYpfOSzO6YPva0Xg3kPbXcjIIJPt4uqt4I0sxJ25j9KtgVq1gJdfBho1ImOYQ2AJRmYmkaRshXrKBbWTG6FfqmpUbtjM42lpRLIXU6iWL8cNoF6zlFTlhr2u7qTcKOlzY69/ajZLmYe384EqN6qHaHITGRmJ06dPo1y5cibzT58+jaioKNk65rZgyQ1bnI8NH2ZR8NLTBvojMRFITASGDQMmTwa+/ZaoOJs3A++9R1JtHDsGLFoEbMq5CD28gIdA0aKkBE5MDBARYZwiI4lawxYCfvyYKDns36mpQHIycO0acP2aDrn5PriaXRxXdwA7dpBSM1ot+ch++WUy1atH9ieq8DpLMNhzYMMxNiuL9C0tzXpzfn6kieBgcjp5uZIzfW6USuLHXa4mcsP1ncrJsbynrSEzE2Czh1szS6lVucnLE0bMlTJLyZXYzlk+N2xbWi15WdjzD1RztBS3b9Ycoim5UT1Ek5uBAwfinXfewbVr19CwYUNoNBocOHAAn332GcaMGaNEH90L3Bf/8+eWAwHPyyYykmR/f/ddYr7680+SEX3+fK4vjheaYReGfBqPLqNL2xUA7EH/zRLcGTIV1xr1weV+M3D4MLBvH3DlClGRTp0i+wfI+yoykpAc7tS8OdCqFc/zz/VlyMgAgoJw8iQ5xitXjGTr8WNpH+6BgYTsFCpE6v916AC0eMYgGHC6zw3DEGJ29y6x3rDTgwfEPzg4uICYZXVFEB4j+O9QRGSRwCHeYBo1Vgbnkjk+wm4N58+TExQZSTIAc+FK5cZWKDj3eufk2I94cqXPjZqipbj99fGRj9y40ixlq2/UoVj1EE1uPvroI4SEhGD27NmYMGECAKBYsWKYPHkyhg8fLnsH3Q5aLXnBZWWRgZ2tpcPCRumFSpWALVuAbduAMWPI2BAWBvTrBwxe3wYVU7YDjfYDPqUd72ZuNkrgNkqUuIaXk4CkJDL/zh1g/35CdPbtAy5cIASLHbRZKwNAlJ6KFYHhw4E+fTgCjVYLBAWBef4cf23TY9aPwF9/We+LlxcQGsr/kcww5F3z/LnRWTozk0wPHgCXLwPffQf44hKaYg86/FoMHQKAMmVsHLxEs5TePxAXUQlHbrfFkYHAkSNEmBD2fltKfvqQH62W9LFyZVLPsHJlMhXXRiAcN6FVq3IjZtBmb5bq1S0ZsBocim0pN+x6ziY3cifxE3Kscio3ACE32dnua5YS0jeq3KgeosmNRqPBqFGjMGrUKDwrSHkfwqbxpiAIDiYvN75QaAFRSm3aAC1aAP/+S5x/AwMB7EolC+XKdcM+lGb9KFYM6NGDTAD5AHvwgCgT3OnqVWDNGtLHoUNJPp8BA4g5rUQJ4FfvXpiFQTjVl9T98vIibb7yClCkiKmjc0iI/WSurIUrI4Oc1owMkkT2zz+BzZsZJCf7YTvaYPsUYMQUo1gQFWWcIiOJKa9dbBCKcc+BjfPz761grPoZOHQIOHakGdJxHrgF4HvT1UNDiZoVHU1+IyPJuSP9ZZCx4S9kIBjPExJx574PHj0ixOi//8yz/J+ABnoUaqdD4SjjOSpdmqh6pUrZPk+KwFy5EQprkVKAes1S3t7kZmQYYX0zJyPu6HMjp0Mxt3/uHi0lhNxQh2LVQjS54YKSGisICiKMgI/cCCya6e1NvugNkDuRn8BQcC8voxnKHLNnA8uXA199RcxNs2cTc1p0NJCa9i0AINBfh6R3vDBqFBAfL727Gg0heVyLSNWqQPv2wJfT0vFvofrYjA7Y/PIsHDikxYMH5BLwH1MVtMfvSHq2Fu3zTdPLAOTdtiGrExbhHexpU5KzxBtByEAdv39Qf1RD1KtHRImiRe1czrx8wLc1+fvgYzDhhXD/PimWff688ffCBRKMxECLx+laPE43bWbJEmD0aGDCBGNZIKdAqnJji9yowaGY76JpNOSZyMoS1je1h4I72+cGEJZkUM3RUvZKLwBUuXEDCCI3tWrVws6dO1GoUCHUrFkTGhuf2SdPnpStc24LW0nspOaXcRG5sYXQUGKSeu89oqDMn08ck1NTgQivxximm4d3VzZHke5N5emzFWieZyAB/yLB+wre3/M50p8B168D9+9bTufPA0eOaLARr2Djs1cQUxLo25eoTl5exMS1ZAmD+/rVAACtlkGHDhp07AjUj7yKSt0qwDs4HJhpJ2SNC+6L2ccHGg0hgNHRRKHjIjexIR6fuIbHX/+Mx9Wa4vFj4NEj4KefgJ07gZkzCaGcORPo3du6v2tuLkkPULSoDMFqUpQbbtkFW8qN2sxSABm4srKEDVxykhshyeO4y+RSbnQ68VFw5u2b+9zY6587OBTbqrtHyY3qIYjcdO7cGX4FF7Nz5842yQ0FbOd5EVs0k4VS5EaGwplaLXHq7dABuHiRqDgtPumOwL/3ANqaDrdvF9xIKY0GoaH84ymLi3vuYUmzH7ECfZCaGoVPPwU+/dRojQA0iMEdDMR3SDo7GrGVCmSSZC0AnfhrICRvBrvYX4uiuIeiRR8DLxnn9+sH/PEH8cW6epX8//XXhFBWrkzcW1hH8FOngAsXGOTlaeDvz6BcOQ3KlwfKlyeOzOXLAzVqiKgOIEW5SU8noXqAadkFFmosnMlCDPGSk9wIvU/ElF8QotwA4qLguOAjT2LMUmpUbqhDsUdAELmZNGmS4e/Jkycr1RfPga3CkWKLZrJQoXLDh4QEMuGrghewM0owiAkDB5BQxQtf4APMwP+wcW02vl+qxbZthNi0agUM7v0cnfrEwQf5QJnxxg2510BM1Wcz5cYmrHyVazRA585A27aE0HzyCUkT0LChtYZI37KzNTh7Fjh71nRpoULAO++QCL3YWDv9l6LcsNtoNPz3utqVG8D5yo1QciO3csOuJ4Xc8KkcQsgXdSimUBiiBevSpUvj0aNHFvOfPn2K0qUdj+LxCLwgZimbcGYJBpHkhn0x+SIP3Tvm4M8/SZTYrVvA9u1At9YZhNgApi849hqwIVxCwa7LOqvagp0vVT8/YOxYEiU2YICxudhY4qz98cfA+nUMrodVRx68cWX6GmzeTHyhhgwh4ftRUURU+ewz4qDcsyeJ/LIKKcoN17eM75hd+eVrTz1Vg3IjpHq8oz433t5Gm6VUkmnL58bdzVLUoditIdqh+Pr169DpdBbzc3JycOvWLVk65fawRW4EOhRbwN3IjTOLZ4olNzzhviYO01w/Au7AzL1mmZnCTXpCimayEPgyL1oU+P57YMYM4ipRpAhn4ZWrQBrxdykTch9l2hPHaxY6HUkUOW8esHs3iXpbs4YkbBw5Euje3ayrUpQbe/e5GhyKHVVuWF8VQJ4MxUJJsFzKjUYjzr/IVvtcIiDElCT0mVCrWYoqN6qHYHLzxx9/GP7etm0bwjg5+nU6HXbu3IlSLolTVSFs+dxQ5UZ+iCU39sJ9rQ0IPj7kS1evJ9ehUCFh+xP6lcpdR+CXKm9S8KNHjX/zkAcvL6LyvPIK8dWZP584LB89CrzxBgnlHzYMGDiw4BClKDcsIbJm6lCzWUpo37gDrpzKjVBTjRBHYDmdp/kgNVpKzWYpsdFSYkzUFE6DYHLTpUsXACTPTd++fU2W+fj4ID4+HrNnz5a1c24LWz43L4pyo2Zyw36xZmeLIzes/8jz5+Kug4LkhhdccmOnn9Wrk4LfM2cC33wDLFxIzHPjxgFTpwL9+wMj7gXDkBNRKBkRqtyo0aFYaN/4fGScVVyVu40tcmNLueHOl3odbPncyGmWystzHokQEi3F3jsMA+TnC1NlKZwKwT43er0eer0eJUuWxP379w3/6/V65OTk4NKlS+jYsaOSfXUfUJ8bdZulANsvdVsDAqtEiLkOYsxSTiY3LKKjgUmTgBs3CNmpWpVcuq++AsptnouuWIfDqC+fWUoNyo2jfePzkXGGciO08jYgr/M0H5wVLQUQEuEMiDFLAdQ0pVKIdihOTk5GRESEEn3xHFCfG3UrN4DtAcwWuZFyHZyp3OTkAKdPG/8XSR78/YG33ybmqh07gHbtSFLBDeiKhjiMzktewYULAhpizVL2CIQ7Kzfs4O3lZVRPnGmW4m5jDfaUG0dJpq08N3KTG2eZpsSSG+pUrEpISu+1c+dOdOzYEWXKlEHZsmXRsWNH/GWreNCLBiWUGymKgS04S7lRK7mRqtxIGbycqdycPm26rcT7RaMBWrYktc4uVO+J/lgCLXT441IFVK1KapHZjB9g92vN50bNDsVilRu+iDolyY2XlzHKyd594grlRoxZSmj5BXvtyQkh14FLaKlyo0qIJjcLFixA27ZtERISghEjRmD48OEIDQ1F+/btsWDBAiX66H4QksRPqnLDjVxxBM5SbjzNLOUs5UZqdMjff5v+LwMZTmAuYgmScB6V0bXMGej1pBREuXLA+PGkZIQF1GyWshcKLtbnxtnkhruOo8qNEj43YqKl7B0rtzaKsyKmhDgUAzRiSuUQTW5mzpyJuXPnYvXq1Rg+fDiGDx+OVatWYe7cuZgxY4YSfXQ/CEni52qfGyuFM2WD2s1SalVuHM3rwfrbsNXo5SAPBfdxRVzCuhYLcegQ8NJLpOnPPiNFPefMMTuVQsmNGs1SalduuOuoWbmRwyyl0Tg/YkqIQzFAsxSrHKLJTXp6Otq2bWsxv3Xr1khPT+fZ4gUE9blRv0OxrReTEHIjRkFzps8NS25efpn8ynG/mOW5adAA2LcP2LiRlH548oSUhahYkYSU6/WwHwquZrOUq5UbOc2XSis3SifxE9qenBDaN6rcqBqiyc0rr7yC9evXW8z//fff0alTJ1k65fag0VLuo9w4w6HYWT43jx6Rwl6AkdzIqNwAMBy3RgN07Egcj5cuBYoVI8VK33oLSEwEdp4vyIpoT7nJzzcmwnMWPEG5EVpfSs46WnyQmsTPmaZasRBLbqhDsSohOkNxQkICpk+fjj179qBBgwYAgCNHjuDgwYMYM2YMvvzyS8O6w4cPl6+n7oQXrHAmL5xJbp49M92nEKjd50YKuWH9bcqVA4oXJ38roNxw4eVFoqt69CDJAGfOJIU7W57qgzaIwufP/0FVvjblqGskBXq98dw6moOHT2lxRp4b7jq27pP8fGOVcVcoN3KRG6rcUEiAaHKzZMkSFCpUCBcuXMAFTkxoeHg4lixZYvhfo9G8uOTmBS6caYDazVJq9blxhNywJql69eS7X3JzTfOLWGkvMBCYMIFEUX3yCbBwgQ7b9G2xa1VLzKlHCnSa5F8zL4HhLHLDvd72zFJqVm6E3Cfc/ivlc+NoEj9n5X4SA0puPAKiyU1ycrIS/fAsqN0sxTUFOCNaSq83hq4qAWf63EgJyXeWcsMlN3JFI5n7Ftk57shIouAMS/0Io3+pj436VzBsGPHR+f57IDS0YEVvbyL76HTOHRyEDPhCHUXVTm64/XemciNntJTQ9uSE0Ggp6lCsaig44rzAYMlNVpalP4EaHIqFvOAdBXsOGEY+tcka3MXnRskXOcMYzVJyKjfmBF0gWSrrm4Lf0RnzOu+Ctzfwyy/EF+fMGc5KrvBZYM+Hl5dpmDEXcig3Op34a6iUcsMSST7IRW6USuLHbU9t0VJqVm6c7cemQohWbhiGwa+//ordu3cbyjBwsW7dOtk657ZgB3aAfPmGhBj/V4Nyw31pK+VzwzUzPH9uek7khtrNUmIkeKkv8qtXgcePSZ+rVwfOnyfznazccNfTABjR+l/UG98cr78O/PcfUL8+sGABqVml8fcHMjOR9ywb504Bx44Bx4+TXVSpAlSrRspAFC8uY0khIc+f0C9yPtLK/WjJyhJXc0juAd9epBQgn0OxktFSancoVhu5GTSIePlXrkweuHr1yG+FCsoq6CqDaHIzYsQILF68GM2aNUN0dDQ0tBqqJQICjFWnnz83JTeOKjd5efYrAdsD+yLz8XGsHVvQagmhef6ckA/e8tUyQK83qgtqJzdKmqVYk1TNmqQNpZQbCVXB69cnTsZ9+pCMx0lJwF9/AVHZn+EYKuFU3QrItjE+FCpESE61aiS/TvPmxjQ+oiGE3Dii3HDbzcri2OEEQO4BX0jQgKuS+Dm7mKwYuHu01NatxPXgzBkyffstmR8WBtStC3zwAdCqlWv76ASIJjcrV67EunXr0L59eyX64xnQaMjAnpFhOjjk5xudM6UqNwB5aYoZyM2htDMxC5bcKOlUzFUWuCTSHhzNc6M2h2KW3NStS37lIjcOKDfcfhQpQvLizJoFTJwI/PwzACSRdXLIe7dOHTIFBwNnzwL//ANcukTy6OzbRyY2CXqNGqQ8RMuWQOPGIvyR5VRu+AZBjYZsn50t/twr5XMjhMjRaCkj3F25Ye/xzz4DHjwg74bjx4G0NFIwbu9eYPNm8vB4MESTm7CwMJQuXVqJvngW+MgNl+FLVW4A9yE3wcHA/fvKhoOzbWs04s6poz43akvix3UmBkzNDQwj3a7DLYCZleVQVXCtlpRraNgQWLQIiN62AnWebEOdH0eg7Jt1eRXznBzg4kVCdk6eBHbtIqTn9GkyffEFOWX16pGpbl0ylSxp5ZCFRCs6otywbTtCbuQiwc5Qbhz1uRFzrGpzKFY7uenalaSFAMi5O3cOmDoV2LAB6NKFPEzsx5AHQjS5mTx5MqZMmYKlS5ciQOwA/SKBL9eNI468Wi15mHJyHP8adya5AZxDboKDxQ3gnhQKzq0EzpIb7rOZkyP9WrPkPCICuHlTklnKHC+/XJBjsOZc4MlpILKP1dAGPz+i0tSoAfTuTebdu0fey3/9RT5Eb94E9u8nE4voaCPRef11oHz5ggVKKzcAOfdPnni+cmMtj46Qvsmd00dOCL0OYqOlPvyQ3Bdffy29b0LAd4/7+BCT9c8/Ax06ADt3Au3bk4cmIUHZ/rgIor2LXnvtNTx58gRRUVGoWrUqatWqZTJRFIAv1w37svP1lebYJVfxTKXrSrFwRq4bKc7EgDByw3d+1Ohzw1YCj4gghZ4AS98PqWDvtSJFyC/XtGoLQnzLJDqzRkcDb7xBinfeuAFcvgwsWwYMGQLUqkWCg+7dI2awjz4iZSHeeIN8uCrucwNINwnKPeALUW4ccSjm7tsTzVJyRkvpdMCMGcDChcDdu471zxa4SSr57nE/P2D9emL/ffQIaN0aSElRrj8uhGjlpl+/fjhx4gTeeust6lBsC3wDu6OKSUAAKcFMlRsjlCQ3rlRuxEjwbAh43bpG9crHhxBovd4xh0f2/mXJDUCO3Z5/kxhy44Csr9EQ5b1cOaBfP+OuT58mp2X7duLE/PPPZOparzo+RE3UklO5Mb+uUsmN3OUXlFZurOXRkZvcqD1aSsjzlZVFzMMAeYcXLSq5ezbBJYDWrntICHkoGjcG/v2XEJz9+x3w0lcnRJObzZs3Y9u2bXjppZeU6I/ngI/cSI2UYiGXk6gzHYoBdZIbW1+srkziJ+Ur1dzfBjD6ID1/Lq9yA4gjN7Y8fRWKNgkIABo0INOIEYToTJ8O/PYbsP5ocazHSbS/eBQf7CHrWFxmVyk3SuW5Eehzo9MBt28DyclkyskBYmLIVKwYUcxMUgNxyQ13gZC+eVK0lBBiyL0XlCwwLdT1ISKCMP9GjYjXfvv2xNYrJihD5RBNbmJjYxEqJrzxRYUtnxtHlBtAPnKjVI4bFrbKUMgFT1VuHCU3ALnPnj+XR7kJDiZ9y80VduxcR2RrkCuLsh3UqEGSCF64AMzofwWrj5bClkf1sKUZOaTq1Y3+OXXqABV8/Ym93hGfG8DqedLryaKMDGNmB70e0D0Mhx7loHsaA+0lMv4ULmzFlaxgn0xOLtLTCClJTyduFYbb1oZywzAk+mz1j41wBTuQvKcSUvxtWxw1GvJxX6wYsX6WiwpAOfRHOZ8bKHdXg6JFC/rqBmYpNrdoWho5b0WLkog9UX0TQ264rgRpaaL7Kxjss6TVWk9SySI2lhCcxo1JNFWXLkTRUXpccBJEk5vZs2dj7Nix+OabbxAfH69AlzwESpmlAPdRbqhZikDJr1RuJXDzyAc57hf2pRwURNrLzbVPRvR64zkUEpXkpGiTSpWAlb23YfLRufg0/ltseNYCjx6R5IHHjhn9PEODE9AZP2DM829Q3VaDPNc1Jwe4lF8J51EM51dWxPnVwLVrpLYrmxXBOtefQaZ5IBPIuB4VRVSTokXJb14ecHv3GNzGGNyeFo/nHxtbKFwY6NWLFDOtyfMRk50NrFoFfPklmy26LJk4qa/i4oBSpYjolpoK3LlDfnU6Evx4/z7rvx4GYAmQB6AYeQQrVACqhdVDdQxHtQd6VH9M+mQCls2xO7QHgWapx49Josg7d0gEtPn06BGxCKWlkYlL5Ly9iZN7p05Ap+dFUQZX5HUodrZy4+8vLMCiYkXgzz+BZs2IcvPOO8Dy5TJmzXQdRJObt956C5mZmShTpgwCAwPhY3ZzPn78WLbOuTVsORS/aGYpqtyQXyWUG9bfpnx5ku2OC/b6OnK/sNcuMJAce1qa/faEpjxwknJjguxslMVVfN/4B3z3QwskJ5NTeOwY+T1xAkjP8MKP6IMfM/ugVWtgzBjilmDxvs/LQwpiseNiA/z1Bhnw//sP0OkWkeUb7XeHjS3QagGv3Exo83OhDfSHztsf6ekFROY2mUxRoaAP5Cc8nOTjfPQI+OorMlUv3hVv4zx6aZ4j5zbxZV28GHj4kGwTEAC81SQFL239EKXK+6DUziUoVow/1kGvJwQhNRW4dYskxP7vyCP89/NxXNZWxA0mDhkZ5PydQDyA+cB5AEWAEiVIAsZatUgJjsQqeSgGQMOeACEnCTA8E0+fkki5y5dNp0eP7DdlDm5Ksl27yDQK+1EJ59FpYSTa9yP+XFFRPPlO1WyWEvNuT0wkNtv27YEVK8hXwLhxyvTPiRBNbubNmydrBxYuXIjPP/8cqampqFy5MubNm4fGjRvb3e7gwYNo0qQJqlSpgtNsGKyaQJUbdSs3Un1u1KbcWDNJAca+OkIezJUbwP6xcyV4hR2KRYNz72s0xLxSujTQsyeZnZ8PHNv2GF923IZf8Bp27PDGjh0kQ/KYMeTL/vBhouZvXzEO/2IusMV0F2E+z1E57zQq1w9FlTeqonx5YvIIDiankf0NCDAjEt3eIpEsc74BBg1CTg5RSe7dI9Pdu+TX2xsovv9nFN/0DYr3a43iX/8PgYFEENmxg0SObdgAnLkdiZGYjw825oPZbFQqSpYkVdqTkoDCp68AW38EfCoDJayfNq2WqEbR0cTMBwB46Trwc1sgpjhyrt7CtWskJ9GZdVfwz09ncca3DpJzS+DWLUKIthjOkz+K4g4ScRyJswLQoDHQooWNZOkFHwU5mTosnAtMm0YiqvlQrBg5vshIyykigpDAsDCSOJq9JlotET83biTTvt35uIDKuLAM+GwZadfLi/geFS9unOqnV8Xr8IK3kOfL2WYpsaal1q1Jxdv33gMmTCASXJcusnfPmRBNbvr27SvbztesWYORI0di4cKFaNSoEb799lu0a9cOFy5cQMmSJa1ul5aWhj59+qBFixa4d++ebP2RFXw+Ny+acqNmcuOocpOZKTw5npI5PbjFMs0hx/1irtwIaY9d7uNj2+7vivT1du59b2+gQWNvNMCbmIkJmP/eFXy/3BtnzxqjsYwoCi10qFcyFa3fLoEGDUhNrGIfDYdm2VKg03RgeFXhfTOLvvLzI24RsbE86z47D2zaCwRXBQp8tr28gLZtyfT4MbC6759YuikKJ/W1AT1xrRgxAujcmXNZ5IiW8vODnx9Jl5KQAHQLTwF+6gaUq4z0Q+dw7hxRtU6cIK4d588zuKuLwSZ0wqZPSBOxscDAgcCAAYSgcMH4+OIXvIYJi97DtadkXtmyxBm8fHnjVLas9NymZcsCo0aR6UlQPLZmNsbGTt9h38lgg0mOJWksvkJrfIx/MfG/zXgrz44wq2blhsW77xJ2+vXXxLZ54ABx4nJTiE62kpKSYnMSgzlz5mDAgAFISkpCQkIC5s2bh9jYWCxatMjmdoMGDcKbb76JBg0aiO2+80CVG882SzGMcAIiJRsr1y/BGriVwPkyjcph9pGi3Agl8S4yS5nsmw8F1z0eNzB3WgZu3iSZ7NlBt1QpYPBgYF3TL/EIRXBo2M+YPBlo06agyGeg66OlChcG3q1+ECeQiIu9PsG//xIH4u7dzfimI+TGWj4YTt9CQ0lG6qFDSV6iM2eA9Et3cQgN8KV2JHr3JoF4N28CH39MVJfu3YkCpdcDBw8CDdeOQA+sxbWnRVC0KPD99ySCecUKkhfv9deJmuRI0nYuCuXdxxv4GasWpeHWLXJqbt4kIum6dcTkN3o0EBGSjasoi/5nRqBiRVKr0qpbEPdecIZyI/XdPm8eqTuVmQm88gqxQ7opRCs38fHxNnPb6ASWWs/NzcWJEycwfvx4k/mtW7fGoUOHrG63bNkyXL16FStXrsQnn3wirNOuAPW58WzlBiDXQYj8KyUyBCBvSlvtX71KHA3YSuDW+uoq5cZewSdXpK9n+2br3udep5wchEcDY8cWfNU/4dSAfeMwgDT1JvErOK8VY9IMLjoWkEm5MYGdaKlArxw0wBE08P8HWDEP2dnE5eObb4hYsG4dmYoWZfPdxSEIGfig7j6M2dleNhLDC73e4jp4exO/oRJmZrup1TdjUd/DmOUzEdeuFcKAAcAnn5C6lFFRRuflp0+BtOMV8BTLUQhP0Pq8Bk0zRdRDEwNH3+3e3sDataSK+KVLxDS1Z4/0McuFEE1uTp06ZfJ/Xl4eTp06hTlz5mD69OmC23n48CF0Oh2io6NN5kdHR+OulQyO//33H8aPH4/9+/fD216YWwFycnKQw3lw05WUBLmgyo26lRupPje+vsaK71lZxIhvD1IcigEycNkiN8nJ5LdcOf4BUYloKSHtCQkDB9Sr3Gg0xlInnL6xkUsGeFKeGynXQCK5MX8e/P2JFaRXL5JF+ttviSpz9y7xhxlQ8ySmnOiAmJqdgWCFCzZz+2znOgSFeeN9zMaQGifwTY/dmDWLPJJDh/KtXb5gAuZvA/wKA02aGM2IFSvKFKAkR/b58HBg0yZi6v77b6B/fxJi52YRVKLJTXWeL8TExEQUK1YMn3/+Obp16yaqPXMViGEYXmVIp9PhzTffxJQpU1DeUCjGPmbOnIkpU6aI6pMsUMLnRkoCOT5Q5Ua6cqPRkOsgJjmelEGLu501sPtn7zVzyEEeHFFuhJIbFzkU24S/P+mXrb65MkOxXLWlHLkG1gpM2uubjeOsUoWYfT79FNi9m/D2Cuu3AyfuOieJn7WSEnwoWB6U9xRjxpDyH998A6xZQw4tLIxM4eFA2NWTCNu2BtdQGlv9uyAlO5o4pW8nJq64OGLqHDbM+uMsCHK928uWJXJaq1YktXfNmkS+dCNIKHDEj/Lly+PYsWOC14+IiICXl5eFSnP//n0LNQcAnj17huPHj+O9996Dt7c3vL29MXXqVJw5cwbe3t7YtWsX734mTJiAtLQ0w3Tz5k1xByYVSio3jtaWouRGOrkBxA9eYpQbLy9jCI1QcmONRMit3AgNLRdKblToUGyAkL55knKTk2MsDyAUUpUbAccZFAR07EiCdpxafoF7Pu09r2bvkMBAQlSOHiXVDDZtAn76ifjnzmi9B+MwC99iMK7X6IoLF4A5c0iQkp8fqZE2YQLhFF9/7QCPk/Pd3rQp6SRAct+4GUSTm/T0dJMpLS0N//77Lz766COUY8urC4Cvry9q166NHTt2mMzfsWMHGjZsaLF+aGgozp49i9OnTxumwYMHo0KFCjh9+jTq8UWLAPDz80NoaKjJ5BSo2eeGFs60Tm4YRn5yI2bQ4q5n7w1nz/zjKuXGRkVw2fsnFuy+5FCVXElu5K4tZa8tPlhzKJaB3JjAmeUX2H14edmITS+AxAzFmvQ0JCQQH65t20hk2w8/EEf1u3dJNHaFCsQ0J9CF1Qi5P1zZMktKOkErBNFmqfDwcF5TUmxsLH7++WdRbY0ePRq9e/dGYmIiGjRogMWLFyMlJQWDBw8GQFSX27dvY8WKFdBqtahSpYrJ9lFRUfD397eYrwpQnxt1KzfcwYsb0p2fb/yCdYVyA5CXeXa2+pQbuc1SrnAopsqNKbjLcnKEEw52fb72HTBL8cKZVcHF9E2mDMWBgUCfPiTX0pIlJI/P9etA374kSu+TT4hfryCXF7nf7awY4CxfVRkhmtzs3r3b5H+tVovIyEiULVtWsJMvix49euDRo0eYOnUqUlNTUaVKFWzZsgVxcXEAgNTUVNHh5aoBLZxpql4JzQkjFo4qN4Cp4661SsdcOEu5sfcl7Qxyw6fc2FNa3D0UnLvc1sBlLbrJUXIjVyZrscpNdra4wonWfG64yg3fcy/mOLntO8MsZe2Y+CBzhmJfX+K307cvsGAB8Tu6cAHo1o04H8+dKyDtjNx1A1lyk5FBZCR7apaKIJrcNGnSRNYODB06FEP53cux3I6db/LkyZg8ebKs/WGh0+mQ58jD5OdHvMTYr3CAvPTi4kiafCkv9LAwsn1wsGMDQkgIaSckRNmBxdub7Acg8ZBKhBMWKkT2ExAg/ljYvj17ZlRrnj0zzmcY/jbj48nxCKmzBJAQG52OvMyFrF+qFCHHZtE6FtBqSV9jYvjXK1KELPf3l3ad9XqSkhYgfY+MJO0JOY64OBI7a2u94GCyXliY8wgO+wwFBdneZ8mSMNQ/sLZe4cL855c9rtBQu8fl4+MDL3bAcIVyo9WS65mXJ15Bs+dzA5D73vyjV0zIO3c9tSk3YnzGuOTm2TPybPHVuQD5jhg7lpR5+uILYPZsYO9eoHZtGMLNedxSTfsit3IDEIJjqC6qfogmNz/88AMiIiLQoUMHAMDYsWOxePFiVKpUCatXrzaoLu4KhmFw9+5dPH361LGGdDriOg+QynkaDfDaa0CHDuSlyIbxikHZsqRNPz9p27NISgJ69yaDlSPt2APDGM/B7dvKsP7PPye/Op24Y+H27e5dY9+4182aajhyJHmJREQI2+eUKeRl5uMjbP3p00k/NBrb69evT1LChoTwr9ewIVkeFCTtOuv1xnPx5AkprlezJhm8bbVXvTrZzt560dFkPV9fZe9DLkaPJoOrvXt/zBgyeNta74MPSFvmz3PRouS4BF7v8PBwFC1aFBpXREsB5H0ihdzYS+LHrmNObtzBLCVE+ZBaFZxhCFGw4/8ZHk6IzDvvkFJPP/9MEhiuWUOSF44YwdNNucmNnx+5Trm5hOx7MrmZMWOGIYPw4cOHsWDBAsybNw+bNm3CqFGjsG7dOtk76UywxCYqKgqBgYE2ExbahF5vvNFKliQPuEZDburixS2LHAoBK2f6+5Ove6nQ68mXRGys3QfMYWRlkYe5ZElx9nwh0OuNZpNSpcSTJ76+sWqMRmP9HIu9jtnZhKywX/n2kJdH+hEbazsuNDWVvPgjIsiAao5Hj8g5CQ0lxygWeXnGL87SpUnFRW9v8oLjrQlQALYAUuHClrn0uXj+nFxDX1/H7mcxYM9tyZL2Y26fPyf9t3aNrbX1/Dm53naOi2EYZGZm4v79+wCAGClJ/Gypy0JNFH5+5H5WQrnh659SZlo5IEW5ycuzqcQAsDRRpqUJfveWLAmsXk0cjUeOJCUsxo0j+YCWLiUmKwOUcDkIDSXPvpv53YgmNzdv3kTZsmUBABs2bMCrr76Kd955B40aNULTpk3l7p9TodPpDMSmSJEijjXGDav09TUmf2P/l3LzsQ+eRiPPzevvr7zfjZcXcdL19pZ/X9yXXWCgeJ8eNhkf93qw102rtd5f9uXt5SXsmNg2hZ5vlqT5+Nhen32ZWruf2Jev1PuFPZ9aLTH7CW3PXr9YcENBlL4PWbDXIiDA9j5ZtcHeNQAsrytbNoNh7G4bUGCqvX//PqK8veEFiIuWkku54a4vFPZ8bgB5yY3azFLc85qba/s8m5MbCUShUSMSZv7jjyRs/No1Ekr+yy+kUgIAZSJh3ZTciA4FDw4OxqOC2vLbt29Hy5YtAQD+/v7IctTR1cVgfWwC5ciLrdEYByn2Zcf+2mL4tsBuZ6/mkD2w2zsj46T5OZAT3PMp5VjY88klolxyYw3svoQek5A2+dq3l3fE3nUU209r7bP95jtfUvpl3j+x+VUcAbsvoX2zde6stSXyvLPvmzxW8nemzw13uVi/J2vKjVZrfO75+qdms5QUh2LudtZgnptMIlHQaonD8eXLQNeu5JR060YSCANQTrkBPJ/ctGrVCklJSUhKSsLly5cNvjfnz59HfHy83P1zCSSbosxhTkbEDnL22pMKR/shBnL1mQ/sl7/U4+AbhIQMfmIIG8MIH1DN+yWURFg7fqFkRGj7Qq+lUBKv5L1hDWL7ZuvcWSNxIo9LwyqIXGXXHuT0uZGapdiWf4qtXDdSUiNYa0tuSM0mLjSCkIWDeWOCg0kJqN69yWvwrbeAxYtByQ0HokeFr7/+Gg0aNMCDBw/w22+/Gcw3J06cwBtvvCF7B90a7EuOHYQdVW4c/RJn4Wg/xMD8HMgJ9jikOirzDWBCzo3U6+AqciOXciP0uIXeX85WbhhGXuJl7SOBe18JPTZzM7Y9iCE3QpUbuXxuANvkxlPMUhqNcT17504Gs5Q5vL1J4uChQ8ntM2gQ8MXZNmQhJTfiyU14eDgWLFiA33//HW3btjXMnzJlCiZOnChr59we1sxSUpUhFZilJk+ejBo1agjfgEflEN2GNUggaXv27IFGoyHRcAKUm6ZNm2LkyJGmjYhRRLjriCU39q6zPQXO0fvFXBkT2p5QZZDbHuc8aTQabNiwQVxfhUDMtRBCvKwpctzjlnLu5TZLOdvnxl7/1GyWEhMtxV1PqFmKLbQrU8ZfrZbkxBk/nvz/wbm+mITJYPwouZH06f706VNs374dK1euxIoVKwzTjz/+KHf/3BtKmaV4vgj79esHjUYDjUYDb29vlCxZEkOGDMGTJ08s21HQLHX9+nVDPzQaDTQVKkBTpw40ERE4cuSIvDsrGHwnL1okmCw1bNgQqampCAsLs63cFAxY69atw7Rp0wyL4+PjMW/JEtN1bYG7jtkgePfuXQwbNgylS5eGn58fYmNj0alTJ+xkz5Ojyo1SPjcyKTeTp01DjTffJP9wjjU1NRXt2rUT21v74J5PR5UbW+ZGbttilRuur54tiCm/4CnKjdrMUoBwkx6r3LBRjTISBY0GmDkTmDGD/D8VkzByTX35TpebkhvR0VIbN25Er1698Pz5c4SEhJj4p2g0GvTu3VvWDro15DZLmX8Rmr0E27Zti2XLliE/Px8XLlxA//798fTpU6xevdq4khQfEAn466+/ULlyZVIR7skToFgxFElIkHcnIhWovLw8+Pr6oij7guH7OjcjfoULF7ZsSAxpsKIWXL9+HY0aNUJ4eDhmzZqFatWqIS8vD9u2bcO7U6fi3zVr1ONzw95nYh2KhZqlzNosyhfWLgdsEE0L2FNubKlAGo0xEk+s07lcphqdjkQpAvKUmuCDVJ8bd1BuhPZNKDFkyU10NPDvv4oQhQkTgNDvvsB7ye/jy60VsLcOyYuTmOhgw25KbkSPsmPGjEH//v3x7NkzPH36FE+ePDFMjx8/VqKP7gvzrz+5yY0Z/Pz8ULRoUZQoUQKtW7dGjx49sH37dpN1li1dioTXXoN/o0aoWLUqFi5caLJ83LhxKF++PAIDA1G6dGl89NFHkjI1FylSBEWLFiVTRASKRkTAx4YD4bJly5CQkAB/f39UrFjRol+3bt1Cz549UbhwYQQFBSExMRFH//4byzduxJQFC3DmzBmDWsRmttZoNPjmm2/QuXNnBAUF4ZNPPjE1SxWcz4OHD6NJkyYIDAxEoVKl0GbYMDwpeJC5ZqmmTZvixo0bGDVpElGjSpfG8+fPERoail9//dWkvxs3bkRQUBCecV8InEFw6NCh0Gg0+Pvvv/Hqq6+ifPnyqFy5MkaPHo0jbFsMg5SUFHTu3BnBwcEIDQ3F66+/jnv37pHlej0mL16MGo0b48cff0R8fDzCwsLQs2dPPHv2zHB8v27fjqpVqyIgIABFihRBy5Yt8bwgPxCf2a1Lly7o16+f4R6Lb9YMn3zyCfq88w6CX34ZcW3a4Pfff8eDBw8MfatatSqOHz9u6NfyjRsRHh+PDRs2oHz58vD390erVq1w8+ZNACT7+JSpU3Hmv//IufT2NrluXLPU2bNn0bx5c0P/33nnHWRw6pX169cPXbp0wRdffIGYmBgUKVIE7777ruV9yyXD9siNEOWGBV9bUiPqxJIbnY7fn01IGREWUh2KhSg3cpil1Fp+ARBPbljirlAhynfDV2EtXkPhkFycOQPUq0fyVjpU3o8tyeHp5Ob27dsYPny4POHS7gCGIUm5pEw5OeSmzsggU2Ym+T8zU9j25l+N3Jeyna/na9euYevWrSaE4rvvvsPEjz7C9CFDcHHtWsyYPh0fffQRfvjhB8M6ISEhWL58OS5cuID58+fju+++w9y5c6WfPwGmjO+++w4TJ07E9OnTcfHiRcyYMcOkXxkZGWjSpAnu3LmDP/74A2fOnMHYsWOh1+nQo1UrjElKQuXKlZGamorU1FT06NHD0PakSZPQuXNnnD17Fv379zfdsUaD05cuoUXXrqhcuTIOHz6MA5s2oVPjxtDxnN9169ahRIkSmDpuHFL//BOpBw8iKCgIPXv2xLJly0zWXbZsGV599VWEsDWvOAPg48ePsXXrVrz77rsI4kkkF14QEszo9ejSpQseP36MvXv3YseOHbh69arx+ArO6dXkZGzYsAGbNm3Cpk2bsHfvXnz66aeAVovUhw/xxoQJ6N+/Py5evIg9e/agW7duYISoOZxrNnfuXDRq0ACnVq5Eh5deQu/evdGnTx+89dZbOHnyJMqWLYs+ffqQdgu2y8zKwvTp0/HDDz/g4MGDSE9PR8+ePQGQunJjxoxB5dKlybm8ccPkurHIzMxE27ZtUahQIRw7dgy//PIL/vrrL7z33nsm6+3evRtXr17F7t278cMPP2D58uWW5VvEmGPFKDd87Un1dxI74AP8gz5XhXGlz42c0VJqVm5sqV4MY/S5UcAsZYLsbLyGX3Hxh2Po1YvcfnPnAlWqAH/+KbFNN1VuRJul2rRpg+PHj6N06dJK9Ed9yMwUX5RRLmRkWGZR1WrJlxrPS3PTpk0IDg6GTqdDdsHDNmfOHMPyadOmYfZnn6FbgXmoVGIiLly8iG+//RZ9+/YFAHz44YeG9ePj4zFmzBisWbMGY8eOFdX1hg0bQqvVGs1gGg3S0tONdXQ4mDZtGmbPno1u3bqRfpUqhQsXLhj6tWrVKjx48ADHjh0zmInKli1LSjqkpiI4KAje3t685ow333zThNQkc9Pha7WY9eOPSKxe3agU3b+Pyq+/bnT846Bw4cLw8vJCSEgIikZEGL5okpKS0LBhQ9y5cwfFihXDw4cPsWnTJuzYsYN3QL1y5QoYhkHFihX5T17BwPrXnj34559/kJycjNiCjMA//vgjKleujGPHjqFOwQtYr9dj+fLlCCnoT+/evbFz505MnzwZqQ8fIl+nQ7euXRFXkKqhatWq/Ps1B+cea9++PQa98w7wzz/4eMAALPr1V9SpUwevvfYaAKL4NWjQAPfu3UPRgu3y8vKwYMEC1KtXDwAp3ZKQkIC///4bdevWRXBwMLluERGk/hbPIPzTTz8hKysLK1asMBDBBQsWoFOnTvjss88QXVBkp1ChQliwYAG8vLxQsWJFdOjQATt37sTAgQMtj0cIuRGj3EjZ3lp7UsgNXwI5lnhotZblD8yhdp8bd3cozs01Xl+2KJRCyg3bj6gYL6xcSULEhwwhVcbbtyeVx+fPJ4+bYLwo5KZDhw744IMPcOHCBVStWtXC1PCKIVUihSKwQW6aNWuGRYsWITMzE99//z0uX76MYcOGAQAePHiAmzdvYsCgQRjIaSs/P5841xbg119/xbx583DlyhVkZGQgPz8foRJKNKxZswYJCQnAgwekdlN4OC+xMfRrwACTgYjbr9OnT6NmzZqW/i8CBqtEWwZnjQanL1/GawWkCoAwfyQzk0PdunVRuXJlrFixAuPHj8ePP/6IkiVL4uWXXzZ+0XHaY1UTq/mUCo7n4n//ITY21kBsAKBSpUoIDw/HxYsXUaeApMTHxRmIDQDExMSQlP5aLaqXK4cWdeqgarVqaNOmDVq3bo1XX30VhYSUjeCYcapVq2boV3RB+gcuSWJJxv379w3kxtvb2+T8V6xY0dD3unXrmu7LClm4ePEiqlevbqJwNWrUCHq9HpcuXTLst3Llyib3V0xMDM6ePcu/DyE+WvbMSty2+NoT6+8kltxw37t8g76YfCdKkhs5zVIMo3xlaiUcirlh4E5Qbrj9atsWOHcO+PhjYN48Up9qzx6S9K9ZM4Ftvijkhh2Apk6darFMo9FAp0Q+E1ciMFC6wTI1lUxs7Z9z58j8WrWE79scNl66QUFBhtIYX375JZo1a4YpU6Zg2rRp0Bes/93XX6NeeDh5QVSqBACGQeHIkSPo2bMnpkyZgjZt2iAsLAw///wzZs+eLfyYCxAbG0v6EhpKXnQ8Sgg5jIJ+ffed4QufBduvAGvVxAU4FPOZfQzQaBDg5yc9QzFnu6SkJCxYsADjx4/HsmXL8PbbbxuTs5n1sVy5ctBoNLh48SK6dOlitX1Gr+clQAzDkPkFx2/+gaHRaMh51Wrh5eWFHV9/jUOZmdi+axe++uorTJw4EUePHkWpUqWg1WotTFQGXxXOPebj42M4J2yfuPtl5+n1epPt+PpvMs8OiTAcKw+4862eAy6kKDfWyIm9e09ps5SXF9mHXs+vjgiNlOKuI6dDsS2zlFRyw25r7X0gB5RwKGZNUl5eZCwAnEZuACL+z54NvPkmyW58/jzQsiWp5/u//wl4HNyU3Ij2udHr9VYnjyM2AHl5BQVJm4KDjTVsAgLIJGZ7W1+EAl6akyZNwhdffIE7d+4gOjoaxYsXx7XkZJSNjUXZuDiULVsWZcuWRamC4n4HDx5EXFwcJk6ciMTERJQrVw43btxw7PzZyeZr6Ne1a4b+mPerWrVqOH36tKXDesH95uvnJ+3e02pRrWxZ7Ny/3zjPzqDl6+tr9MfhHNNbb72FlJQUfPnllzh//rzBzMdHbgoXLow2bdrg66+/Njj2cvG04CVSqVw5pKSkGJxwAeDChQtIS0tDQsWK9lWBAlVBo9GgUYMGmDJlCk6dOgVfX1+sX78eABAZGYnU1FTDJjqdDudYEm5+ToVG1xWcl/z8fKOTMYBLly7h6dOnBnOcr6+v8bpZOZZKlSrh9OnTJufp4MGD0Gq1KF++vLD+mPVLlM+NPeVGrhxDYpUb7rqOKjdKOhTLaZbibqsUlHAoZpWbgAAjUVDKLGXjuteuTWpTsbECH30EtGtHxHWbcFNyI1q5oRABbii4XLllRMjdTZs2ReXKlTFjxgwsWLAAkydPxvDhwxH6/DnaNW2KHK0Wx48fx5MnTzB69GiULVsWKSkp+Pnnn1GnTh1s3rzZMAiKxaNHj3D37l3g6VNSdC0rC+ElS8Kf56Ez9Cs0FO3atUNOTo5Jv9544w3MmDEDXbp0wcyZMxETE4NTp06hmF6PBqVKIb5kSSQnJ+P06dMoUaIEQkJC4Cfki1WrxYR+/VD1zTcxdOhQDB48GL5PnmD3n3/itZ49EcGzSXx8PPYdOoSeNWrALyjIsE6hQoXQrVs3fPDBB2jdujVKlChBFlgxhSxcuBANGzZE3bp1MXXqVFSrVg35+fnYsWMHFn31FS6uXo2WL72EatWqoVevXpg3bx7y8/MxdOhQNGnSBIm1agGnTvG2zcXRc+ew8++/0frNNxEVG4ujR4/iwYMHxGQIoHnz5hg9ejQ2b96MMmXKYO7cuSSSDLAcmLkhztZgpvYMGzYMX375JXx8fPDee++hfv36BpNUfHw8km/fxulLl1CiSBGE+PhYXLdevXph0qRJ6Nu3LyZPnowHDx5g2LBh6N27t8EkJRhizFL2njN7bSkdLcWum53NP+BLUW7kdCiW0yxlrxCnnFDCoZiP3ChBFBjGLqkNCgKWLSOVxIcOBbZvB2rUIOaqxo1Nm7p7F7h4Efh3T0k8xQRk3SuErPfJLrKyyJSXR75ffXyIaxf76+0NFCpE1CFXQRK52bt3L7744gtcvHgRGo0GCQkJ+OCDD9CYe3YoTL/e5Cp5IPKLcPTo0Xj77bcxbtw4JCUlIVCjweezZmHsV18hqCCElw0F7ty5M0aNGoX33nsPOTk56NChAz766CNMnjxZdDfZgqpcrF692hAtw0VSUhICAwPx+eefY+zYsQgKCjLpl6+vL7Zv344xY8agffv2yM/PR6VKlfB1gZNz91dewbq//kKzZs3w9OlTLFu2jIQy24NGg/Jxcdi+ciX+N28e6tatiwA/P9SrVAlvvPUW7yZTp07FoHfeQZmuXZGTm2ti0hkwYABWrVplGpVlRQkqVaoUTp48ienTp2PMmDFITU1FZGQkateujUUF2bg0ADZs2IBhw4bh5ZdfhlarRdu2bfHVV18Jvv6hISHYd/Ik5q1di/RnzxAXF4fZs2cbkuT1798fZ86cQZ8+feDt7Y1Ro0ahGWuM5+s76/NlDZzzERgYiHHjxuHNN9/ErVu38NJLL2Hp0qWG5d27d8e65cvRbMgQPH32jPe6BQYGYtu2bRgxYgTq1KmDwMBAdO/e3cRRXjCUUG7kMkux7QmNIALkU26U8LmRM1qKdYrOz1deuVHCLMWSm8BAgPVvVEK5yc833m92rnu/fiT/zWuvkbQ7zZoRx+OsLODCBUJq2G8coCiAGUAOABEeCjExriU3GkZQTKgRK1euxNtvv41u3bqhUaNGYBgGhw4dwvr167F8+XK8yWYcVSnS09MRFhaGtLQ0C0fZ7OxsJCcno1SpUrwKg2g8fkzq0gcHA8WLA5cukYdBaLQKHy5fJqy/VCmgwLFTFNLSgP/+Iw9agc+NosjIIE+Po8fNh4sXSch82bJWfXps4tYt8nkSFQWULEnmpaQA9++TJ7N4cf7tsrOJ/5RWa+I/9dNPP2HEiBG4c+cOfNmXI3u+AwKAypWF9evOHTJFRADWitHm5ABnz5LBtXZt62398w95YSckWEbe2cOlS8CzZ0Dp0gDrzH3mDBmcrLWXlwecOYPlGzdi5Pz5RhXIGv79l9wjZcqQTz0l8fAhCRsJDQXsmbTS08mzZu262buvr14lyStjY40RMlaQnZ2N5PPnUap7d/jHxgJcM6ktFC9O7pOTJ4GaNU2Xbd8OtGkDVK8OnD5tu52ZM4nzRf/+AJt9WwjCw8n9femS5fl85RVg40bgu++ApCTTZZ07A3/8wb/MGgIDycibnGz9mZADAweSzHfTp5NzYg99+gA//gh8/jnw/vv86+zdCzRtClSsSK5tZCSZn5dnP5JNDJ49MypDmZmCfJMyMgipWbnScplWSx7LhHL5iNyyHAHIQsDIQfAP8TV4W/j6Ek6Vn08Oh/t3UJCwUygGtsZvc4g+s9OnT8esWbMwatQow7wRI0Zgzpw5mDZtmurJjVPB9TeRyywld0p9paHmquB8pgchpguzmkiZWVlITk7GzJkzMWjQICOx4bYnpo9CTI9S6jeJBd+9Yq89sXXLHL2fxUBKnhtHlRuloqW469rKcyNGuRHrUOysaCl2XdYOoiSUVG64ZimAkBE5CT33+gkMZQ8OBlasIFFV27aRb+ZKlci3S/nyBbcP4wV4vUPu0bGvkg8/N4DoUeHatWvo1KmTxfxXXnnFNIcIhanPjYvMUhZwtHinWJiXoJAT5uUBxIJvABNyfsxqB82aNQs1atRAdHQ0JkyYYLqulFIXQhI1ii1xIDe5sRdFJPQ+d7REhBgoES3lSodiWwRCjM+NFIdihpGexE8queFuqxSUNkv5+hrPt9ymKZbc+PqKGmc0GqBXL0JypkwBevQAqlXj8GKNxi2dikWPtLGxsdi5c6fF/J07d5rk46CAKnxuLKBg0Uxe8KlXckGuchZiQ8HNymBMnjwZeXl52LlzJ4LNEz66mtw4Qh4cUG76deli3yQFOFe5cQefG1dES0nxucnPN/ZZ6WgpbntqjZaypXqxoeCsmUgpoiDmmosF2+dnz+RvWyGINkuNGTMGw4cPx+nTp9GwYUNoNBocOHAAy5cvx/z585Xoo/uCO7DLpZi4q3LD7lvOBFysGuRs5cZKwUdeKE1uHK2RZAt8ZMDeoC/2/nKmcqPmaCkWcpEbpaOluPuUapaS4jzt7mYpgDgV378vv3LD9kFJcuNGyo1ocjNkyBAULVoUs2fPxtq1awEACQkJWLNmDTp37ix7B90aZr4ZJvMcbVPqYOBs5UYpciPHOZWq3Gg0xgRq9gYvR8iNrbbFKjdSyA2fT5NQnxuxZimq3JBfuZUbpciNvcKcQqKl1KjcKJmh2BOUG08mNwDQtWtXdO3aVe6+eB64g6ejzq/mbbqLQ7EYIiAG3LakHotU5Ybdp9LkxpU+N5wCmKLIjVjCKbAQrCwQoypxn92C2mii2nI1uRHzFS/FoZhtX6Ph/2CR2yzlbJ8bOWtLsWYpNuM8Gw5OyY2iEDwqPHnyBF999RXSeQ4uLS3N6rIXGtwXfH4++XW1WUrKYOsolHAq5g4uzo6W4m5r7zpIMQOqIVqKu2/u4GWvb2JJvDOVGynRUtzt+NqSK1qKhVymGmcpN35+/OdAiWgpwDPMUkplKabkxgSCR4UFCxZg3759vLHlYWFh2L9/P0kuRmGEVmt88Fly86KFggN2SzBIgqP+NgC/aiB0ABR6HVyt3EgdZK0pY56g3IiJlgJskxu1R0spVX7BnsIhd7SUOzsUU7OUSyB4hPvtt98wePBgq8sHDRqEX3/9VZZOeRTYl5xc5MbdHIoBZb7O5SBpfP2S21FXil+QGhyKue2LKHTpFj43YqqCc7fjQi3lFwDX+txYa18ps5Q7KzfOMksJNamJgSeTm6tXr6JcuXJWl5crVw5Xr16VpVMeBVZZUAu5kcGhePLkyahRo4bh/379+vFXt2ahhFlKp8P1O3egqV4dp+1lYLUGjYa0UaWKsQ2x5h4Ho6Xi4+Mxb948i37xtb18+XKEs5mYlfa5sda+3A7FzlRuxJqlNBr0mzwZXbp3t96Wq6qCc9eVy+dGCrmx1l9bSovY8gvc/bijQ7G1UHBqllIUgkc4Ly8v3Llzx+ryO3fuQOtMU4e7wFy5kcvnxmww6NevHzQFFaB9fHxQunRpvP/++5ZVpxUwS82fPx/Lly+3vgLHLHX9+nVoNBrphISFwEGjadOmhvPCnQYPHsx/DhxURCyOz84geOzYMbzzzjumM60Qkh49euDy5cuGZZMXL0aNDh2E9dMOeWAYBosXL0a9evUQHByM8KJFkdinD+atWoVM9uXMbc+dlRuevvHel1ot5r//PpYvXmzZlidGS0lxKPY0s5QSDsV8oeAANUspDMHRUjVr1sSGDRtQv3593uXr169HTfP6JhRONUu1bdsWy5YtQ15eHvbv34+kpCQ8f/4cixYtMq5UsF1efj5EfDfZRBj7sEros2SIUIEGDhyIqVOnmswLDAzkH5jkdtS1Mgjm5ubC19cXkWydGb62zQhJQEAAAtgXpMyOz71798a6devw4YcfYsGCBYgMCMCZbdswb80axG/fblTm7JElqcqN2sxSBeuFBQcbByO+tuz53Dij/IIrlRtqliK/1KFYdRA80r733nuYPXs2FixYAB1nYNHpdPjqq68wd+5cvPvuu4p00q3hRHLj5+eHokWLIjY2Fm+++SZ69eqFDRs2ADCakpauXYvSnTvDr2RJMAyDtLQ0vPPOO4iKikJoaCiaN2+OM2fOmLT76aefIjo6GiEhIRgwYACyzb7wzM1Ser0en332GcqWLQs/Pz+UfPllTF+6FNDpUKpUKQCELGs0GjRt2tSw3bJly5CQkAB/f39UrFgRCxcuNNnP33//jZo1a8Lf3x+JLVrg1KVLgk5ZYGAgihYtajKFhoZaJTcXrl1D+65dERwcjOjoaPTu3RsPHz40Pb7Fi1G2a1f4FS+OkiVLYvr06QBgeXyvvUbO0ejR6NKlC2bOnIlixYqhfEGhQXOz1NOnT/HOsGGIbtMG/nXqoEqVKti0aRMAU7PU8jVrMOW773DmwgWDGrV8+XL0798fHTt2NB6PVov8/HwUrVPHpCI3F2vXrsVPP/2E1atX43//+x/q1KmD+NhYdG7SBLuWLjVUCdfr9Zg6dy5KdOgAvwoVUKNGDWzdutXQzvXr16GJjcW6XbvQrEcPBAYGonr16jh8+LBhnRs3bqBTp04oVKgQgoKCUPnll7Hl4EGAYUzNbgXYsGEDNJzrZLiPly5FyZIlERwcjCFDhkCn02HWrFkoWrQooqKiDNeDhUajwaLVq9Fu+HAEFC+OUqVK4ZdffjEs570vtVpilnr9dcN6OTk5GD58OKKqV4d/o0Z4qXt3HDt2zLB8z5490Gg02Ll3LxL79EFgnTpo2LAhLtm7Vx1xKHY0WkoJh2KloqXUZpYSk6GY9blRiijQJH4mEKzcdO/eHWPHjsXw4cMxceJElC5dGhqNBlevXkVGRgY++OADvPrqq0r21SVgGOO9KQk53kAWh9BkaoDn1lfngldcEPGlGxAQgDzOi+/KlStYu2ULfps1C14lSgAAOnTogMKFC2PLli0ICwvDt99+ixYtWuDy5csoXLgw1q5di0mTJuHrr79G48aN8eOPP+LLL79E6dKlre53woQJ+O677zB37ly89NJLSD11Cv+ePg3o9fj7779Rt25d/PXXX6hcubKhyOR3332HSZMmYcGCBahZsyZOnTqFgQMHIigoCH379sXz58/RsWNHNG/eHCtXrkTyyZMYYV7HSSx4iGbqvXtoMmgQBg4ciDnz5yMrKwvjxo3D66+/jl27dpke38iReKltW6Tm5+Pff/8FAMvju3+fvDA1GuzcuROhoaHYsWMHGJ4ver1ej3bt2uFZejpWTp2KMnFxuABiEjZHjw4dcO7UKWw9fhx/7dkDgCho5cuXx8svv4zU1FTExMQAGg22HDyIjMxMvM4ZpLn46aefUKFCBdMknAX3l8bb26DMzZ8/H7O/+Qbfjh+PmomJWLpzJ1555RWcP3/exB9v4qJF+GLKFJSrXx8TJ07EG2+8gStXrsDb2xvvvvsucnNzsW/fPgQFBeHCkSMIzsgQpdxcvXoVf/75J7Zu3YqrV6/i1VdfRXJyMsqXL4+9e/fi0KFD6N+/P1q0aGGiNH/05Zf49N13MX/BAvy4fj3eeOMNVKlSBQkJCfz35e3bZEPOtRo7dix+++03/DBnDuL8/THrl1/Qpk0bXLlyBYXZyukAJk6ditkjRiAyIgKD589H//79cfDgQfsH50rlJj+fXAchH2D2fG6sKS3cOntqNEtJjZaiZin1gRGJo0ePMsOHD2fat2/PtGvXjhkxYgRz9OhRsc24DGlpaQwAJi0tzWJZVlYWc+HCBSYrK8swLyODzeLl/Ckjg+cAMjIY5tgxhjl92mR23759mc6dOxv+P3r0KFOkSBHm9ddfZxiGYSZNmsT4+Pgw9w8dIts/esTs3LmTCQ0NZbKzs03aKlOmDPPtt98yDMMwDRo0YAYPHmyyvF69ekz16tV5952ens74+fkx3333nXGDlBSyz5s3meTkZAYAc+rUKZM2Y2NjmVWrVpnMmzZtGtOgQQOGYRjm22+/ZQoXLsw8f/6cLLx9m1k0fjxvW1w0adKE8fHxYYKCgkym5cuXM0xODpP8++8mbXyUlMS0rl+fYXJyDG3cvHmTAcBcunTJeHwzZ5Jjun3bZH8Wx3f9OsMcO8b0fe01Jjo6msnhtMswDBMXF8fMnTuXYRiG2bZtG6PVaplL//xD2j5xwmTdZcuWMWFhYeSfq1eZSQMHMtUrV7Y45kqVKjGfffYZ+efBA6ZL06ZMv27drJ6jhIQE5pVXXjGd+fAh6cOlS4ZZxYoVY6b/738m8+vUqcMMHTrU5Ni///BDhklNZRiGYc6fP88AYC5evMgwDMNUrVqVmTx5snE/jx+T9i5eND2+Aqxfv57hvqYmTZrEBAYGMunp6YZ5bdq0YeLj4xmdTmeYV6FCBWbmzJmG/wEwg19/neyrYNt69eoxQ4YMMem7yb107hzTt0MHpnOHDgzDMExGRgbj4+PD/PTTTwxz4wbDHDvG5CYnM8WKFWNmzZrFMAzD7N69mwHA/LVli+Eabt68mQFg8l7hIisri7mwdy+TFRfHMF9/zbsOL0aNIi+KceMsl7VtS5YtW2a/nfR040snM1PYvtesIeu//DL/8iVLyPKCc2dAZqZxX8+eCdsXwzBMnz5km88/F76NFPj5kf3cuCFs/aNHyfpxcdbXqVOHrLNxI/l//37yf9myDnfXBB9+SNodNkzedhmGYQ4eJG2XKSN/2yJga/w2h+gMxXXr1kXdunXlYVYU4mHDLLVp0yYEBwcjPz8feXl56Ny5s0nuobi4OESGhwPPnwNaLU6cOIGMjAwUKVLEpJ2srCxD5NvFixctUgA0aNAAu3fv5u3exYsXkZOTgxYtWlj22YqfzIMHD3Dz5k0MGDAAAwcONMzPz883qAYXL15E9erVia9MwfE3qFqVtz1z9OrVCxMnTjSZFxUVZZnLhGFw4uJF7D5+HMGcr3AWV69exdOnT8nxvfSScTtb4Ph5VK1a1aBU8eH06dMoUaIEMVmdPSssFJwHSUlJWLx4McaOHYv7jx5h84ED2GnD4ZthGBPTj0n7BecoPT0dd+7cQSNWCSlY3qhRIwszZrWyZQ3bxcTEAADu37+PihUrYvjw4RgyZAi2b9+Oli1bonurVqjm7y9KuYmPj0dISIjh/+joaHh5eZkENERHR+P+/fsm2zWoVs3kmBo0aGDbsd0skuvq1avIy8tDo0aNDPN8fH1Rt25dXLx40fQcVKsGpKYCej1iihY1nIOSJUvy74u91nJFEElRbgDy9c8qDLYg1OfGvG9cJUdt0VIM45xQcJrnximQVH7hRUJgIJCR4UADKSkAx18DZcrwOyha2bcFbJCbZs2aYdGiRfDx8UGxYsXgY/byCAoKMnFE1ev1iImJwZ4CswYX5r4PQhHA92LkJvHj8XPRFxzLd999h3r16pltSrZlzAd6EQ7FYWFhKFu2rOUCbhsFtar0ej06NW6MzxYtskgQGBMTg2vXrpF/xDoUo+D824Dh3AlxRrWx3z59+mD8+PE4fPgwDu/ahfhixdC4Vi2r65cvX95icLbmMKsx6xsfMfLx9jZsxy5jr3FSUhLatGmDzZs3Y/v27Zg5cyZmjxiBYf36QavVWlznPB5/EvP7mo0QNJ+nNz9HPMdkQeq4MDt2tm8ajcZ472g0/OeAM+izSyz6wwdXREtxz51QvxupSfy4fRVDbpxhltLpjM+cEuUXzM1S1KFYUdDYbTvQaICgIAemEC2CAvTGKVgjeFve9655zRsOgoKCULZsWcTFxVm87A3gvOBr1aqFu3fvwtvbG2XLljWZIiIiAJCiqEeOHDFpwvx/LsqVK4eAgADs3LnTss86nUG54DqlR0dHo3jx4rh27ZpFP1hHz0qVKuHMmTPIYr+C8vNx5Nw5q/0QBHPlRq9HrYoVcf7aNcSXKmXRl6CgIOPxHTpEtjMbsCyOT0SG4mrVquHWrVu4/N9/pv3iA8PA18cHOp4Bs0iRIujSpQuWLVuGZatW4e2OHW2SoTfffBOXL1/G77//bpxZsD6j0SAtLQ2hoaEoVqwYDhw9arL80KFDSEhIsGzUht9GbGwsBg8ejHXr1mHMsGH4bsMGQK9HZGQknj17ZpK+wOGUARwc+ecf8kfBtThy5AgqVqwIgOe6cdZjr0HZsmXh6+uLAwcOGObl6XQ4fvy45TngHr8QUuPKaCmNRrxTsdRoKbavHAIsCEKipRgGmDwZ2LxZeLt8fePuzx4cyVCckyPOidsenJHET+4+KwhKbpSGtSRocrQnJfEZh9y0bNkSDRo0QJcuXbBt2zZcv34dhw4dwocffojjx48DAEaMGIGlS5di6dKluHz5MiZNmoTz589bbd7f3x/jxo3D2LFjsWLFCly9ehVHTp7Ekt9/B/R6REVFISAgAFu3bsW9e/eQVvD1MnnyZMycORPz58/H5cuXcfbsWSxbtgxz5swBQAZgrVaLAQMG4MKFC9iycye+WLlS0CFnZmbi7t27JtOTJ08s6wcxDN597TU8Tk/HG7164e+//8a1a9ewfft29O/fHzqdznh806djxebNuJqcjCNHjmDJkiUAYHl8Ir50mjRpgpdffhnde/TAjqNHkXz7Nv7cssUkIskAvR7xMTFITknB6dOn8fDhQ+RwXjpJSUn44YcfcPHSJfTt2NHmvfL666+jR48eeOONNzBz5kwcP34cN27cwKb9+9HyrbcMJsgPPvgAn82fjzXbt+PS1asYP348Tp8+jREjRlg2auU+HzlyJLZt24bk5GScPHkSu/btQ0J8PMAwqFevHgIDA/G///0PV65cwapVq2znTxKJX/76C0v/+AOXr1zBpEmT8Pfff+O9994DwHPd0tIsjiEoKAhDhgzBBx98gK179uDCtWsY+P77yMzMxIABA0x3Zq82lTlcGS3FXU8suRGbxE9KpBR3fVvKzalTwJQpwKBB4tpmwT12Z4SCA8CzZ8L7Zw9KKjccM7CsfVYQlNwoDbnJjb208PbAURI0Gg22bNmCl19+Gf3790f58uXRs2dPXL9+HdHR0QBI4riPP/4Y48aNQ+3atXHjxg0MGTLE5i4++ugjjBkzBh9//DESEhLQY+BA3H/8GNDr4e3tjS+//BLffvstihUrZojQSUpKwvfff4/ly5ejatWqaNKkCZYvX25QboKDg7Fx40ZcuHABNWvWxMS5c/FZwcBkD9999x1iYmJMpjfeeIMs5Cphej2KRUbi4JIl0Ol0aNOmDapUqYIRI0YgLCzM4NPx0UcfYczgwfj422+R0Lo1evToYfDvsDg+GyVL+PDbb7+hTmIi3pg4EZV69MDYceNM1QQWej26N2+Oti1aoFmzZoiMjMTq1asNi1u2bImYmBi0adkSxSIjbd4rGo0Gq1atwpw5c7B+/Xo0adIE1Vq3xuTvvkPnNm3Qpk0bAMDw4cMxZtgwjJk/H1W7d8fWrVvxxx9/8Gcut3Kf63Q6vPvuu0hISEDbtm1RoXx5LBw3DtDrUbhwYaxcuRJbtmxB1apVsXr1akyePFnU+bOFKe+8g5+3b0e1xET88MMP+Omnn1CpUiUAPNetc2fe7Mmffvopunfvjt7vv49avXvjSnIytm3bhkKFCpnujFvQVSlyI5dyA0gnN1LNUmLJjRCz1KNH5Pf2bWnmE27b3gI9Ntjzq9cb032YwzwU3MuLSPOAvKYpJcmNl5ex/+5imhLrrTxp0iTm+vXrYjdTDcRGSzmMe/dI1AQ7CY1GsAW2LbPIG0E4eZJsK+cx2sPTp2Sf58/L1+apU/KcT/Z8ZGaSc8ITpcSL+/fJupcv217v0iWy3sOHwvuk1xuvcW4u/zpsRBUnaoiL58+fM2FhYcxvq1aR9WxElPEiOZk3GozJzCTzT56U1C8LiDnnDgAAs/7zz8m+8vOFbXT1Kln/7l3LZZcvk2UPHljfXuA9mpWVxVzYuZNES23dKqxvDMMwCxYQvfHVVy2XFS9Olh0/Lqyt2Fiy/t9/C1t/yhSy/qBB/Mt37iTLzaP5zp4l86OihO2HxeTJZLuC6DZe/PabMRLr2DFx7TMMieoEGMbXV/g23HBavvDW/Hzjcu69EhND5ll7jqSgY0fS5vffy9cmF0WLkvbFvktkhJhoKdEywsaNG1GmTBm0aNECq1atskjoRmEG8xwlcpQ9YNuUUqvJFYUzHekvHxjG+JUk9AvLGvh8mMRUjVaqKri9mktWHH71ej3u3LmDjz76CGFhYXilUydh/bTWvrX7V+7yC1JMrFIhR/ZkIc+RmMzcnqbc2IuWEuNMbKs9LriKgsAEnyaQoipxj5/v3HHHR26whRJOxUom8QPczqlY9Eh74sQJnDx5EtWqVcOoUaMQExODIUOGmGTpFIOFCxeiVKlS8Pf3R+3atbF//36r6x44cACNGjVCkSJFEBAQgIoVK2Lu3LmS9us0yG2WAowDuliyIHYAlwtyl1/gSt2OkhvuACaG+MlUONNuv2w4FJv0owApKSkoXrw41q5di6VLl8KbfVGLJQ/2CmfyOLTb6pdVcI/TGQTHvMq5Ldi6xkKOU0xpCbnJjVifG6kOxWKT+Cnpc8P1BXGE3IhxyOU6RvOdO24GWC65UYIoKGmWAjyf3AAkqmPu3Lm4ffs2li5ditu3b6NRo0aoWrUq5s+fb3AStYc1a9Zg5MiRmDhxIk6dOoXGjRujXbt2SElJ4V0/KCgI7733Hvbt24eLFy/iww8/xIcffojFfIXt1ALzl58cigk7oIuts8J9STtTuZG7KjhXtZGzEKkSyo1UpUyichMfHw+GYXDz5k2Sa8geGbEGa+TGnqOsVOXGWnsygcnORpemTcUraAD/NRZCWp3lc8P3HlCLciM3ubH1zpOL3Ijtm62IKdaZ2M/P9F5XIksxJTcmcOjzXa/XIzc3Fzk5OWAYBoULF8aiRYsQGxuLNWvW2N1+zpw5GDBgAJKSkpCQkIB58+YhNjbWtNAjBzVr1sQbb7yBypUrIz4+Hm+99RbatGljU+1xOZQwS7HkxpoDmzVwX9LOVG64eW7kGMCkStt84JIIKcqNULOU2PNti9xw+yq0wCcgTjmzp9zwtSemXywcdZAXCrH94q5rS7mRyyzFQo7cL3q98RlRKlpKam0pJR2KueTm8mVx7QPiSy+wsKV6mUdKsVCieCYlNyaQNMKdOHEC7733HmJiYjBq1CjUrFkTFy9exN69e/Hvv/9i0qRJGD58uM02cnNzceLECbRu3dpkfuvWrXGIzSFiB6dOncKhQ4fQpEkTq+vk5OQgPT3dZLIHRs4vSCWVG7HkxtXKDSDPACaXvw1gOgCpxeeGu74t1UBIu1LJgxDlho/csJBCbpQ0S0khN3IpN3bOO8OSQoaRxyzFHWTFKjdCfSidHS0lVrm5fFn8u8ZR5UYKuaHKjWIQTW6qVauG+vXrIzk5GUuWLMHNmzfx6aefmmSA7dOnDx48eGCznYcPH0Kn0xlCjllER0fj7t27NrctUaIE/Pz8kJiYiHfffRdJSUlW1505cybCwsIMU2xsrNV12cR3mQ5VyjQD92Wq1bqW3HCVCXcmN0ooN1xyI+TcCPWnkEpubKkGYhQ47rUWc+5ZEyIfubE2aEtRBqX2TyykXAch10AGcpOZmQnk5sLn4UN5yA2XoLg6z401ciP22RXic8MddDMzjYVPhUIJcmMeBs5CCYdiSm5MIPrT97XXXkP//v1RvHhxq+tERkYKSzUOy/TnDF+dGzPs378fGRkZOHLkCMaPH4+yZcsa85aYYcKECRg9erTh//T0dKsEx8vLC+Hh4YacJYGBgXb7Yhfch1GjEf5lZAvsyzYnR1x77MMnVz/EQKMh/c7Kctz3hu27HMfBPZdcU5C9dtmXtk5ne132OcjNFddXbr/MFSruPcXttzWw5z47W7g6wl6jvDzLfrP7M5/P9kujEZfFVEr/xILbV6HXgf144DsH7HXlW2a+jpXnlGEYZGZm4v79+wjftAlemZnyKjcajXASoVSGYmvRUkqbpQCi3tj4mLWAFIdi7vpqUW6UyFAMeD65YRjGMmEVSLHFzz//HB9//LGgdiIiIuDl5WWh0ty/f99CzTEHm9itatWquHfvHiZPnmyV3Pj5+cFPxMUuyilyJwv0emNtKS8vIDnZ8TYzMkjCqufPxTkV5+aSvsjVDzF49IicCx8f8S82czx8SI49P99xcnP/PnkBsYPqo0fkRW+PMOTnG6+rrXN5/z4hCmKP+8EDcm29vCy/xPLyyL41GuD6dfttPXokvg/375Pr5etrOUA+fEja8/Y2fZFy+yXm/nr4UL57wxqyssh+fH2FmzOfPQMePyZf3+YqqZDryt6nOp3NASE8PBxFCzJcy6rc+PkJV6rk9rnhmpEYxtgPZ5mlAOJUzC3gaw9KOhSbkxt3dih2kwzFosnNlClTMHjwYGN15gJkZmZiypQpgsmNr68vateujR07dqBr166G+Tt27DBkrRUChmFM0s47Co1Gg5iYGERFRfEW7RMNnQ7o0IH8XbIksH27423u3Am8+y5QrRqwdq3w7f75Bxg8GChenLThTAwcCNy6Bfz8M1ChgmNtzZwJ7NsHTJ8OdO/uWFtz55JrMmkSGfQ++gho1gyw4tRuwNOnAJtD5uxZ61/Ib74JPHkCbNoEFJByQXj/feD8eWDxYuDll02XXb5MrmOhQsDhw/bbknLuO3cmL/vdu4GCyt4GDBlCSNWPPwIFtZkAkMFk8GAgIgI4cEDYfgAgKYmYENaudfzesIbt24Hhw4HEREBg2Q78+ivw4Yf894OQ67p0KfDLL8DIkeS88MDHxwdegFEpkyNaSkq+E6WipQAjEQacEwpevjx5RsRGTCnhUGzNLEUdihWHJOWGz1Rz5swZFC5cWFRbo0ePRu/evZGYmIgGDRpg8eLFSElJweCCF8GECRNw+/ZtrFixAgDw9ddfo2TJkoZidwcOHMAXX3yBYcOGiT0Mu/Dy8jJUpHYY9+6RGy8kRJ4bLzwcuHGD/6veFrKzyXYBAco9ANaQnk72nZHh+L4vXCBthYc73tbz56Stp0/JS+3GDfKStNduWBhZFyBfptbWv3rV2J6Yvj55QtrPyrLcLieHLNPrhbX5+DFZPztb2Pr5+QBbvDM01HIbtm+ZmabL2PuLW4hRCB49Etc/KXj2jOyjfHnh+9BoyDa3blluc+0auadtXVf2Oj18aHuf7Nc9IK22lC3lRiikOhTb87lh++couRFjlkpMJORGbMSUOzsUcyPkKLkBIILcFCpUCJqCekTly5c3ITg6nQ4ZGRkGUiIUPXr0wKNHjzB16lSkpqaiSpUq2LJlC+Li4gAAqampJjlv9Ho9JkyYgOTkZHh7e6NMmTL49NNPMUhqoTRnIShI3hd3ZCT5Zc0iQqG0TdYW2FoqnIrPknHvHvmNinK8Le6LiTVNCTk/3GuZlWVaDI8LJXwMrL0wrYFdjzuI2gLXoZ69bkLaY7cT2i8WYlUDKWD7KpeaIWQgFHrepVSj5q5rzefGlcoN9zi4ypIzzFKJicCqVeKVG2eSG7kdiqVEyImFp5KbefPmgWEY9O/fH1OmTEEYe3FATEzx8fFo0KCB6A4MHToUQ4cO5V1mXhF42LBhiqg0iiMoyOjLIQciIshvejp5IIU+jErLlrYQHEx+MzL4l+v1wiJsGIb4OwCAHd8sQZBKbjQa8sLKyrI9eCkRHSKW3LDXW+hXOUtANRr+c8Hu17w9tl/mErzc/ZMCKfe+rX4pRW7E3CdCfG6EQimHYsCUkEiNdBQTLZWYSH6vXxf3UakkubFmlpKLKHDvUUpuAIggN3379gVAnHkbNmxoCJumEAD261fsF601hIcTIqDXE9Jk7hNhDWolN1OmALNnE/+O2rVtt5OWZnwJuVK5AcgLKyvLuhql0xkjZuT0MXCWchMUxO+Qaq09sf1ioVZyY23QEnpdxZIbsRm3Xanc2HMo1mqN7yhu/5QySzGM8d1StiwZiNPTiVm4cmVh+3A0Worv/rWmZsrtUMzu28tLnvxffHAzciMoGQU38V3NmjWRlZVlkRhPaIK8FxLswC4XqdBqgSJFyN9iTFOuJDfWzFIHDxJy8+yZMGdrVrXh8wWRAu7Aau9r1Bz2pGXuF6srlRux5Ia9RtYUGHc0SzlCbqyFvAPykhu5CLAjPjdy5bnhLnOGWSoz00g4Q0KMjuliTFNS++ZohmI50h84493uZuRGEMUrVKgQUlNTERUVhfDwcF6HYtbRWCdX/SBPgtzKDUBMUw8ekEko1KbcZGUB/fsbH24rNcVMIKe/DcD/UpeL3Ej1peCu7wqzFFe54YM95cbTzFLmg5bc5MZRvyy9nqhJbACEI8qNXBmK2f5lZ8tDbuwpN6y/jUZD7tsKFYBjx8SRG6nRUo44FLPpLBwdG5xJbjIyTO83lUIQudm1a5chEmrXrl2OJ7Z70cAOEnLeeKzfjbsoNyy54So3kyaZRjTcvGm/HVa5cQdyI4dyw/el6iyzlFjlRqpZypnKjZi+CVFubF1Xdl/2Mp47qmYAxpxIgHOVG3vkBpDHLGXP54YlN8HBhOCUL0/+FxMx5cwMxWw/GYa8P+QiN0oGi3CDJjIyjO8/lUIQueHWbmratKlSffFcKKHcSImYUoNZilVujh4lfjYAMHQosHChMHLDKjdyOBMD8pCbp0/5l7MvS61W/FeOGsxSUpUbT/G5safc2PORcZZZim3DvL+u9Lnh9s8ZZinWVMIOwM40S0lRbrRaYj5LTydTQfJYyXDGu93Pj5yb3FzSZ5WTG9G1pZYtW4ZffvnFYv4vv/yCH374QZZOeRzk9rkB3Fe5ycggL4H+/Ymc3qsXSQoHuEa5cYbPjRTnezlDwaWapaT63KjRLOVIKLg15cbeIKg0uTHPJcNCDdFS3P7JES0l1CwVEkJ+XUFuxGQoBuR1KnbWu92N/G5Ek5tPP/0UEezAykFUVBRmzJghS6c8DuHh5Jd98OSAu5EbrkPxtGkkEV9UFDB/vrH+y5Mn9vPgKKncyE1upL4suduoUblh7x93NEtJVW64Tp9qITdeXkZF0LzeGKCccsMwwvxTlDBLcSPVuDAnN2wh58ePSUSpEEiNlpKSoRiQN0sxJTcWEE1ubty4YajtxEVcXJxJwj0KDgYNAt55B3j7bfnadDdywyo3J04An35K/l64kER9hYUZX0r21BslfW7EkhuWtCqh3KjBodhZPjdqNUux9wHD8KsPriY33G24/VM6QzF3X842S5m3x8Kc3AQFGT+ahKo3znQoBuQlClIIrRR4MrmJiorCP//8YzH/zJkzKMKGJ1OYonx54NtvgdKl5WuTJTdioqWc9QDwgSU316+Tr6/XXjOtC8W+iOyRG6rcuF65eVFCwbnrcvumlHIjl/lSaeWGuy+xZim5TXAszMkNIN405UyfG0DeLMVUubGAaHLTs2dPDB8+HLt374ZOp4NOp8OuXbswYsQI9OzZU4k+UvDB3ZQb7kBZpAjw1Vemy0uWJL/21L8XxedGDUn8pCo3avS5cUS54W4PqMcsxd3GUZ8bMeRGqPO9EmYpgF+5MXcoBozkRmjElLPJjZxEgZIbC4hOZfjJJ5/gxo0baNGiBbwLMiHq9Xr06dOH+tw4E+4WLcUqNwDw5ZeWyosalBu2/IOalBs5QsGlll+wR26slV9Qs1lKTN+0WjJA5+U5R7mRi9xIUW7EOBSz69iLApTTLOXlxZ/xmAWfcsOGgztLubGVoZjvWaIOxYpCNLnx9fXFmjVrMG3aNJw5cwYBAQGoWrWqodglhZPAVW4YRljadlcWzqxVC6hXj9R9eeMNy+VCyE12tvGhUsLnRm5yIzU5G3cbV5dfENOepzkUA6RveXmOKTd5ebaTnsl9nzhLubHXvpzRUuw2OTnKm6XkdCgWotxQs5QikFyEonz58ijPMmMK54MlN9nZZDCyNhBx4UrlJiAAOHLE+nIh5Ib1L/LxMTrzOgo5yI29PDdKhYILNf8opdx4eig4uz6bvoCFWHLD7p+rXnKhBuVGjEOxUBIgp1mK3SYnR5hDMWAkN1euCMuo6wlmKaU/XD2N3IwePRrTpk1DUFAQRo8ebXPdOXPmyNIxCjsICiI3ck4OUW/UTm7sQQi54ZZekCtLNvelTpUbAlcpN2rzuQH4+yaUtDqT3MgVLSWnciOnWYq7jVDlJjbW+I68ccN+QIcS0VJCzFLupNyw59dTyM2pU6eQV3CDnjx50mr5BVqWwYnQaIh6c/s2UTSEmAXdhdxYM7OxzsRy+dsAppIy+2UnltykpxNfAK2Zf74jyo0aoqWcHQquRrMUX9+EDtBarTGjq61z78iAbytaSmlyY6+/ckZLcdsTSm68vIBy5YBz54hpyh65kVu5YRjqUOxCCCI3u3fvNvy9Z88epfpCIRaRkYTcCHUqVjO5KVGC/D5/Tsw8hQpZriN30UzA9MUkltywpjGGIaYLbqQGoJ5QcGcVzpQaCq7WaCnAtnIj5LoGBChLbpRwKLbnwyfW50ZOsxQgPFoKIKYplty0a2e7fbkdivPyjAkHaYZip0NUKHh+fj68vb1x7tw5pfpDIQZiw8HVTG4CA43HYy0cXAnlxpE8N/7+xhc4n7TsrqHgjio3Yn1u1OxQ7IhyAwg792oKBWcYUqnaFhxxKFZKzeRTbgBxBTSlXgdryiP3mivtUEyT+FlAFLnx9vZGXFwcdDqdUv2hEANPIjeAfb8bJZWbvDzjy0jooKDR2LabqyUUXG6fG7nLLyit3DCMvMqNGF8qMeRGrgHfEYdiwP51EOpQzHcPO+KHJtYsBYiLmJIaLWWNnLPPEWueNAc1SykK0Un8PvzwQ0yYMAGPHz9Woj8UYvCikRslfW4AaS83W+RGLcqNUtFSubkkCkVqv6T2Tyzy8oy1oeTwB/Jk5Qawr6CJ9blxhllKDnIjt0Mx93ngM/NRs5SiEB0K/uWXX+LKlSsoVqwY4uLiEGT2dXfy5EnZOkdhBy8auVFSubE3zxpshYMr5TzpiHIjJCeSUJ8bgNxTQUHElMEOOmozS3FJkyt8bgDlyQ13wJei3LBFOHU64eTGEbOUs5Qb1ix1+zbxi7MWseZI34SQGz5wiYLQXGXWQMmNBUSTm86dO9OoKLVAbH0pV9aWEgKhyo2c5IZPVVGzcpOfb/SJEKvcsAUg7b28hSo3gJHc2PMvENI/pZQbbt/k8KeQm9zIbaqRmvPE359ce7nIjbNCwXNzjX0ydyguXJi8Jx8+BP77D6hZ03r7cjsU28v7xL479Hpy3m0RL3twBblxlJApDNHkZvLkyQp0g0ISxJZg8BTlRk6zlEZjzIXBQi5yo8SLXAqJMM+3Yqs/DGNfufH2JlN+vrE/3H5JVUeUVm78/cW/jN1FuXHU5wYgxyqE3LgyiR9gaZZiVRvAUrkBiGnq4UNimlKS3OTmmg749pSbgACjWpaW5l7kRq8nxydWpXUiRPvclC5dGo8ePbKY//TpU5SWs+o1hX2IMUvl5xv9I9yR3Oj1RoVKTuUGMD0fWi0ZuIWCDQd3lnIjhUT4+lq+cK2Bu9zWi8t80OaGgYslEEorN468+B1VbliCyKphfFCDzw13fXvXwZXlFwDLZ4IlN/7+/M+u0IgpR6OlzPtmj9xoNPKZeZyVoTgoyPh8q9w0JZrcXL9+nTdaKicnB7du3ZKlUxQCIYbcOOJ34CywlcFv3TLmh2Dx6JFxHqtYyQXuC0Hsy8FVyo0YFUKjEU4gWJICiCM3Up2JAWPfdDr7YchS4Ai5cSRDMSBs8FKTcsPd3hqEOt4qZZayptzwqTaAcKdiR6OlANNzJyTvk1xOxc5SbuQkZApD8CfqH3/8Yfh727ZtCGMvCgCdToedO3eiVKlS8vaOwjZYcsMO/OYZcrngvpxdUThTCIoVIw9Pbi7xryla1LiM9bcpXFjaV58tKEVu5Ci/YP4il0oiAgLItvaUG1Zh8POzXYtHTnJjPjiIUc6EwJXKjb36Y2LbM4cSyo3cDsVs33Q64weKnIRfDnKj1xuJtdi+cdfnnjsheZ/kynXjTJeD0FDSX08hN126dAFASiz07dvXZJmPjw/i4+Mxe/ZsWTtHYQdFipBf1mbLl9WXBXvz+/jYJkGuhI8PEBMD3LlDTFNccqOEvw0LpZUbJcxSSjnt2vO3YaEUuWEdlOUEe8yO9E2qz42Q+kFy3CcsCWYYY3tyJCzkg1ifG7Zv3HtZzmgpe+SGa5ay5gTrSN+0WtK3vDx+cuMM5caZwSJuotwIHuX0ej30ej1KliyJ+/fvG/7X6/XIycnBpUuX0LFjRyX7SmEOPz/jjWYvYkrtzsQsrPndKBEpxYJ7TtSi3Fh7kTui3HC3twZ7kVLW2pNaERwgSg2rEinhd6MG5UYIuZHjPpHqGM9dX6loKUfJjTU101rpBRZlyhAC8uwZcPcu/zqO9o2PBAt5Vt1VuQE8h9ywSE5ORgRrDqFwPYT63bg7uVG7cmMrz42jyg2bgA5QntwIVW7MsxQ7otxw21MiYorrpyQWjmYoVprcmCt83PMnR04fPkhN4sclEHKqmfaUGz8/gHWZsGaacrRvfPevEMIvt0MxJTcGiCY3w4cPx5dffmkxf8GCBRg5cqQcfaIQgxeF3Cip3KjZoRgwdbJV2iwlVbmRi9xQ5UYczAd87vmTKxmdOaRGS7G/3t7S8qNINUsBRtPUf//xL2fbZJMZigXfuXOmWYqSGwuIJje//fYbGjVqZDG/YcOG+PXXX2XpFIUIvCjkRu3KjVKh4IC48FJrkFu5kdMsBSib60apaCm5yI0cjufmyo2fn/ScPnL53FgzS0k5Tr72WAghNzEx5Nea+V5s0VxzSCU3cpilHKmdJgWeSm4ePXpkEinFIjQ0FA+FJpOjkA+eRm7YcHB3UW5s5bmRS7mRg9xQ5cazlRt2wHck34lQ06DUaCm5yI0U5cbee9LRvvGRG2eFgnNrp1FyY4BoclO2bFls3brVYv6ff/5Jk/i5Ap5GbljlJiXFdL6Syo0cDsXPnlnm5nFEueGGQ3O/VNWm3LD3laPkRqi/hxSoXblRwqHYkWOVu3CmXMqNNbOUPYdiwHnkhs+hWOlQcGen+XATciM6ocTo0aPx3nvv4cGDB2jevDkAYOfOnZg9ezbmzZsnd/8o7IFNaOdp0VKpqcTXhB3k1arcsIMXwxCCw1U1HXlhajRku9xc55qlXK3cKGmWciTBoKPKja1aPEr43EgZ5OTOUKwms5TS5IbvPhFjlnKEKFBywwvR5KZ///7IycnB9OnTMW3aNABAfHw8Fi1ahD59+sjeQQo7EKrcqL1oJovoaGPOiDt3jGYqltyozefG399IQtLSTMmNI8oNu51c5EbpPDeO+tyo1SzlaIZibnHEjAz+AViJaClnKDfONks54lDsCrOUsxyKuYTWGYUs3YTcSMrmNmTIENy6dQv37t1Deno6rl27RomNq+BpZimtFihenPzN+t1kZBgHT7UpN4B104MSPgaeqtwoaZZyJBTcUeUmMNAYfWPN9CBnskc5lBulkvg5SvYd8blhE57y1EU0aVNOh2IxoeCOmKWc/eHqyeQmPz8ff/31F9atWwemwJHpzp07yMjIkLVzFALgaeQGsIyYYlWbwEDHKudagyM+N4D1XDeORMFwt3OHaCl3MEvJpWaIITcajX2/G3dVbsTWllKzWUroMVmDGpQbSm5MINosdePGDbRt2xYpKSnIyclBq1atEBISglmzZiE7OxvffPONEv2ksAax5EatdaW4MCc3rDOxEqoNoLxyI+eXqlqjpTzVLMXXL7GDdFgY8PixsuRGzdFSznIoFkJu0tPJ9uZ9UNKhWOlQcEpueCFauRkxYgQSExPx5MkTBHAuWteuXbFz505ZO0chAOxD+/Sp5RcNF+6k3JiHgyvpTAwoR26ociMcas9zw+2X2OtqT7mRs0yHHMqNXA7Fzg4FtxUtFR5urKnHZ5pSwqFYSCg42+eMDFIjUAooueGFaOXmwIEDOHjwIHzNboK4uDjcvn1bto5RCEShQuSh1evJQ8stNsmFO5Eb83BwJcPAAcfJjbVcN3IpN3KEgsut3ChVfsETlRtbuZCktMeFmn1u1GSW0mqJ382DB0TlZpP6sVDSodjWs8QNQMjIMP1fKCi54YVo5Uav10PHwzBv3bqFEFs3lxUsXLgQpUqVgr+/P2rXro39+/dbXXfdunVo1aoVIiMjERoaigYNGmDbtm2i9+lR8PICChcmf9syTbkjuXGWciOXz40nKTdSHYodzVCsNnLjqM8NYFu5YRhlMhSrwefGmkOxnGYpNgoNsE1uANsmfFdFS/n5Gfcp1TTlKnKTk6OM0ioTRJObVq1ameSz0Wg0yMjIwKRJk9C+fXtRba1ZswYjR47ExIkTcerUKTRu3Bjt2rVDinkCtwLs27cPrVq1wpYtW3DixAk0a9YMnTp1wqlTp8QehmdBiN+NO5MbtSs3Svnc8L3MnRUtJTUUXM0OxY7kuXHU5wawXaJDTHtcuEK5cdQsJaeSyd6vgH1yYytiSsloKXv3naNOxc72p+SeZ1Y1UyFEk5u5c+di7969qFSpErKzs/Hmm28iPj4et2/fxmeffSaqrTlz5mDAgAFISkpCQkIC5s2bh9jYWCxatIh3/Xnz5mHs2LGoU6cOypUrhxkzZqBcuXLYuHGj2MPwLHgquXnwgPTbXX1u1BQKLjbPjSeVX5CjKrhebyxgKie54V5bVys3cjsUK2WW4p4zdnDVau3fe7bek3JFS4nNUAw47lTs7He7l5fxmFRsmhLtc1OsWDGcPn0aq1evxsmTJ6HX6zFgwAD06tXLxMHYHnJzc3HixAmMHz/eZH7r1q1x6NAhQW3o9Xo8e/YMhVmzDA9ycnKQw3lY01V8MSTD08hN4cLkRZWVBdy65T7KjbVQcDVESyml3MhdfkFtDsXcbbKzSSoCNZIbOaKlhJoGpea5USJailt6wV4COyXNUubEUK83/m3vmXDUh8UV7/bQUPIhpOLxVDS5AYCAgAD0798f/fv3l7zjhw8fQqfTIdpswIqOjsbdu3cFtTF79mw8f/4cr7/+utV1Zs6ciSlTpkjup1uALcHgKeRGoyHqzeXLxDTlrj43L5Jy46mh4Nz7ISeHHB+r4AglrULIjUZjTPYnBq6sLSXGLMUwyjgUC3EmZuFMnxvufay0WcoV2edDQ4G7d92f3Pzxxx+CG3zllVdEdUBjxrYZhrGYx4fVq1dj8uTJ+P333xFlY9CbMGECRo8ebfg/PT0dsazZw1PAPrS26ku5E7kBSDg4S27cRbmx5lDsycqNO2QoduTe9/Ii9c3y80k7UnxkhJAbuQiw0j43YkgKd7lOp6xZSm3khiX7gHDlxl3MUoBbREwJIjddunQR1JhGo+GNpOJDREQEvLy8LFSa+/fvW6g55lizZg0GDBiAX375BS1btrS5rp+fH/zcIXGdIxBilnKX2lIsWAJ67RpJgAa8uD43rqgK7omFM6Xe+35+hNzk5EgzIzmT3Cit3HCPX6hyA5B7WI5aa+Z9UCu5YZ8HX1/7ipxcDsWU3JhAkEOxXq8XNAklNgDg6+uL2rVrY8eOHSbzd+zYgYYNG1rdbvXq1ejXrx9WrVqFDh06CN6fR8PTfG4AI7k5eZL8snkqlIASeW70emNSLjUoN0LMPrm5RpOLGOWGYRwPBVerWYq7nRLKjVzpAtj7TekMxVLJDbcArKvMUraipYSa2qzBXHkU85xS5UYRSPK5kQujR49G7969kZiYiAYNGmDx4sVISUnB4MGDARCT0u3bt7FixQoAhNj06dMH8+fPR/369Q2qT0BAAMKkJD/yFHgyuTl+nPxGRhozjMoNJXxuHA3xBZwfCs6V0oUqN/n5xJRVUGNO1WYpOZyd2Wuh1Qr3kXGGcgOQe07pDMVc4iM0zw3bNyXMUkJKL7BwpkOxmNQI7upQDHhGKHj79u2Rxnk4p0+fjqec6JBHjx6hUqVKonbeo0cPzJs3D1OnTkWNGjWwb98+bNmyBXFxcQCA1NRUk5w33377LfLz8/Huu+8iJibGMI0YMULUfj0OnkxuUlPJr1L+NoB8Zqlnz4xqjRzkxvxlzjDSB2rz6CY+sP42Xl72+8y9j548sdyPWChplnIkFJy7XXa2tEHw/+2dfXCU1b3Hv5tkkwAJkdeECIRAEREUIVgFQW21WIodqY6XWr3AoHMnRSsvbccCM4U6rbH3D6ZyK3EqL9bRXpi5qJdO6UCUV0WhRiJRGRR5VcmNUHkTSEhy7h+nZ59n35Ldfc6zz9lnv5+Znd3sy8l5zu4+57vf3+/8TrrETWur+zk36rHc3K7FXW6u9YNEh7jpLCzV2dYLCi/CUok4mWq1b6oV/uncxCRh52bz5s1hS6p///vf48EHH8RV/7Lk29racPDgwaQ7MHfuXMydOzfmYy+++GLY39u3b0+6/awgmdVSmZJ/FJn07Va+DaBP3ADyZHvVVeEnYF1hqWRWYERin6CFiL1s1p5v01VSv/3/K5s/Ly/1Y3XLuWlrs0JtTnJugHDnJhVxc+5c9NjrmvBVW27n3CQbvgkGrXHzOiylxM2FC/JzZh8jt8RNIt/TW26R19u3y/Bisg41xU1MEh5FoWznOH8TD1Ff2osXw0MLCrXvFJB6TkS6iRQ3Jjs3BQXW65SbqU7AqS7xBaLFjT2klKpzA8QXEImulALkCVj1TyV8p+raAO45N/b2vHZu2tvDK+oCzidVe3gsHc5NspV87YLETecmEXFTUmKNVWTejVurpRL5Ttx8s6yfdPo0sH9/8v/bix+ufhI3xGCKiqwvZSz3Zs8eOQEVFwOjRqW3b6lSXGwl6gJmOzdAdOjBXmo+gdIGMYknbvLy5CUZIovRxSLZWjXqxK1T3Oh2buzt6Sitn8ok2L27Nanq3qLD3henzo16zZUr8gdRLFJxblSbukojXLli5XglI24CgfihKV3bL6SSUBwMArfdJm+/+Wby/5vOTUwSFjeBQCCq/kwi9WhIGujsSwsAr70mr++5J3PCUkC4e+Omc+M0oRiIL25S/SVof62aFJwstw4GrQk2XlJxMs6NvR86xI1bYSnVXiqCUOHUuQkE3CsXYH+tLucGiO/eJLtNgd1t0ZlfpEKNyYgbIP6KKafbL0Q6j8muHrzzTnn9xhvJ/2+Km5gk/G0XQmD27NmhmjGXL19GdXU1evzrRNjiRiIgSZy+fYEvv4wWN0IAr74qb//oR+nvlxMGDQIaG+XtTHNunP5KBeI7N6mKiG7dZL5BPHHj1LlxEvJ0Kyyl48Qfy7lJ9n0tKZHj5Ka40bVaCpDtxPqcJevcuBGWAmRbwWD49guJ0JVz40XODQCoem07d8q+JNMPryoUA/4QN7NmzQr7++GHH456zsyZM533iKRGvKTiDz8EPvtMfvmmTk1/v5xgd25MFzeRtW50TFqROQY6CuWpZMpYeOncuB2W0tW3VN9XN50b++fEiXNj70NXzk0qYSndy96B5J2bdImbZHJuAGD0aHkO/+or4N13rTBVItC5iUnC4mbt2rVu9oM4Jd6XVrk2U6bI3JxMIl1hqYICeWJoaUn8JBlJpjg39nYi8TLnxq2NM50uAwec59wAXW+uqiss5eRXfCAg27K3E0myuSk6w1L2sKJqy1Rxk2xYKidHhqbWrZN5NxQ3jmFCsV+It7+UyrfJtJAUkD7nJicHeOklYNWq8CTmZHAz50ancwPod250rMSLXKquCx0n/lgVik1ybnTl3ABdhweTzU3RGZYKBKLdTNPFTTLf1VTzbihuYuJphWKikVhf2sOHgQ8+kImkP/yhN/1yQrrEDQA88ICz19O50ePcAPJ4dSW+u5VzY6q4cZp/0VVit5erpQB5rPa2dIkbp9svqPFua5NL/pMNSwFW3s3evfK4Ej0mL8XNhQvyeFMtd+EidG78QqwvrXJtbrvNejyTuOYa6apcfbX5lZUjww4mOjddiRsTcm4AvaEp3c6N6eLGqXPTVXgwVXGjIyxlf21rq3T4ktl+AYi/WkqXcwPIMUrluzpkCDB0qBRIO3cm/jovxQ0gBY6BUNz4hc7ETSaGpAApajZvBv72N6970jXxnBtTloIDXYelknVuVHu6nRudScXZ6ty4JW68LOIHhIullhZrSbgpq6WAcHGTbKhWuTfJhKa8KOJXUGCNlaGhKYobvxC5WqqpCdi9W96ePt2TLmnhrruAMWO87kXXdFbEL1UyxbnRkXOjklkB88RNOpwbJ58T9Vr7L2gdW03EQgngRNvXuVrK/torV8I3bUx0sYRb4iYvzyrWmapzA1h5N8kU8/Nq30DD824obvxC5Jf2f/9X2rY33RS9lQHRjxvOjRtLwYGuE4qTzblRE6ET5wZwp9aNjqXgmeLc2Cd7txKK/+//5HWiOXBuhqXU8dorQHeFW+ImEAj/nKSScwMA3/2uvG5stMa6KyhuYkJx4xfsX1ohrJDUffd516dsIl6dm0xybtSuxIkuu4/sh1Nx40aV4kxxbnRM+PZJRtc2ApGcPCmvBwxIrm/2JGBdgj/ZZGIg/j58Ot4H++ck1bBU377AjTfK21u3dv389nbnG8OmCsUNSQsqUa6tDTh2zLI1MzXfJtNwM+cmXeLmk0/k9YgRybWncLopqxvOjVt1blKpUAykx7kJBpPfWVrRVVgqWXETKyylQ/Dbw1LJiJviYuv/25OKneYq2V/rJCwFJLckXMfGsKmixE3kZ9oQKG78QmGhFXf+85+lyBk5MvGJijgjE3JuOgtLXbggt+8AgOHDE2tPt3PjRpVi050bnSJYTfa6hFwsUhU3usJS9vaS3XoBkOGjWCumdPRNR1gKsJKK33yz65pPOjaGTZXBg+V1KjuZpwGKGz+hLNfVq+U1Q1LpQ01eFy5IYZlpS8GVa9OvH9CrV3Ltxfs7WUwNS2Vazo0u9yEWTsJSbuXcJFtVPDLvRgj94sbJd3XyZCnijh2Ttco6Q8fGsKmiHKa6uvT+3wShuPETasXUiRPymiGp9KEmL0D+otRZxE/XUnD1uljiQYmba65Jvj2FiWEp0yoUnzsX/mtcp5uhnAy3nJsrV6wK6E7CUrpXSzkVNypnxWnfYombVL4TPXoAEybI212FprxKJgaA731PXv/jH1Y5CIOguPET9kJ9gwcD48Z515dsIz/fOsGcPeuOc+PE6gas/nXm3DgRN34NS+l0btrbrVVpgHnOTWfvgVq9k5eXeFFQN8NSusSN6pfTvtnHzul3NdEl4V6Km6uvBq67Tor1RJKf0wzFjZ+wn3B+9COr7gJJD/bQgw7nRvdS8ETCUl6KGzc2zzQl56ZHD2u5sj005cZqKbecG5WTVVaWeMKyvXaRcqx05aHpEjf2YzUhoRiw8m62bgU6OuI/z4sCfnaUe7Nlizf/vxMobvxEpLgh6cUubkzMuensV3kq4iZyEjXZufG6zk0gEHt1ic7Ec7dzbpLNtwGs47K7VbrCUsluvaBQCcWRzk0g4GyPJF1hKUDWJysqkknPH3wQ/3leOjcAMGWKvN6yRe+GtxqguPETStz06wdMmuRtX7IRe62bTNo4Uwgzcm7cSCjWsRRch3MDxE4qdiMs5ZZz40Tc2Ksn6w5LJbNaCrDOk2q1lP09cOJ2q7FTm0kCqX9Xg0Hgjjvk7c7ybrwWN7ffbiU/HzrkTR/iQHHjJyZOlNePPmrkLq2+xy3npq1NWtNuiZuvvpJ9DgSAYcOSby/e38liakKxDucGiC70CJiXc6Nb3Ki+2Z0bL+vcAPFzbpy8B4A1dl9/bd3n5DuRSN6N013gndKjB3DrrfK2YaumKG78xB13yD2lfvtbr3uSncTKudExaQGyPbfCUsq1GTw4ubazJaHYTedGZ9VeHTk3nb0HOsJSwaAzd8SNnBtd4kaN3Zkz8tq+JUMqqLybXbviC36vnRvA2Lwbihu/UVqaenVS4gw1eZ05ozeXAtAjbuI5N6mEpGL1w8SwlEnOjdthKbWk2STnJlLcOBUQbq6W0u3cdOvmTMiNGiVrTl28CBw8GPs5JogblXezbVv4snqP4SxIiC7cdG5aW913bpyKG7+GpWI5N6mIVrfFjcLEnBu7c+MEHQnFkfvw6dh6wf565dw4/T4EAtamx2rsIzFB3IwdC/TuLd+PvXu960cEFDeE6CJWzo2Tk3lurvXLT4e46cq5SXarjmyrUNzebo2dic6NwiTnJjLnxqk7EisslWxCsVotperR6HZudIkbwBprk8VNbq6VH2RQaIrihhBd6HZu7K/XUTvD7bCU0xOsm86NjqXggDWhmixu3HBu2tutIn5OVkuZEJYqKrL6cfq0e2Epp2FaIDPEDWCFpgxKKqa4IUQXup0bIPyXryrmpTMs1d5uLeF0Im4KC53nermRUKxzKTiQGeLGjQrFp07Jz0ogIPP6EkV3zo2O1VKBQHhoSndCsT3nximZIm5UUvGePcbsEk5xQ4gu7Et9dZ0w1evtJwydzs2JE/JXen6+tctvohQUWGEzHSdyU8NSeXmWcFN5HrrFjY5K1go3nBs1ufbvn9wGjW6FpS5dsrY4SFbcAO6IGy/DUl5VKFZUVMgfR+3tMrHYAChuCNGF7u0XgOjS+k6Wl6qT7ZUrVpExFZL61reSr40UCFgTqY4TuakJxfbX+925iSdu1NYLyYSkAOvzr94HXWEp+0aNTsWNOlY3Vks5JVOcG8C4JeEUN4ToQncRP/vr1YRYWJj68lL7CVCdFFPNt4lsU0d+gW7nRgh9J3/Vt0xIKHbTuUlV3MT7O1nsuTKAdJFSEXOxnBtdq6XUDxudOTdKXEbidRE/O4bl3VDcEKILe50b3c6NmhCd/Bq0nwDVJO1U3Kj+mOjc2Hd71uXcKEwq4pcO5yZVcRPZN13OjRI3PXumJvbt+0vpDkspdDs3sfZuMsm5ueMO6f4eOgQcOeJ1byhuCNGGmrwuXrTyAXSdzHWIm9zc6DCBieJGl3Njb0eXc6Pwq3MT7z3Q5dzocjJVWCqVkBQQvr+U7oRihU5xc/ly7ERdk8RNz57ALbfI2wa4NxQ3hOhCTV6AVf1Ud1jK6QkzMqlYl7gxMSyl2gkE9E9cTsWNEDLvSa2A07H9gsIk58YtcaOcG6fixk3nRsd3ols3a6FCrLwbk8QNYFRoiuKGEF0Eg9Zkr8SNSWEp++svX5aXo0fl3yY5N7rCUvZl4E7K4APRE5eTCsXt7eHF4wBznJtMCUup/pksbnR8J4DOk4pNEzcqqfjNN61FCx5BcUOITtSvLF1hqcjVUrq2OLh0CfjsM+kg9Owpl/imQiaEpXSc+HU4Nz16WCvS7EnnqbYX77XZ4NwodIgb3dsvKLJR3Nx0kxTxX38N1Nd72hWKG0J0Yg9NAeY6N5cuhYekUnU2dIobt8JSOk78OnJuAgFrqwD75qqAvg1WAX3OjUpgFcKaWMvLk2vPrdVSimS3XlDQuXGHvDzgu9+Vtz1eEk5xQ4hOIsWNaTk3dnfEab6NvT868gt0h6VMc26A2OUC8vKchc3cqFDc0WHt8Pz111Zfy8qc9U1XWEqRqnNjXy2lq85N5GdEx3cCSEzceF3Ez44heTcUN4ToJFOdG6ft0blJDDdrISl0ODeANemrSbV37+QnUdPDUq2tVtE9Ojd6UHk3775rhec9wHNxs3LlSlRWVqKwsBBVVVXYtWtX3OeePHkSP/nJTzBixAjk5ORg/vz56esoIYlgunNjsrjJVudGt5uhI+cGiBY3yebbAOaKm+7drfdUFcgzXdzEKuRnorgZNgz4n/+RW7vocq9SwFNxs379esyfPx9LlizBvn37MHnyZEydOhXHjx+P+fyWlhb069cPS5YswZgxY9LcW0ISQLdzo7PODRA7LDViROrtDRkSfu0E1Tf79hBOcNO5SWZ/JTtu7hyv0LWPlg5xY2pYyr555hdfyGsTl4IDnTs3JlUotnP//akvUtCEp+Jm+fLleOSRR/Doo49i5MiR+MMf/oBBgwahtrY25vOHDBmCZ599FjNnzkRJ5CRCiAlkinPT1AQ0N8vbw4en3t4vfwns2AE88oizfgGxXQMnqKXgOl0lQL4nqebIpCMspWvVj3oPUt1XCnA/oThVcQNY4kYdH1dL+QrPxE1rayvq6+sxRSUf/YspU6Zg9+7d2v5PS0sLzp07F3YhxDXcyrlRv/J1OTf798vrAQOcTRCFhcBttzk/TnvfAD3iZutWed2rl/O27BOXEzFies6N/fVq4jQ5LJXqainAEjdK5JtYoRiwVqidP2/trq6guImLZ+Lm1KlTaG9vR2lpadj9paWlaGpq0vZ/ampqUFJSEroMGjRIW9uERKHq3Ch0n8x1OTcffCCvneTb6MYeEjl82Flbu3cDq1bJ2/PmOWsLiHZuUsX0nBv76/0clgKsFVNqybupOTfFxbJGEhDt3lDcxMXzhOJAhL0rhIi6zwmLFi3C2bNnQ5cTJ05oa5uQKNxybhS6xM3Bg/LaJHETCABTp8rb//7vwIULqbXT1gb89Kfy9uzZwOTJzvtmn7icvKduiJucnPAcIF37aPk5oRiwnJt4bSeLWzk3QOzQlM5d732IZ+Kmb9++yM3NjXJpmpubo9wcJxQUFKBnz55hF0Jcw62cG4WusJTa08gkcQMAa9ZIG/7AAeA//iP2TshdsWKFDLv17g3853/q6Zebzo2OkJ69DZOcG93iRqdz47a40eXcALHFjc5d732IZ+ImPz8fVVVVqIso9FNXV4eJEyd61CtCHBIpblJdVaNwy7lRmCZu+vcH1q+X2xT8938DK1cm9/rPPweWLpW3f/97oF8/Pf0yOecmsg23xE2y1YkB+T7m2KYZPzs3kQ6a2+LGXg/KpCJ+huBpWGrhwoVYtWoV1qxZgwMHDmDBggU4fvw4qqurAciQ0syZM8Ne09DQgIaGBly4cAFfffUVGhoa8PHHH3vRfUKisYubYND5ho2Rv1T9Lm4AYNIky3FZsADYsyfx186fL8NZEycCc+bo65PJOTf2NvLyrP2rUsWeUGxPYk3FuQHCP8OmbL8ARIsbHQLB/jlxIyxlr3VjFzc6PkM+w+HPSmfMmDEDp0+fxlNPPYWTJ09i9OjR2LRpEyoqKgDIon2RNW/Gjh0bul1fX4+//OUvqKiowFG1uzEhXmIXN7p/kQP6wlKA/KU5dKiz9txiwQLg7beBV18FHngAeP/96Mkokk2bgA0b5OReWxvuGDglU5wbnTV9Wlosp8Ce1JoswaC+LQ5MDksBcuxUrli6nBsdu977EE/FDQDMnTsXc+fOjfnYiy++GHWfSCUGT0i6iHRunOJmWKqy0txffIGAzL/Zvx84dAh4+GEpXuIJlosXgccfl7fnzwduuEFvf9xwbnQV8bO3ocN9iCVuUnVtgPDj0x2WSlVwAdZqqXhtp4J9/N0WN6YW8DMEz1dLEeIr7Da56c6NiSEpOyUl0onp1g3YvBn47W/jP/fpp4EjR4CBA4Fly/T3xQ3nRpebYW/DLefGibixi3ydzk1RkTN3zi3nBpDhQaf5dna6cm5IFJ47N4T4imBQ/pr85hvznRvTxQ0gHZjaWrmke9ky4OhRudXD1VdLITNwoKxErHJ0VqyQk55udDs37e3AmTPO21Ooz5pu50ZVxjZF3OTkyLBje7uzkBTgrnOje08lipukobghRDclJVLcmOjcZJq4AYBZs4C33pJF+daujf+8adOA6dPd6YMu50a5DR0dwFdfOW8vsk86xc3ly862XlDoDEupNi5dci5uuneXF7VztY6+KaGhMyQFWOP/z39K0VlQQHHTBRQ3hOimpEROCiY6N5kUlrJTWwt873uy/s3nn8vNDj//XF6+/lrWtPmv/3IvsVKXcxMIyM/H11+7I2507oDuRlhK13fi0iVnK6UUffsCatGKTmGoW9z07i2Pu7VV7gtXUUFx0wUUN4ToRoUedIYbFNno3AAyf+Hf/i32Y998I0MVbp7kdVUoBqLFjU4RbGJCsc6wlL09p84NEC5uTA5LBQLyPTh2TL4nFDddwoRiQnSjxI2Jzo16fbduMl/FD/To4f4JXpdzA1ifD1Odm1jiJpUCfgo3wlKAHnFjz7vRKW50OzdAdN4NxU2n0LkhRDc6nRvd4mbUKOCWW4Bbb9VbB8bv6Mq5AazPx6lTetoD3EsoNtG50Slu7CumMkXcqDwoJW5YnTgmFDeE6MZkcVNYCLzzjrM2spFsdG7OnLFWdJkkbnSHpRQmJxQDdG6ShD/dCNGNW2GpYNB5aX2SGm44NzpX6ejMuVGT5bFj1t+Re6Ylg1thKV0JxQqdrpfunBsgWtywiF+nUNwQopurrpLXup0bN34NksRww7nR1Z69DZ3OjdrSZsAAZ6vQ3FgtBZjp3DDnxhgobgjRjVvODcWNd7jh3Ohqz96GTvdBrSJyEpICMicspeP7SnFjDBQ3hOjmBz8AqqrkfkhOsZ9wKW68IxudG7X3lVNxkwmrpfLy9CTYpzMsRXHTKUwoJkQ3Q4cC772npy06N2aQlydDM0LoFzc6HAM3VkspTHNuKipkxerhw523pZwbXRvIVlaGX+tEvQ/NzUBbG8VNF9C5IcRkKG7MIBCwJhETnZu77pJVbO+803lbkZOlaeLm+eeBf/wDmDTJeVsjRsjyCNOmOW8LAObOBd59F3jiCT3t2enXT7pLQkiBQ3HTKXRuCDEZihtzKCiQZf91VCi2o2PCv/deWTdHx/YTkc6NkwJ+QPSKP6cUFQHjxztvB5DCoLFR37YdeXnAzTfraSuS3FygrEzWuTl5kuKmC+jcEGIyFDfmYLJzA+iboN0MS+kQN7pxaz8yN7AX8mMRv06huCHEZChuzEFNIqaKG124JW6CwcwSEiZiTyqmc9MpFDeEmAzFjTmY7tzoQre4Ucdn2nFmInZxwyJ+nUJxQ4jJcCm4OWSLc2OfLPPywjeXTAX1GTbtODMROjcJQ3FDiMnk5FhbLlDceIsu56aoKLymimmTvt25KStzXv+F4kYfFDcJQ3FDiOmoSYHixlsmTpTvwQ03OGsnEAjfF8m0Sd8ubpyGpADr+ExMJs40KG4ShuKGENOhuDGD5cuB06eBa65x3pY9NGXapK9b3NC50QfFTcKwzg0hpkNxYwaBgL73wC5uTJv0KW7MRb0fTU3WuFLcxITODSGmQ3HjP0wWN/bJ0mkBP4CrpXRSWiqv29pkrRuA4iYOFDeEmA7Fjf8wWdzQuTGX/Hy5DQNgLQVnEb+YUNwQYjpqcqC48Q8mixt7fyhuzCPyPaFzExOKG0JMh86N/zBZ3OTkWIKEq6XMg+ImIShuCDEdihv/YbK4AYDRo4GrrgKGD3fe1siRMhn7uuuct0UobhKEq6UIMZ3Zs2Uhv8mTve4J0YXp4uatt+RSY3s9nlQZP14mv/bv77wtQnGTIHRuCDGdn/0M2LsX6N3b654QXZgubrp31/t501HpmEgixQ0TimPCTxshhKQbJW4CAWt7DUISwS5ugkF+fuJAcUMIIelGiRsTXRtiNnZxw5BUXChuCCEk3VDckFShuEkIihtCCEk3ZWXy2p57Q0giUNwkBFdLEUJIuhk2DKitldeEJEO3blIUnz3LZOJOoLghhBAvqK72ugckUykvl+KGzk1cGJYihBBCMgkVmqK4iQvFDSGEEJJJUNx0CcUNIYQQkklQ3HSJ5+Jm5cqVqKysRGFhIaqqqrBr165On79jxw5UVVWhsLAQQ4cOxfPPP5+mnhJCCCEGoMQNE4rj4qm4Wb9+PebPn48lS5Zg3759mDx5MqZOnYrjx4/HfP6RI0fwgx/8AJMnT8a+ffuwePFiPPHEE9iwYUOae04IIYR4xNSpcqXd/fd73RNjCQghhFf//Oabb8a4ceNQW1sbum/kyJGYPn06ampqop7/5JNPYuPGjThw4EDovurqanzwwQd45513Evqf586dQ0lJCc6ePYueOjaFI4QQQojrJDN/e+bctLa2or6+HlOmTAm7f8qUKdi9e3fM17zzzjtRz7/77rvx3nvv4cqVKzFf09LSgnPnzoVdCCGEEOJfPBM3p06dQnt7O0pLS8PuLy0tRVNTU8zXNDU1xXx+W1sbTp06FfM1NTU1KCkpCV0GDRqk5wAIIYQQYiSeJxQHAoGwv4UQUfd19fxY9ysWLVqEs2fPhi4nTpxw2GNCCCGEmIxnFYr79u2L3NzcKJemubk5yp1RlJWVxXx+Xl4e+vTpE/M1BQUFKGBGOSGEEJI1eObc5Ofno6qqCnV1dWH319XVYeLEiTFfM2HChKjnb9myBePHj0cwGHStr4QQQgjJHDwNSy1cuBCrVq3CmjVrcODAASxYsADHjx9H9b/2XFm0aBFmzpwZen51dTWOHTuGhQsX4sCBA1izZg1Wr16NX/ziF14dAiGEEEIMw9ONM2fMmIHTp0/jqaeewsmTJzF69Ghs2rQJFRUVAICTJ0+G1byprKzEpk2bsGDBAjz33HMoLy/HihUrcD/X+hNCCCHkX3ha58YLWOeGEEIIyTwyos4NIYQQQogbUNwQQgghxFdQ3BBCCCHEV1DcEEIIIcRXUNwQQgghxFdQ3BBCCCHEV3ha58YL1Mp37g5OCCGEZA5q3k6kgk3WiZvz588DAHcHJ4QQQjKQ8+fPo6SkpNPnZF0Rv46ODnz55ZcoLi7udPfxVDh37hwGDRqEEydOZG2BQI6BhOPAMQA4BgqOA8cAcD4GQgicP38e5eXlyMnpPKsm65ybnJwcDBw40NX/0bNnz6z98Co4BhKOA8cA4BgoOA4cA8DZGHTl2CiYUEwIIYQQX0FxQwghhBBfQXGjkYKCAixduhQFBQVed8UzOAYSjgPHAOAYKDgOHAMgvWOQdQnFhBBCCPE3dG4IIYQQ4isobgghhBDiKyhuCCGEEOIrKG4IIYQQ4isobjSxcuVKVFZWorCwEFVVVdi1a5fXXXKVnTt34oc//CHKy8sRCATw+uuvhz0uhMCyZctQXl6Obt264Y477sBHH33kTWddoqamBjfddBOKi4vRv39/TJ8+HQcPHgx7jt/Hoba2FjfccEOoKNeECRPw97//PfS4348/FjU1NQgEApg/f37ovmwYh2XLliEQCIRdysrKQo9nwxgAwBdffIGHH34Yffr0Qffu3XHjjTeivr4+9Hg2jMOQIUOiPguBQACPPfYYgDSNgSCOWbdunQgGg+KFF14QH3/8sZg3b57o0aOHOHbsmNddc41NmzaJJUuWiA0bNggA4rXXXgt7/JlnnhHFxcViw4YNorGxUcyYMUMMGDBAnDt3zpsOu8Ddd98t1q5dKz788EPR0NAgpk2bJgYPHiwuXLgQeo7fx2Hjxo3ib3/7mzh48KA4ePCgWLx4sQgGg+LDDz8UQvj/+CPZu3evGDJkiLjhhhvEvHnzQvdnwzgsXbpUjBo1Spw8eTJ0aW5uDj2eDWPwz3/+U1RUVIjZs2eLPXv2iCNHjog33nhDHDp0KPScbBiH5ubmsM9BXV2dACC2bdsmhEjPGFDcaODb3/62qK6uDrvv2muvFb/61a886lF6iRQ3HR0doqysTDzzzDOh+y5fvixKSkrE888/70EP00Nzc7MAIHbs2CGEyN5x6NWrl1i1alXWHf/58+fF8OHDRV1dnbj99ttD4iZbxmHp0qVizJgxMR/LljF48sknxaRJk+I+ni3jEMm8efPEsGHDREdHR9rGgGEph7S2tqK+vh5TpkwJu3/KlCnYvXu3R73yliNHjqCpqSlsTAoKCnD77bf7ekzOnj0LAOjduzeA7BuH9vZ2rFu3Dt988w0mTJiQdcf/2GOPYdq0abjrrrvC7s+mcfj0009RXl6OyspK/PjHP8bhw4cBZM8YbNy4EePHj8cDDzyA/v37Y+zYsXjhhRdCj2fLONhpbW3Fyy+/jDlz5iAQCKRtDChuHHLq1Cm0t7ejtLQ07P7S0lI0NTV51CtvUcedTWMihMDChQsxadIkjB49GkD2jENjYyOKiopQUFCA6upqvPbaa7juuuuy5vgBYN26dXj//fdRU1MT9Vi2jMPNN9+Ml156CZs3b8YLL7yApqYmTJw4EadPn86aMTh8+DBqa2sxfPhwbN68GdXV1XjiiSfw0ksvAciez4Kd119/HWfOnMHs2bMBpG8Msm5XcLcIBAJhfwshou7LNrJpTB5//HHs378fb731VtRjfh+HESNGoKGhAWfOnMGGDRswa9Ys7NixI/S434//xIkTmDdvHrZs2YLCwsK4z/P7OEydOjV0+/rrr8eECRMwbNgw/PnPf8Ytt9wCwP9j0NHRgfHjx+Ppp58GAIwdOxYfffQRamtrMXPmzNDz/D4OdlavXo2pU6eivLw87H63x4DOjUP69u2L3NzcKMXZ3NwcpUyzBbVCIlvG5Gc/+xk2btyIbdu2YeDAgaH7s2Uc8vPz8a1vfQvjx49HTU0NxowZg2effTZrjr++vh7Nzc2oqqpCXl4e8vLysGPHDqxYsQJ5eXmhY/X7OETSo0cPXH/99fj000+z5rMwYMAAXHfddWH3jRw5EsePHweQPecExbFjx/DGG2/g0UcfDd2XrjGguHFIfn4+qqqqUFdXF3Z/XV0dJk6c6FGvvKWyshJlZWVhY9La2oodO3b4akyEEHj88cfx6quvYuvWraisrAx7PFvGIRIhBFpaWrLm+O+88040NjaioaEhdBk/fjweeughNDQ0YOjQoVkxDpG0tLTgwIEDGDBgQNZ8Fm699daochCffPIJKioqAGTfOWHt2rXo378/pk2bFrovbWOgLTU5i1FLwVevXi0+/vhjMX/+fNGjRw9x9OhRr7vmGufPnxf79u0T+/btEwDE8uXLxb59+0LL35955hlRUlIiXn31VdHY2CgefPBB3y13/OlPfypKSkrE9u3bw5Y9Xrx4MfQcv4/DokWLxM6dO8WRI0fE/v37xeLFi0VOTo7YsmWLEML/xx8P+2opIbJjHH7+85+L7du3i8OHD4t3331X3HPPPaK4uDh0HsyGMdi7d6/Iy8sTv/vd78Snn34qXnnlFdG9e3fx8ssvh56TDeMghBDt7e1i8ODB4sknn4x6LB1jQHGjieeee05UVFSI/Px8MW7cuNByYL+ybds2ASDqMmvWLCGEXPK4dOlSUVZWJgoKCsRtt90mGhsbve20ZmIdPwCxdu3a0HP8Pg5z5swJfe779esn7rzzzpCwEcL/xx+PSHGTDeOgapUEg0FRXl4u7rvvPvHRRx+FHs+GMRBCiL/+9a9i9OjRoqCgQFx77bXiT3/6U9jj2TIOmzdvFgDEwYMHox5LxxgEhBBCnw9ECCGEEOItzLkhhBBCiK+guCGEEEKIr6C4IYQQQoivoLghhBBCiK+guCGEEEKIr6C4IYQQQoivoLghhBBCiK+guCGEZBTLli3DjTfe6HU3CCEGwyJ+hBBj6GpX4FmzZuGPf/wjWlpa0KdPnzT1ihCSaVDcEEKMwb5T8Pr16/HrX/86bCPCbt26oaSkxIuuEUIyCIalCCHGUFZWFrqUlJQgEAhE3RcZlpo9ezamT5+Op59+GqWlpbjqqqvwm9/8Bm1tbfjlL3+J3r17Y+DAgVizZk3Y//riiy8wY8YM9OrVC3369MG9996Lo0ePpveACSGuQHFDCMl4tm7dii+//BI7d+7E8uXLsWzZMtxzzz3o1asX9uzZg+rqalRXV+PEiRMAgIsXL+I73/kOioqKsHPnTrz11lsoKirC97//fbS2tnp8NIQQp1DcEEIynt69e2PFihUYMWIE5syZgxEjRuDixYtYvHgxhg8fjkWLFiE/Px9vv/02AGDdunXIycnBqlWrcP3112PkyJFYu3Ytjh8/ju3bt3t7MIQQx+R53QFCCHHKqFGjkJNj/VYrLS3F6NGjQ3/n5uaiT58+aG5uBgDU19fj0KFDKC4uDmvn8uXL+Oyzz9LTaUKIa1DcEEIynmAwGPZ3IBCIeV9HRwcAoKOjA1VVVXjllVei2urXr597HSWEpAWKG0JI1jFu3DisX78e/fv3R8+ePb3uDiFEM8y5IYRkHQ899BD69u2Le++9F7t27cKRI0ewY8cOzJs3D59//rnX3SOEOITihhCSdXTv3h07d+7E4MGDcd9992HkyJGYM2cOLl26RCeHEB/AIn6EEEII8RV0bgghhBDiKyhuCCGEEOIrKG4IIYQQ4isobgghhBDiKyhuCCGEEOIrKG4IIYQQ4isobgghhBDiKyhuCCGEEOIrKG4IIYQQ4isobgghhBDiKyhuCCGEEOIrKG4IIYQQ4iv+H0HrZLaLQtJrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock_prediction(y_pred=y_pred_latest, validY=validY, scaler=scaler, numerical_cols=numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (time_series_env)",
   "language": "python",
   "name": "time_series_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
