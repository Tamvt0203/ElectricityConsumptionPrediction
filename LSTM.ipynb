{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đọc dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', index_col='date')\n",
    "df_test = pd.read_csv('test.csv',index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chia tập validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validate = df_train.iloc[-100:, :]\n",
    "df_train = df_train.iloc[:-100, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>new_total_usage_lag6</th>\n",
       "      <th>new_total_usage_lag7</th>\n",
       "      <th>new_total_usage_lag14</th>\n",
       "      <th>new_total_usage_lag21</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>new_total_usage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>False</td>\n",
       "      <td>28.12</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2618.0</td>\n",
       "      <td>2178.0</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-12</th>\n",
       "      <td>False</td>\n",
       "      <td>31.07</td>\n",
       "      <td>6149.0</td>\n",
       "      <td>6369.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>7128.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>True</td>\n",
       "      <td>23.25</td>\n",
       "      <td>2156.0</td>\n",
       "      <td>3982.0</td>\n",
       "      <td>4323.0</td>\n",
       "      <td>3784.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-16</th>\n",
       "      <td>False</td>\n",
       "      <td>22.06</td>\n",
       "      <td>4466.0</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>1331.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>False</td>\n",
       "      <td>26.62</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>5841.0</td>\n",
       "      <td>4862.0</td>\n",
       "      <td>5346.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5368.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  new_total_usage_lag6  \\\n",
       "date                                                        \n",
       "2024-03-30       False        28.12                2013.0   \n",
       "2023-07-12       False        31.07                6149.0   \n",
       "2024-02-14        True        23.25                2156.0   \n",
       "2023-02-16       False        22.06                4466.0   \n",
       "2022-05-20       False        26.62                2992.0   \n",
       "\n",
       "            new_total_usage_lag7  new_total_usage_lag14  \\\n",
       "date                                                      \n",
       "2024-03-30                2618.0                 2178.0   \n",
       "2023-07-12                6369.0                 2409.0   \n",
       "2024-02-14                3982.0                 4323.0   \n",
       "2023-02-16                4554.0                 1276.0   \n",
       "2022-05-20                5841.0                 4862.0   \n",
       "\n",
       "            new_total_usage_lag21  weekday_1  weekday_2  weekday_3  weekday_4  \\\n",
       "date                                                                            \n",
       "2024-03-30                 2299.0      False      False      False      False   \n",
       "2023-07-12                 7128.0      False       True      False      False   \n",
       "2024-02-14                 3784.0      False       True      False      False   \n",
       "2023-02-16                 1331.0      False      False       True      False   \n",
       "2022-05-20                 5346.0      False      False      False       True   \n",
       "\n",
       "            weekday_5  weekday_6  new_total_usage  \n",
       "date                                               \n",
       "2024-03-30       True      False           2508.0  \n",
       "2023-07-12      False      False           6347.0  \n",
       "2024-02-14      False      False           1573.0  \n",
       "2023-02-16      False      False           3817.0  \n",
       "2022-05-20      False      False           5368.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chuẩn bị dữ liệu để train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop các cột không dùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>new_total_usage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-03-30</th>\n",
       "      <td>False</td>\n",
       "      <td>28.12</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-12</th>\n",
       "      <td>False</td>\n",
       "      <td>31.07</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>True</td>\n",
       "      <td>23.25</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-16</th>\n",
       "      <td>False</td>\n",
       "      <td>22.06</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3817.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-20</th>\n",
       "      <td>False</td>\n",
       "      <td>26.62</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5368.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  weekday_1  weekday_2  weekday_3  \\\n",
       "date                                                                   \n",
       "2024-03-30       False        28.12      False      False      False   \n",
       "2023-07-12       False        31.07      False       True      False   \n",
       "2024-02-14        True        23.25      False       True      False   \n",
       "2023-02-16       False        22.06      False      False       True   \n",
       "2022-05-20       False        26.62      False      False      False   \n",
       "\n",
       "            weekday_4  weekday_5  weekday_6  new_total_usage  \n",
       "date                                                          \n",
       "2024-03-30      False       True      False           2508.0  \n",
       "2023-07-12      False      False      False           6347.0  \n",
       "2024-02-14      False      False      False           1573.0  \n",
       "2023-02-16      False      False      False           3817.0  \n",
       "2022-05-20       True      False      False           5368.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(['new_total_usage_lag6', 'new_total_usage_lag7', 'new_total_usage_lag14', 'new_total_usage_lag21'], axis='columns')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>new_total_usage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-13</th>\n",
       "      <td>False</td>\n",
       "      <td>25.60</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-24</th>\n",
       "      <td>False</td>\n",
       "      <td>20.10</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-30</th>\n",
       "      <td>False</td>\n",
       "      <td>26.78</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5951.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-18</th>\n",
       "      <td>False</td>\n",
       "      <td>22.22</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4345.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-28</th>\n",
       "      <td>False</td>\n",
       "      <td>27.29</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5038.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  weekday_1  weekday_2  weekday_3  \\\n",
       "date                                                                   \n",
       "2022-11-13       False        25.60      False      False      False   \n",
       "2023-12-24       False        20.10      False      False      False   \n",
       "2022-09-30       False        26.78      False      False      False   \n",
       "2024-01-18       False        22.22      False      False       True   \n",
       "2023-04-28       False        27.29      False      False      False   \n",
       "\n",
       "            weekday_4  weekday_5  weekday_6  new_total_usage  \n",
       "date                                                          \n",
       "2022-11-13      False      False       True           1364.0  \n",
       "2023-12-24      False      False       True           1309.0  \n",
       "2022-09-30       True      False      False           5951.0  \n",
       "2024-01-18      False      False      False           4345.0  \n",
       "2023-04-28       True      False      False           5038.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validate = df_validate.drop(['new_total_usage_lag6', 'new_total_usage_lag7', 'new_total_usage_lag14', 'new_total_usage_lag21'], axis='columns')\n",
    "df_validate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>new_total_usage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>False</td>\n",
       "      <td>21.46</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-12</th>\n",
       "      <td>False</td>\n",
       "      <td>27.57</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5654.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>False</td>\n",
       "      <td>24.71</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23</th>\n",
       "      <td>False</td>\n",
       "      <td>26.05</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>False</td>\n",
       "      <td>22.61</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>3597.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  weekday_1  weekday_2  weekday_3  \\\n",
       "date                                                                   \n",
       "2022-12-20       False        21.46       True      False      False   \n",
       "2024-04-12       False        27.57      False      False      False   \n",
       "2024-02-05       False        24.71      False      False      False   \n",
       "2022-09-23       False        26.05      False      False      False   \n",
       "2022-03-02       False        22.61      False       True      False   \n",
       "\n",
       "            weekday_4  weekday_5  weekday_6  new_total_usage  \n",
       "date                                                          \n",
       "2022-12-20      False      False      False           3454.0  \n",
       "2024-04-12       True      False      False           5654.0  \n",
       "2024-02-05      False      False      False           4499.0  \n",
       "2022-09-23       True      False      False           4917.0  \n",
       "2022-03-02      False      False      False           3597.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.drop(['new_total_usage_lag6', 'new_total_usage_lag7', 'new_total_usage_lag14', 'new_total_usage_lag21'], axis='columns')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuyển kiểu dữ liệu True/False thành 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_int(df, weekday_cols_prefix='weekday_', holiday_col='is_holiday', num_weekdays=6):\n",
    "    \"\"\"\n",
    "    Chuyển đổi các cột 'weekday' và 'is_holiday' của DataFrame sang kiểu dữ liệu int.\n",
    "\n",
    "    Tham số:\n",
    "    - df: DataFrame cần chuyển đổi.\n",
    "    - weekday_cols_prefix: Tiền tố của các cột 'weekday'. Mặc định là 'weekday_'.\n",
    "    - holiday_col: Tên cột ngày lễ cần chuyển đổi. Mặc định là 'is_holiday'.\n",
    "    - num_weekdays: Số lượng cột 'weekday' cần chuyển đổi (mặc định là 6).\n",
    "    \n",
    "    Trả về:\n",
    "    - DataFrame với các cột đã được chuyển sang kiểu int.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chuyển đổi các cột weekday_{i} thành int\n",
    "    for i in range(1, num_weekdays + 1):  # Lặp từ 1 tới num_weekdays\n",
    "        df[f'{weekday_cols_prefix}{i}'] = df[f'{weekday_cols_prefix}{i}'].astype(int)\n",
    "    \n",
    "    # Chuyển đổi cột is_holiday thành int\n",
    "    df[holiday_col] = df[holiday_col].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = convert_columns_to_int(df=df_train)\n",
    "df_validate = convert_columns_to_int(df=df_validate)\n",
    "df_test = convert_columns_to_int(df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale dữ liệu số về 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['temperature', 'new_total_usage']\n",
    "scaler = MinMaxScaler()\n",
    "df_train[numerical_cols] = scaler.fit_transform(df_train[numerical_cols])\n",
    "df_test[numerical_cols] = scaler.transform(df_test[numerical_cols])\n",
    "df_validate[numerical_cols] = scaler.transform(df_validate[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>new_total_usage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-20</th>\n",
       "      <td>0</td>\n",
       "      <td>0.199131</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.415254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-12</th>\n",
       "      <td>0</td>\n",
       "      <td>0.641564</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0.434468</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.549435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.531499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-02</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282404</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is_holiday  temperature  weekday_1  weekday_2  weekday_3  \\\n",
       "date                                                                   \n",
       "2022-12-20           0     0.199131          1          0          0   \n",
       "2024-04-12           0     0.641564          0          0          0   \n",
       "2024-02-05           0     0.434468          0          0          0   \n",
       "2022-09-23           0     0.531499          0          0          0   \n",
       "2022-03-02           0     0.282404          0          1          0   \n",
       "\n",
       "            weekday_4  weekday_5  weekday_6  new_total_usage  \n",
       "date                                                          \n",
       "2022-12-20          0          0          0         0.415254  \n",
       "2024-04-12          1          0          0         0.697740  \n",
       "2024-02-05          0          0          0         0.549435  \n",
       "2022-09-23          1          0          0         0.603107  \n",
       "2022-03-02          0          0          0         0.433616  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_past = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo tập X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giải thích: mỗi dòng dữ liệu ở tập X sẽ bao gồm n_past dòng dữ liệu ở quá khứ (bao gồm cả 8 features và 1 target value), tập y sẽ là target value ở dòng dữ liệu n_past+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            dataX.append(dataset.iloc[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset.iloc[i,-1])\n",
    "    return np.array(dataX),np.array(dataY)\n",
    "trainX,trainY=createXY(df_train,n_past=n_past)\n",
    "testX,testY=createXY(df_test,n_past=n_past)\n",
    "validX, validY = createXY(df_validate, n_past=n_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 30, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = trainX.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 30, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.6813903 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 0.29378531])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         0.000000\n",
       "temperature        0.681390\n",
       "weekday_1          0.000000\n",
       "weekday_2          0.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          1.000000\n",
       "weekday_6          0.000000\n",
       "new_total_usage    0.293785\n",
       "Name: 2024-03-30, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.66545981, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.92231638])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         0.000000\n",
       "temperature        0.665460\n",
       "weekday_1          0.000000\n",
       "weekday_2          0.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          0.000000\n",
       "weekday_6          0.000000\n",
       "new_total_usage    0.922316\n",
       "Name: 2023-06-05, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[29, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.744350282485945"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_holiday         0.000000\n",
       "temperature        0.416365\n",
       "weekday_1          0.000000\n",
       "weekday_2          1.000000\n",
       "weekday_3          0.000000\n",
       "weekday_4          0.000000\n",
       "weekday_5          0.000000\n",
       "weekday_6          0.000000\n",
       "new_total_usage    0.744350\n",
       "Name: 2022-11-30, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[30, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the build_model function\n",
    "def build_model(optimizer='adam'):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(50, return_sequences=True, input_shape=(n_past, n_cols)))\n",
    "    grid_model.add(LSTM(50))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1))\n",
    "    grid_model.compile(loss='mse', optimizer=optimizer)\n",
    "    return grid_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_prediction(y_pred, validY, scaler, numerical_cols, title='Predict Electricity Consumption'):\n",
    "    \"\"\"\n",
    "    Hàm dự đoán và vẽ biểu đồ cho dữ liệu giá cổ phiếu dựa trên mô hình và scaler.\n",
    "\n",
    "    Tham số:\n",
    "    - y_pred: Dự đoán của mô hình, đầu vào cần được reshape\n",
    "    - validY: Giá trị thực của dữ liệu kiểm tra, đầu vào cần được reshape\n",
    "    - scaler: Bộ scaler đã được dùng để chuẩn hóa dữ liệu\n",
    "    - numerical_cols: Số lượng cột dữ liệu (các thuộc tính số cần dự đoán)\n",
    "    - title: Tiêu đề của biểu đồ (tùy chọn)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape y_pred và validY để chuẩn bị cho việc đảo ngược chuẩn hóa\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "    validY = validY.reshape(-1, 1)\n",
    "    \n",
    "    # Tạo các bản sao của y_pred và validY với số cột bằng số lượng thuộc tính số\n",
    "    y_pred_copies_array = np.repeat(y_pred, len(numerical_cols), axis=-1)\n",
    "    y_copies_array = np.repeat(validY, len(numerical_cols), axis=-1)\n",
    "    \n",
    "    # Inverse transform để đưa dữ liệu về giá trị ban đầu\n",
    "    y_pred_before_scale = scaler.inverse_transform(y_pred_copies_array)[:, -1]\n",
    "    y_before_scale = scaler.inverse_transform(y_copies_array)[:, -1]\n",
    "    \n",
    "    # Vẽ biểu đồ\n",
    "    plt.plot(y_before_scale, color='red', label='Real Electricity Consumption')\n",
    "    plt.plot(y_pred_before_scale, color='blue', label='Predicted Electricity Consumption')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Electricity Consumption')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(optimizer='Adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved_models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the checkpoint callback to save only 5 most recent models\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(save_dir, 'model_epoch_{epoch:02d}_loss_{loss:.4f}.keras'), \n",
    "    monitor='loss',\n",
    "    save_best_only=False,  # Save the model every epoch\n",
    "    mode='auto',\n",
    "    verbose=1,\n",
    "    save_weights_only=False  # Save the entire model (architecture + weights)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(directory):\n",
    "    model_files = sorted(\n",
    "        [f for f in os.listdir(directory) if f.endswith('.keras')],\n",
    "        key=lambda x: os.path.getmtime(os.path.join(directory, x))\n",
    "    )\n",
    "    \n",
    "    if model_files:\n",
    "        latest_model = model_files[-1]\n",
    "        print(f\"Loading model: {latest_model}\")\n",
    "        return load_model(os.path.join(directory, latest_model))\n",
    "    else:\n",
    "        print(\"No model found, training from scratch.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for epoch set: 1 to 10\n",
      "No model found, training from scratch.\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2098\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.2115.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - loss: 0.2099 - val_loss: 0.2085\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.2280\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.2091.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.2270 - val_loss: 0.2042\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2050\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.2046.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.2049 - val_loss: 0.2000\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1916\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.2009.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.1921 - val_loss: 0.1957\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1857\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.1954.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1865 - val_loss: 0.1915\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1920\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.1918.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1920 - val_loss: 0.1872\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.1825\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.1893.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.1832 - val_loss: 0.1831\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1842\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.1861.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1844 - val_loss: 0.1789\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1686\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1751.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1690 - val_loss: 0.1748\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1856\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1745.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1850 - val_loss: 0.1707\n",
      "Epoch 1: Training Loss = 0.21146497130393982\n",
      "Epoch 2: Training Loss = 0.20908796787261963\n",
      "Epoch 3: Training Loss = 0.2046113908290863\n",
      "Epoch 4: Training Loss = 0.20087042450904846\n",
      "Epoch 5: Training Loss = 0.1953750103712082\n",
      "Epoch 6: Training Loss = 0.19179242849349976\n",
      "Epoch 7: Training Loss = 0.1892736554145813\n",
      "Epoch 8: Training Loss = 0.1860734522342682\n",
      "Epoch 9: Training Loss = 0.1750650852918625\n",
      "Epoch 10: Training Loss = 0.17447598278522491\n",
      "Starting training for epoch set: 11 to 20\n",
      "Loading model: model_epoch_10_loss_0.1745.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1623\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1696.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.1628 - val_loss: 0.1667\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1610\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.1621.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.1611 - val_loss: 0.1627\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1422\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.1606.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1427 - val_loss: 0.1588\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1600\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.1579.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1599 - val_loss: 0.1549\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1477\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.1549.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1483 - val_loss: 0.1511\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1383\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.1493.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1392 - val_loss: 0.1473\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.1390\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.1473.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1395 - val_loss: 0.1436\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1503\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.1431.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.1501 - val_loss: 0.1399\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1387\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1381.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1387 - val_loss: 0.1364\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.1424\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1343.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1418 - val_loss: 0.1329\n",
      "Epoch 11: Training Loss = 0.1696227341890335\n",
      "Epoch 12: Training Loss = 0.16210080683231354\n",
      "Epoch 13: Training Loss = 0.160573348402977\n",
      "Epoch 14: Training Loss = 0.15791811048984528\n",
      "Epoch 15: Training Loss = 0.1548590213060379\n",
      "Epoch 16: Training Loss = 0.14932775497436523\n",
      "Epoch 17: Training Loss = 0.14730188250541687\n",
      "Epoch 18: Training Loss = 0.14313380420207977\n",
      "Epoch 19: Training Loss = 0.13810019195079803\n",
      "Epoch 20: Training Loss = 0.1342887282371521\n",
      "Starting training for epoch set: 21 to 30\n",
      "Loading model: model_epoch_10_loss_0.1343.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1292\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1333.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.1295 - val_loss: 0.1296\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1298\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.1282.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1298 - val_loss: 0.1263\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1201\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.1234.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.1204 - val_loss: 0.1231\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1304\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.1202.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.1293 - val_loss: 0.1200\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.1231\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.1239.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.1231 - val_loss: 0.1170\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.1055\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.1137.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.1059 - val_loss: 0.1141\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.1145\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.1121.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.1143 - val_loss: 0.1113\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1010\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.1126.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1013 - val_loss: 0.1087\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.1009\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.1088.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.1012 - val_loss: 0.1060\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1051\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.1060.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.1051 - val_loss: 0.1035\n",
      "Epoch 21: Training Loss = 0.13327187299728394\n",
      "Epoch 22: Training Loss = 0.12824666500091553\n",
      "Epoch 23: Training Loss = 0.12337709218263626\n",
      "Epoch 24: Training Loss = 0.12019246816635132\n",
      "Epoch 25: Training Loss = 0.1238950565457344\n",
      "Epoch 26: Training Loss = 0.1136898547410965\n",
      "Epoch 27: Training Loss = 0.11214789003133774\n",
      "Epoch 28: Training Loss = 0.11257569491863251\n",
      "Epoch 29: Training Loss = 0.10875526815652847\n",
      "Epoch 30: Training Loss = 0.10596439242362976\n",
      "Starting training for epoch set: 31 to 40\n",
      "Loading model: model_epoch_10_loss_0.1060.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.1036\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.1004.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.1032 - val_loss: 0.1011\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0981\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0989.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0981 - val_loss: 0.0989\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0972\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0964.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0974\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0950.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0972 - val_loss: 0.0947\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0992\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0950.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0989 - val_loss: 0.0928\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0970\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0950.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0968 - val_loss: 0.0910\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0953\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0925.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0951 - val_loss: 0.0893\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0916\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0886.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0915 - val_loss: 0.0877\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0890\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0894.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0890 - val_loss: 0.0861\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0820\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0848.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0823 - val_loss: 0.0847\n",
      "Epoch 31: Training Loss = 0.1003536731004715\n",
      "Epoch 32: Training Loss = 0.0988621711730957\n",
      "Epoch 33: Training Loss = 0.09644465148448944\n",
      "Epoch 34: Training Loss = 0.09497388452291489\n",
      "Epoch 35: Training Loss = 0.09495387971401215\n",
      "Epoch 36: Training Loss = 0.09498783200979233\n",
      "Epoch 37: Training Loss = 0.09245215356349945\n",
      "Epoch 38: Training Loss = 0.08860211819410324\n",
      "Epoch 39: Training Loss = 0.0894021987915039\n",
      "Epoch 40: Training Loss = 0.08478576689958572\n",
      "Starting training for epoch set: 41 to 50\n",
      "Loading model: model_epoch_10_loss_0.0848.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0891\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0857.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0890 - val_loss: 0.0833\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0857\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0825.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0856 - val_loss: 0.0821\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0843\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0811.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0842 - val_loss: 0.0810\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0819\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0782.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0817 - val_loss: 0.0799\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0751\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0787.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0752 - val_loss: 0.0789\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0755\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0801.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0757 - val_loss: 0.0780\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0779\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0804.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0781 - val_loss: 0.0772\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0697\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0755.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0701 - val_loss: 0.0764\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0758\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0764.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0758 - val_loss: 0.0757\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0711\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0755.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0714 - val_loss: 0.0751\n",
      "Epoch 41: Training Loss = 0.08573371171951294\n",
      "Epoch 42: Training Loss = 0.08252114802598953\n",
      "Epoch 43: Training Loss = 0.08112090826034546\n",
      "Epoch 44: Training Loss = 0.07815726846456528\n",
      "Epoch 45: Training Loss = 0.07869887351989746\n",
      "Epoch 46: Training Loss = 0.0801488533616066\n",
      "Epoch 47: Training Loss = 0.08042271435260773\n",
      "Epoch 48: Training Loss = 0.07550039887428284\n",
      "Epoch 49: Training Loss = 0.07635898143053055\n",
      "Epoch 50: Training Loss = 0.07545243203639984\n",
      "Starting training for epoch set: 51 to 60\n",
      "Loading model: model_epoch_10_loss_0.0755.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0755\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0762.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0756 - val_loss: 0.0745\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0751\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0757.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0751 - val_loss: 0.0739\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0792\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0754.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0787 - val_loss: 0.0734\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0729\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0724.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0729 - val_loss: 0.0730\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0740\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0740.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0740 - val_loss: 0.0725\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0705\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0708.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0705 - val_loss: 0.0721\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0732\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0728.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0732 - val_loss: 0.0718\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0683\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0720.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - loss: 0.0685 - val_loss: 0.0714\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0684\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0732.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0686 - val_loss: 0.0711\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0721\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0720 - val_loss: 0.0709\n",
      "Epoch 51: Training Loss = 0.07616009563207626\n",
      "Epoch 52: Training Loss = 0.07574819028377533\n",
      "Epoch 53: Training Loss = 0.07539397478103638\n",
      "Epoch 54: Training Loss = 0.07238142937421799\n",
      "Epoch 55: Training Loss = 0.07398983836174011\n",
      "Epoch 56: Training Loss = 0.07078763097524643\n",
      "Epoch 57: Training Loss = 0.07276596128940582\n",
      "Epoch 58: Training Loss = 0.07201704382896423\n",
      "Epoch 59: Training Loss = 0.07324732095003128\n",
      "Epoch 60: Training Loss = 0.07070781290531158\n",
      "Starting training for epoch set: 61 to 70\n",
      "Loading model: model_epoch_10_loss_0.0707.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0666\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0705.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - loss: 0.0670 - val_loss: 0.0706\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0673\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0675 - val_loss: 0.0704\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0725\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0732.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0725 - val_loss: 0.0702\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0662\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0665 - val_loss: 0.0701\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0771\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0767 - val_loss: 0.0699\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0692\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0731.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0695 - val_loss: 0.0698\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0751\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0708.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - loss: 0.0749 - val_loss: 0.0696\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0741\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0716.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0739 - val_loss: 0.0696\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0736\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0712.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0735 - val_loss: 0.0694\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0709\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0709 - val_loss: 0.0693\n",
      "Epoch 61: Training Loss = 0.07054849714040756\n",
      "Epoch 62: Training Loss = 0.07031768560409546\n",
      "Epoch 63: Training Loss = 0.07319454103708267\n",
      "Epoch 64: Training Loss = 0.06972265243530273\n",
      "Epoch 65: Training Loss = 0.07019049674272537\n",
      "Epoch 66: Training Loss = 0.07312075793743134\n",
      "Epoch 67: Training Loss = 0.07083259522914886\n",
      "Epoch 68: Training Loss = 0.07156578451395035\n",
      "Epoch 69: Training Loss = 0.07123461365699768\n",
      "Epoch 70: Training Loss = 0.07021664828062057\n",
      "Starting training for epoch set: 71 to 80\n",
      "Loading model: model_epoch_10_loss_0.0702.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0680\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0681 - val_loss: 0.0693\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0691\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0691 - val_loss: 0.0692\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0687\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0687 - val_loss: 0.0691\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0733\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0718.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0732 - val_loss: 0.0691\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0680\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0681 - val_loss: 0.0690\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0738\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0735 - val_loss: 0.0690\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0697\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0697 - val_loss: 0.0689\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0678\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0679.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.0678 - val_loss: 0.0689\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0663\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0665 - val_loss: 0.0689\n",
      "Epoch 10/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0677\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0719.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0681 - val_loss: 0.0688\n",
      "Epoch 71: Training Loss = 0.06919063627719879\n",
      "Epoch 72: Training Loss = 0.06823861598968506\n",
      "Epoch 73: Training Loss = 0.06933431327342987\n",
      "Epoch 74: Training Loss = 0.07182450592517853\n",
      "Epoch 75: Training Loss = 0.07020928710699081\n",
      "Epoch 76: Training Loss = 0.07066255062818527\n",
      "Epoch 77: Training Loss = 0.07027775794267654\n",
      "Epoch 78: Training Loss = 0.06794657558202744\n",
      "Epoch 79: Training Loss = 0.07102817296981812\n",
      "Epoch 80: Training Loss = 0.07189549505710602\n",
      "Starting training for epoch set: 81 to 90\n",
      "Loading model: model_epoch_10_loss_0.0719.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0681\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0711.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 0.0683 - val_loss: 0.0688\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0671\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0671 - val_loss: 0.0688\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0703\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0714.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0704 - val_loss: 0.0688\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0681\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0681 - val_loss: 0.0688\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0674\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0675 - val_loss: 0.0687\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0718\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0713.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0717 - val_loss: 0.0687\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0718\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0694.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0718 - val_loss: 0.0687\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0730\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0709.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0729 - val_loss: 0.0687\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0709\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0708 - val_loss: 0.0687\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0698\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0718.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0698 - val_loss: 0.0687\n",
      "Epoch 81: Training Loss = 0.07105948030948639\n",
      "Epoch 82: Training Loss = 0.0676179900765419\n",
      "Epoch 83: Training Loss = 0.07140311598777771\n",
      "Epoch 84: Training Loss = 0.06813349574804306\n",
      "Epoch 85: Training Loss = 0.06867750734090805\n",
      "Epoch 86: Training Loss = 0.07129929214715958\n",
      "Epoch 87: Training Loss = 0.06936236470937729\n",
      "Epoch 88: Training Loss = 0.07092225551605225\n",
      "Epoch 89: Training Loss = 0.06923917680978775\n",
      "Epoch 90: Training Loss = 0.0718308836221695\n",
      "Starting training for epoch set: 91 to 100\n",
      "Loading model: model_epoch_10_loss_0.0718.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0671\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0696.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0673 - val_loss: 0.0687\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0681\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0704.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0683 - val_loss: 0.0687\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0698\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0715.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0700 - val_loss: 0.0687\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0704\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0700.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0704 - val_loss: 0.0687\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0703\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0703 - val_loss: 0.0687\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0710\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0727.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0711 - val_loss: 0.0687\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0691\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0719.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0692 - val_loss: 0.0687\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0731\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0723.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0730 - val_loss: 0.0687\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0696\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0696 - val_loss: 0.0687\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0675\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0705.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0678 - val_loss: 0.0686\n",
      "Epoch 91: Training Loss = 0.06959288567304611\n",
      "Epoch 92: Training Loss = 0.07038727402687073\n",
      "Epoch 93: Training Loss = 0.07150953263044357\n",
      "Epoch 94: Training Loss = 0.06998194009065628\n",
      "Epoch 95: Training Loss = 0.07073444128036499\n",
      "Epoch 96: Training Loss = 0.07266555726528168\n",
      "Epoch 97: Training Loss = 0.07187020778656006\n",
      "Epoch 98: Training Loss = 0.07225576043128967\n",
      "Epoch 99: Training Loss = 0.07022461295127869\n",
      "Epoch 100: Training Loss = 0.0704938992857933\n",
      "Starting training for epoch set: 101 to 110\n",
      "Loading model: model_epoch_10_loss_0.0705.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0674\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0665.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0673 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0729\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0715.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0727 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0711\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0716.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0711 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0710\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0708 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0692\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0692 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0704\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0694.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0703 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0695\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0695 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0687\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 50ms/step - loss: 0.0687 - val_loss: 0.0686\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0668\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0669 - val_loss: 0.0686\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0706\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0705 - val_loss: 0.0686\n",
      "Epoch 101: Training Loss = 0.06647560000419617\n",
      "Epoch 102: Training Loss = 0.0715169757604599\n",
      "Epoch 103: Training Loss = 0.0715714544057846\n",
      "Epoch 104: Training Loss = 0.06898477673530579\n",
      "Epoch 105: Training Loss = 0.0702415332198143\n",
      "Epoch 106: Training Loss = 0.06943386793136597\n",
      "Epoch 107: Training Loss = 0.06994727998971939\n",
      "Epoch 108: Training Loss = 0.06829781085252762\n",
      "Epoch 109: Training Loss = 0.0691487193107605\n",
      "Epoch 110: Training Loss = 0.06795039772987366\n",
      "Starting training for epoch set: 111 to 120\n",
      "Loading model: model_epoch_10_loss_0.0680.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0724\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0721 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0693\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0693 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0661\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0662 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0665\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0667 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0669\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0670 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0664\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0667 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0744\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0709.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0741 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0687\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0687 - val_loss: 0.0686\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0731\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0696.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0728 - val_loss: 0.0686\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0651\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0654 - val_loss: 0.0686\n",
      "Epoch 111: Training Loss = 0.06934556365013123\n",
      "Epoch 112: Training Loss = 0.06924839317798615\n",
      "Epoch 113: Training Loss = 0.06819135695695877\n",
      "Epoch 114: Training Loss = 0.06827664375305176\n",
      "Epoch 115: Training Loss = 0.06771807372570038\n",
      "Epoch 116: Training Loss = 0.06847880035638809\n",
      "Epoch 117: Training Loss = 0.07085686177015305\n",
      "Epoch 118: Training Loss = 0.06895705312490463\n",
      "Epoch 119: Training Loss = 0.06963344663381577\n",
      "Epoch 120: Training Loss = 0.06913705915212631\n",
      "Starting training for epoch set: 121 to 130\n",
      "Loading model: model_epoch_10_loss_0.0691.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0719\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0719.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - loss: 0.0719 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0662\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0705.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0665 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0655\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0658 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0706\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0704 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0691\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0690 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0693\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0692 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0691\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0691 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0636\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0673.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0638 - val_loss: 0.0686\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0719\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0718.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0719 - val_loss: 0.0686\n",
      "Epoch 10/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0694\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0695 - val_loss: 0.0686\n",
      "Epoch 121: Training Loss = 0.07187020778656006\n",
      "Epoch 122: Training Loss = 0.07046568393707275\n",
      "Epoch 123: Training Loss = 0.06846564263105392\n",
      "Epoch 124: Training Loss = 0.0697614848613739\n",
      "Epoch 125: Training Loss = 0.06806832551956177\n",
      "Epoch 126: Training Loss = 0.0678417980670929\n",
      "Epoch 127: Training Loss = 0.06883686035871506\n",
      "Epoch 128: Training Loss = 0.0673261508345604\n",
      "Epoch 129: Training Loss = 0.0717770904302597\n",
      "Epoch 130: Training Loss = 0.07018227875232697\n",
      "Starting training for epoch set: 131 to 140\n",
      "Loading model: model_epoch_10_loss_0.0702.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0695\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0695 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0704\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0701 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0719\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0719 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0716\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0714 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0669\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0670 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0708\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0708 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0693\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0693 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0712\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0712.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0712 - val_loss: 0.0686\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0696\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 0.0696 - val_loss: 0.0686\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0635\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 0.0638 - val_loss: 0.0686\n",
      "Epoch 131: Training Loss = 0.070668064057827\n",
      "Epoch 132: Training Loss = 0.06824813038110733\n",
      "Epoch 133: Training Loss = 0.06945888698101044\n",
      "Epoch 134: Training Loss = 0.06951437890529633\n",
      "Epoch 135: Training Loss = 0.06876567006111145\n",
      "Epoch 136: Training Loss = 0.07066337764263153\n",
      "Epoch 137: Training Loss = 0.06797301024198532\n",
      "Epoch 138: Training Loss = 0.07118907570838928\n",
      "Epoch 139: Training Loss = 0.06954457610845566\n",
      "Epoch 140: Training Loss = 0.06815541535615921\n",
      "Starting training for epoch set: 141 to 150\n",
      "Loading model: model_epoch_10_loss_0.0682.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0730\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0701.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 36ms/step - loss: 0.0727 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0681\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0683 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0690\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0689 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0654\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0657 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0721\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0719 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0677\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0677 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0656\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0658 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0676\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0678 - val_loss: 0.0686\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0733\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0713.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0731 - val_loss: 0.0686\n",
      "Epoch 10/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0704\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0701 - val_loss: 0.0686\n",
      "Epoch 141: Training Loss = 0.07007624208927155\n",
      "Epoch 142: Training Loss = 0.0696881040930748\n",
      "Epoch 143: Training Loss = 0.06797556579113007\n",
      "Epoch 144: Training Loss = 0.06828296184539795\n",
      "Epoch 145: Training Loss = 0.06878092885017395\n",
      "Epoch 146: Training Loss = 0.06916072964668274\n",
      "Epoch 147: Training Loss = 0.06945942342281342\n",
      "Epoch 148: Training Loss = 0.07025336474180222\n",
      "Epoch 149: Training Loss = 0.0712663009762764\n",
      "Epoch 150: Training Loss = 0.06771890819072723\n",
      "Starting training for epoch set: 151 to 160\n",
      "Loading model: model_epoch_10_loss_0.0677.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0664\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0665 - val_loss: 0.0686\n",
      "Epoch 2/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0771\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0701.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0763 - val_loss: 0.0686\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0705\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0715.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0705 - val_loss: 0.0686\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0696\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0695 - val_loss: 0.0686\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0666\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0694.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0667 - val_loss: 0.0686\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0665\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0667 - val_loss: 0.0686\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0678\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0678 - val_loss: 0.0686\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0691\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0705.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0691 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0709\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0708 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0709\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0709 - val_loss: 0.0685\n",
      "Epoch 151: Training Loss = 0.06828329712152481\n",
      "Epoch 152: Training Loss = 0.07013589143753052\n",
      "Epoch 153: Training Loss = 0.07152208685874939\n",
      "Epoch 154: Training Loss = 0.0689239576458931\n",
      "Epoch 155: Training Loss = 0.06939776241779327\n",
      "Epoch 156: Training Loss = 0.06923390924930573\n",
      "Epoch 157: Training Loss = 0.06804827600717545\n",
      "Epoch 158: Training Loss = 0.07048344612121582\n",
      "Epoch 159: Training Loss = 0.07020918279886246\n",
      "Epoch 160: Training Loss = 0.06994213163852692\n",
      "Starting training for epoch set: 161 to 170\n",
      "Loading model: model_epoch_10_loss_0.0699.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0706\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0713.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 0.0706 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0666\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0667 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0700\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0699 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0697\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0696 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0695\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0695 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0639\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0643 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0650\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0653 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0682\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0682 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0702\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0701 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0681\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0681 - val_loss: 0.0685\n",
      "Epoch 161: Training Loss = 0.07126354426145554\n",
      "Epoch 162: Training Loss = 0.06974776089191437\n",
      "Epoch 163: Training Loss = 0.06905529648065567\n",
      "Epoch 164: Training Loss = 0.06765235960483551\n",
      "Epoch 165: Training Loss = 0.07028982043266296\n",
      "Epoch 166: Training Loss = 0.06723837554454803\n",
      "Epoch 167: Training Loss = 0.06877975910902023\n",
      "Epoch 168: Training Loss = 0.06916027516126633\n",
      "Epoch 169: Training Loss = 0.06854119896888733\n",
      "Epoch 170: Training Loss = 0.06865935027599335\n",
      "Starting training for epoch set: 171 to 180\n",
      "Loading model: model_epoch_10_loss_0.0687.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0707\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.0707 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0692\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0692 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0655\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0656 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0691\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0691 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0669\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0704.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0672 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0721\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0719 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0700\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0700 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0711\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0711 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0701\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0701 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0684\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0683 - val_loss: 0.0685\n",
      "Epoch 171: Training Loss = 0.06883474439382553\n",
      "Epoch 172: Training Loss = 0.06900543719530106\n",
      "Epoch 173: Training Loss = 0.0678347647190094\n",
      "Epoch 174: Training Loss = 0.06933022290468216\n",
      "Epoch 175: Training Loss = 0.07041289657354355\n",
      "Epoch 176: Training Loss = 0.06858684122562408\n",
      "Epoch 177: Training Loss = 0.06980845332145691\n",
      "Epoch 178: Training Loss = 0.06850505620241165\n",
      "Epoch 179: Training Loss = 0.0686894953250885\n",
      "Epoch 180: Training Loss = 0.06675351411104202\n",
      "Starting training for epoch set: 181 to 190\n",
      "Loading model: model_epoch_10_loss_0.0668.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0708\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 0.0708 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0684\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0685 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0669\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0670 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0678\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0679 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0668\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0669 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0663\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0664 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0697\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0696 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0731\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0729 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0677\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0677 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0682\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0682 - val_loss: 0.0685\n",
      "Epoch 181: Training Loss = 0.07098089158535004\n",
      "Epoch 182: Training Loss = 0.06990278512239456\n",
      "Epoch 183: Training Loss = 0.06825170665979385\n",
      "Epoch 184: Training Loss = 0.06874486804008484\n",
      "Epoch 185: Training Loss = 0.06783101707696915\n",
      "Epoch 186: Training Loss = 0.0667729601264\n",
      "Epoch 187: Training Loss = 0.0671587735414505\n",
      "Epoch 188: Training Loss = 0.07031285017728806\n",
      "Epoch 189: Training Loss = 0.06972956657409668\n",
      "Epoch 190: Training Loss = 0.06689983606338501\n",
      "Starting training for epoch set: 191 to 200\n",
      "Loading model: model_epoch_10_loss_0.0669.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0696\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.0697 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0713\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0712 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0666\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0662.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0666 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0709\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0710 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0686\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0688 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0692\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0691 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0719\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0666.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0714 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0742\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0739 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0737\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0735 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0647\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0659.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0648 - val_loss: 0.0685\n",
      "Epoch 191: Training Loss = 0.06986980885267258\n",
      "Epoch 192: Training Loss = 0.06949862092733383\n",
      "Epoch 193: Training Loss = 0.06616920232772827\n",
      "Epoch 194: Training Loss = 0.07096479833126068\n",
      "Epoch 195: Training Loss = 0.07095403969287872\n",
      "Epoch 196: Training Loss = 0.06810686737298965\n",
      "Epoch 197: Training Loss = 0.06656360626220703\n",
      "Epoch 198: Training Loss = 0.06994754821062088\n",
      "Epoch 199: Training Loss = 0.07102314382791519\n",
      "Epoch 200: Training Loss = 0.06592260301113129\n",
      "Starting training for epoch set: 201 to 210\n",
      "Loading model: model_epoch_10_loss_0.0659.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0734\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0704.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 0.0731 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0641\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0653.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0641 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0679\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0679 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0712\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0706.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0712 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0685\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0685 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0669\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0669 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0659\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0662 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0682\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0682 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0673\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0672 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0707\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0715.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0708 - val_loss: 0.0685\n",
      "Epoch 201: Training Loss = 0.07043836265802383\n",
      "Epoch 202: Training Loss = 0.06525452435016632\n",
      "Epoch 203: Training Loss = 0.06882181018590927\n",
      "Epoch 204: Training Loss = 0.07056564837694168\n",
      "Epoch 205: Training Loss = 0.06859620660543442\n",
      "Epoch 206: Training Loss = 0.06800101697444916\n",
      "Epoch 207: Training Loss = 0.06885625422000885\n",
      "Epoch 208: Training Loss = 0.06892763078212738\n",
      "Epoch 209: Training Loss = 0.06700175255537033\n",
      "Epoch 210: Training Loss = 0.07154868543148041\n",
      "Starting training for epoch set: 211 to 220\n",
      "Loading model: model_epoch_10_loss_0.0715.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0675\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - loss: 0.0675 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0676\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0700.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0677 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0719\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0718 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0690\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0690 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0699\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0699 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0664\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0665 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0661\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0663 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0704\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0703 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0712\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0707 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0639\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0641 - val_loss: 0.0685\n",
      "Epoch 211: Training Loss = 0.06783965975046158\n",
      "Epoch 212: Training Loss = 0.07002067565917969\n",
      "Epoch 213: Training Loss = 0.06965050846338272\n",
      "Epoch 214: Training Loss = 0.06819064915180206\n",
      "Epoch 215: Training Loss = 0.06929471343755722\n",
      "Epoch 216: Training Loss = 0.06925689429044724\n",
      "Epoch 217: Training Loss = 0.0682302862405777\n",
      "Epoch 218: Training Loss = 0.06921473145484924\n",
      "Epoch 219: Training Loss = 0.06829966604709625\n",
      "Epoch 220: Training Loss = 0.06870990991592407\n",
      "Starting training for epoch set: 221 to 230\n",
      "Loading model: model_epoch_10_loss_0.0687.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0689\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.0689 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0688\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0721.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0688 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0714\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0711 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0660\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0661 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0688\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0687 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0677\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0678 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0692\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0692 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0701\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0700 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0719\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0707.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0718 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0694\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0693 - val_loss: 0.0685\n",
      "Epoch 221: Training Loss = 0.06825417280197144\n",
      "Epoch 222: Training Loss = 0.07206571847200394\n",
      "Epoch 223: Training Loss = 0.06850092858076096\n",
      "Epoch 224: Training Loss = 0.06891955435276031\n",
      "Epoch 225: Training Loss = 0.06688788533210754\n",
      "Epoch 226: Training Loss = 0.06895282119512558\n",
      "Epoch 227: Training Loss = 0.06954590231180191\n",
      "Epoch 228: Training Loss = 0.06822642683982849\n",
      "Epoch 229: Training Loss = 0.07066915184259415\n",
      "Epoch 230: Training Loss = 0.06850042194128036\n",
      "Starting training for epoch set: 231 to 240\n",
      "Loading model: model_epoch_10_loss_0.0685.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0708\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 40ms/step - loss: 0.0706 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0691\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0691 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0729\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0723 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0737\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0736 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0661\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0661 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0662\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0662 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0661\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0662 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0679\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0671.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0679 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0679\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0673.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0679 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0649\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0650 - val_loss: 0.0685\n",
      "Epoch 231: Training Loss = 0.06890340894460678\n",
      "Epoch 232: Training Loss = 0.06848270446062088\n",
      "Epoch 233: Training Loss = 0.06696588546037674\n",
      "Epoch 234: Training Loss = 0.06889769434928894\n",
      "Epoch 235: Training Loss = 0.06826020777225494\n",
      "Epoch 236: Training Loss = 0.06692860275506973\n",
      "Epoch 237: Training Loss = 0.06686480343341827\n",
      "Epoch 238: Training Loss = 0.06709866970777512\n",
      "Epoch 239: Training Loss = 0.06733261048793793\n",
      "Epoch 240: Training Loss = 0.06688077747821808\n",
      "Starting training for epoch set: 241 to 250\n",
      "Loading model: model_epoch_10_loss_0.0669.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0722\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - loss: 0.0719 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0668\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0668 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0721\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0718 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0679\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0704.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0681 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0754\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0732.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0753 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0694\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0693 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0671\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0672 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0713\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0712 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0661\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0664 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0644\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0645 - val_loss: 0.0685\n",
      "Epoch 241: Training Loss = 0.06946435570716858\n",
      "Epoch 242: Training Loss = 0.06724168360233307\n",
      "Epoch 243: Training Loss = 0.06828901916742325\n",
      "Epoch 244: Training Loss = 0.07041478902101517\n",
      "Epoch 245: Training Loss = 0.07324555516242981\n",
      "Epoch 246: Training Loss = 0.06816771626472473\n",
      "Epoch 247: Training Loss = 0.06807231903076172\n",
      "Epoch 248: Training Loss = 0.06967945396900177\n",
      "Epoch 249: Training Loss = 0.06929805129766464\n",
      "Epoch 250: Training Loss = 0.066792793571949\n",
      "Starting training for epoch set: 251 to 260\n",
      "Loading model: model_epoch_10_loss_0.0668.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0662\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.0662 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0694\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0694 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0692\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0692 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0674\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0679.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.0674 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0709\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0702.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0709 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0674\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0675 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0674\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0675 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0730\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0727 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0678\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0680 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0689\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0717.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0691 - val_loss: 0.0685\n",
      "Epoch 251: Training Loss = 0.06679659336805344\n",
      "Epoch 252: Training Loss = 0.06825920194387436\n",
      "Epoch 253: Training Loss = 0.06878460198640823\n",
      "Epoch 254: Training Loss = 0.06786471605300903\n",
      "Epoch 255: Training Loss = 0.07016058266162872\n",
      "Epoch 256: Training Loss = 0.0689423605799675\n",
      "Epoch 257: Training Loss = 0.0690898597240448\n",
      "Epoch 258: Training Loss = 0.06863104552030563\n",
      "Epoch 259: Training Loss = 0.06948227435350418\n",
      "Epoch 260: Training Loss = 0.07167896628379822\n",
      "Starting training for epoch set: 261 to 270\n",
      "Loading model: model_epoch_10_loss_0.0717.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0653\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 0.0654 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0657\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0659 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0652\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0666.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0653 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0671\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0671 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0680\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0680 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0692\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0694.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0692 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0713\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0712 - val_loss: 0.0685\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0693\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0693 - val_loss: 0.0685\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0669\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0670 - val_loss: 0.0685\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0687\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0688 - val_loss: 0.0685\n",
      "Epoch 261: Training Loss = 0.06681733578443527\n",
      "Epoch 262: Training Loss = 0.06853705644607544\n",
      "Epoch 263: Training Loss = 0.06658657640218735\n",
      "Epoch 264: Training Loss = 0.06899844855070114\n",
      "Epoch 265: Training Loss = 0.06815603375434875\n",
      "Epoch 266: Training Loss = 0.06936144828796387\n",
      "Epoch 267: Training Loss = 0.06979262083768845\n",
      "Epoch 268: Training Loss = 0.06862036138772964\n",
      "Epoch 269: Training Loss = 0.06804341077804565\n",
      "Epoch 270: Training Loss = 0.06897663325071335\n",
      "Starting training for epoch set: 271 to 280\n",
      "Loading model: model_epoch_10_loss_0.0690.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0648\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - loss: 0.0649 - val_loss: 0.0685\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0682\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0711.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0685 - val_loss: 0.0685\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0658\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0709.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0664 - val_loss: 0.0685\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0711\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0663.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0705 - val_loss: 0.0685\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0701\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0701 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0706\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0706 - val_loss: 0.0685\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0712\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0696.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0711 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0637\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0639 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0683\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0683 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0689\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0689 - val_loss: 0.0684\n",
      "Epoch 271: Training Loss = 0.06753972917795181\n",
      "Epoch 272: Training Loss = 0.07110890746116638\n",
      "Epoch 273: Training Loss = 0.07088559120893478\n",
      "Epoch 274: Training Loss = 0.06630607694387436\n",
      "Epoch 275: Training Loss = 0.06909573823213577\n",
      "Epoch 276: Training Loss = 0.06976494938135147\n",
      "Epoch 277: Training Loss = 0.06961977481842041\n",
      "Epoch 278: Training Loss = 0.06806749850511551\n",
      "Epoch 279: Training Loss = 0.06756318360567093\n",
      "Epoch 280: Training Loss = 0.0677843913435936\n",
      "Starting training for epoch set: 281 to 290\n",
      "Loading model: model_epoch_10_loss_0.0678.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0693\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0691 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0689\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0689 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0685\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0684 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0710\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0707 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0699\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0697 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0635\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0637 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0687\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0685 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0712\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0711 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0722\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0716 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0640\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0645 - val_loss: 0.0684\n",
      "Epoch 281: Training Loss = 0.06715985387563705\n",
      "Epoch 282: Training Loss = 0.06699878722429276\n",
      "Epoch 283: Training Loss = 0.0669267326593399\n",
      "Epoch 284: Training Loss = 0.06850956380367279\n",
      "Epoch 285: Training Loss = 0.06815747171640396\n",
      "Epoch 286: Training Loss = 0.067373126745224\n",
      "Epoch 287: Training Loss = 0.06719937175512314\n",
      "Epoch 288: Training Loss = 0.06910091638565063\n",
      "Epoch 289: Training Loss = 0.06680642068386078\n",
      "Epoch 290: Training Loss = 0.06802428513765335\n",
      "Starting training for epoch set: 291 to 300\n",
      "Loading model: model_epoch_10_loss_0.0680.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0724\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0716 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0682\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0681 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0662\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0663 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0658\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0659 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0719\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0716 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0676\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0676 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0682\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0682 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0717\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0712 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0661\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0663 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0669\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0669 - val_loss: 0.0684\n",
      "Epoch 291: Training Loss = 0.06680695712566376\n",
      "Epoch 292: Training Loss = 0.06770637631416321\n",
      "Epoch 293: Training Loss = 0.0692184641957283\n",
      "Epoch 294: Training Loss = 0.06765154749155045\n",
      "Epoch 295: Training Loss = 0.06840099394321442\n",
      "Epoch 296: Training Loss = 0.06798877567052841\n",
      "Epoch 297: Training Loss = 0.069216288626194\n",
      "Epoch 298: Training Loss = 0.06902336329221725\n",
      "Epoch 299: Training Loss = 0.0683489590883255\n",
      "Epoch 300: Training Loss = 0.06702986359596252\n",
      "Starting training for epoch set: 301 to 310\n",
      "Loading model: model_epoch_10_loss_0.0670.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0705\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.0702 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0685\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0685 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0647\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0656.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0648 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0673\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0692.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0675 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0615\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0623 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0657\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0654.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0657 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0686\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0687 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0703\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0702 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0712\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0716.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0713 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0729\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 0.0726 - val_loss: 0.0684\n",
      "Epoch 301: Training Loss = 0.06823926419019699\n",
      "Epoch 302: Training Loss = 0.06889128684997559\n",
      "Epoch 303: Training Loss = 0.06559910625219345\n",
      "Epoch 304: Training Loss = 0.0691700205206871\n",
      "Epoch 305: Training Loss = 0.06721969693899155\n",
      "Epoch 306: Training Loss = 0.06536716967821121\n",
      "Epoch 307: Training Loss = 0.06949180364608765\n",
      "Epoch 308: Training Loss = 0.06850889325141907\n",
      "Epoch 309: Training Loss = 0.07158316671848297\n",
      "Epoch 310: Training Loss = 0.0676570013165474\n",
      "Starting training for epoch set: 311 to 320\n",
      "Loading model: model_epoch_10_loss_0.0677.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0678\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0678 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0705\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0703 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0655\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0665.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0655 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0705\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0704 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0691\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0703.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0691 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0698\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0699.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0698 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0664\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0665 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0682\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0682 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0626\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0651.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0628 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0661\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0661 - val_loss: 0.0684\n",
      "Epoch 311: Training Loss = 0.06761251389980316\n",
      "Epoch 312: Training Loss = 0.06895671039819717\n",
      "Epoch 313: Training Loss = 0.06647203862667084\n",
      "Epoch 314: Training Loss = 0.06812387704849243\n",
      "Epoch 315: Training Loss = 0.07033973187208176\n",
      "Epoch 316: Training Loss = 0.06985367834568024\n",
      "Epoch 317: Training Loss = 0.06741560995578766\n",
      "Epoch 318: Training Loss = 0.06837356090545654\n",
      "Epoch 319: Training Loss = 0.06506247818470001\n",
      "Epoch 320: Training Loss = 0.06744356453418732\n",
      "Starting training for epoch set: 321 to 330\n",
      "Loading model: model_epoch_10_loss_0.0674.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0720\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0717 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0689\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0688 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0712\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0711 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0684\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0684 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0694\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0694 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0690\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0689 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0709\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0694.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0708 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0696\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0695 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0674\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0674 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0673\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0674 - val_loss: 0.0684\n",
      "Epoch 321: Training Loss = 0.06911372393369675\n",
      "Epoch 322: Training Loss = 0.06699643284082413\n",
      "Epoch 323: Training Loss = 0.0683174580335617\n",
      "Epoch 324: Training Loss = 0.06852146238088608\n",
      "Epoch 325: Training Loss = 0.06968513131141663\n",
      "Epoch 326: Training Loss = 0.06756933778524399\n",
      "Epoch 327: Training Loss = 0.06943817436695099\n",
      "Epoch 328: Training Loss = 0.06770884245634079\n",
      "Epoch 329: Training Loss = 0.06876514852046967\n",
      "Epoch 330: Training Loss = 0.068128801882267\n",
      "Starting training for epoch set: 331 to 340\n",
      "Loading model: model_epoch_10_loss_0.0681.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0652\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 0.0653 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0664\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0664 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0669\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0661.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0669 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0686\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0686 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0718\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0663.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0712 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0723\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0722 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0676\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0676 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0667\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0665.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0666 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0691\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0690 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0697\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0697 - val_loss: 0.0684\n",
      "Epoch 331: Training Loss = 0.06796732544898987\n",
      "Epoch 332: Training Loss = 0.06871278584003448\n",
      "Epoch 333: Training Loss = 0.0661100447177887\n",
      "Epoch 334: Training Loss = 0.0680854544043541\n",
      "Epoch 335: Training Loss = 0.06634560972452164\n",
      "Epoch 336: Training Loss = 0.0688394084572792\n",
      "Epoch 337: Training Loss = 0.06768002361059189\n",
      "Epoch 338: Training Loss = 0.06646067649126053\n",
      "Epoch 339: Training Loss = 0.06738162040710449\n",
      "Epoch 340: Training Loss = 0.06767649203538895\n",
      "Starting training for epoch set: 341 to 350\n",
      "Loading model: model_epoch_10_loss_0.0677.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0752\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 39ms/step - loss: 0.0750 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0664\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0665 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0645\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0667.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0647 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0684\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0683 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0672\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0679.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0673 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0688\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0671.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0686 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0621\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0658.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0622 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0672\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0672 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0695\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0695 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0651\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0652 - val_loss: 0.0684\n",
      "Epoch 341: Training Loss = 0.06836853176355362\n",
      "Epoch 342: Training Loss = 0.06724667549133301\n",
      "Epoch 343: Training Loss = 0.06666910648345947\n",
      "Epoch 344: Training Loss = 0.06806695461273193\n",
      "Epoch 345: Training Loss = 0.06786374002695084\n",
      "Epoch 346: Training Loss = 0.06709150224924088\n",
      "Epoch 347: Training Loss = 0.06577116250991821\n",
      "Epoch 348: Training Loss = 0.06755238026380539\n",
      "Epoch 349: Training Loss = 0.06850985437631607\n",
      "Epoch 350: Training Loss = 0.06876029074192047\n",
      "Starting training for epoch set: 351 to 360\n",
      "Loading model: model_epoch_10_loss_0.0688.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0730\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0693.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - loss: 0.0729 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0688\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0688 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0661\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0662 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0715\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0716.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0715 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0694\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0694 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0680\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0697.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0682 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0692\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0692 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0707\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0705 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0657\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0657 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0662\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0663 - val_loss: 0.0684\n",
      "Epoch 351: Training Loss = 0.06926342844963074\n",
      "Epoch 352: Training Loss = 0.06969001144170761\n",
      "Epoch 353: Training Loss = 0.06752917170524597\n",
      "Epoch 354: Training Loss = 0.07155229151248932\n",
      "Epoch 355: Training Loss = 0.0685805007815361\n",
      "Epoch 356: Training Loss = 0.06970404088497162\n",
      "Epoch 357: Training Loss = 0.06880320608615875\n",
      "Epoch 358: Training Loss = 0.06901539862155914\n",
      "Epoch 359: Training Loss = 0.06678695231676102\n",
      "Epoch 360: Training Loss = 0.06868446618318558\n",
      "Starting training for epoch set: 361 to 370\n",
      "Loading model: model_epoch_10_loss_0.0687.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0644\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 45ms/step - loss: 0.0645 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0666\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0695.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0668 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0646\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0664.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0647 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0695\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0695 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0669\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0670 - val_loss: 0.0684\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0678\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0678 - val_loss: 0.0684\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0679\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0679 - val_loss: 0.0684\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0743\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0740 - val_loss: 0.0684\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0672\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0673.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0672 - val_loss: 0.0684\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0668\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0669 - val_loss: 0.0684\n",
      "Epoch 361: Training Loss = 0.06877556443214417\n",
      "Epoch 362: Training Loss = 0.06945043802261353\n",
      "Epoch 363: Training Loss = 0.06640142947435379\n",
      "Epoch 364: Training Loss = 0.06842603534460068\n",
      "Epoch 365: Training Loss = 0.06817689538002014\n",
      "Epoch 366: Training Loss = 0.06762436032295227\n",
      "Epoch 367: Training Loss = 0.06718342751264572\n",
      "Epoch 368: Training Loss = 0.06978805363178253\n",
      "Epoch 369: Training Loss = 0.06732235848903656\n",
      "Epoch 370: Training Loss = 0.06818681210279465\n",
      "Starting training for epoch set: 371 to 380\n",
      "Loading model: model_epoch_10_loss_0.0682.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0739\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0708.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0737 - val_loss: 0.0684\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0693\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0692 - val_loss: 0.0684\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0662\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0664 - val_loss: 0.0684\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0630\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0658.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0630 - val_loss: 0.0684\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0669\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0669 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0696\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0695 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0609\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0667.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0615 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0663\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0664 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0689\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0687 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0699\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0697 - val_loss: 0.0683\n",
      "Epoch 371: Training Loss = 0.0708402618765831\n",
      "Epoch 372: Training Loss = 0.06683742254972458\n",
      "Epoch 373: Training Loss = 0.06872973591089249\n",
      "Epoch 374: Training Loss = 0.06577878445386887\n",
      "Epoch 375: Training Loss = 0.0681462213397026\n",
      "Epoch 376: Training Loss = 0.0680181235074997\n",
      "Epoch 377: Training Loss = 0.06670591980218887\n",
      "Epoch 378: Training Loss = 0.06882617622613907\n",
      "Epoch 379: Training Loss = 0.06745576113462448\n",
      "Epoch 380: Training Loss = 0.06751301884651184\n",
      "Starting training for epoch set: 381 to 390\n",
      "Loading model: model_epoch_10_loss_0.0675.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0694\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - loss: 0.0693 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0657\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0659 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0651\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0653 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0668\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0671.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0668 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0736\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0729 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0663\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0689.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0664 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0692\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0690 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0689\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0652.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0686 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0677\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0676 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0667\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0687.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0667 - val_loss: 0.0683\n",
      "Epoch 381: Training Loss = 0.06835656613111496\n",
      "Epoch 382: Training Loss = 0.06804517656564713\n",
      "Epoch 383: Training Loss = 0.06772524863481522\n",
      "Epoch 384: Training Loss = 0.06706354022026062\n",
      "Epoch 385: Training Loss = 0.0669044777750969\n",
      "Epoch 386: Training Loss = 0.06894980370998383\n",
      "Epoch 387: Training Loss = 0.06752938777208328\n",
      "Epoch 388: Training Loss = 0.06521302461624146\n",
      "Epoch 389: Training Loss = 0.06687743961811066\n",
      "Epoch 390: Training Loss = 0.06865666061639786\n",
      "Starting training for epoch set: 391 to 400\n",
      "Loading model: model_epoch_10_loss_0.0687.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0704\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0702 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0719\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0716 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0665\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0665 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0726\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0725 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0687\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0687 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0689\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0688 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0687\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0687 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0709\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0706 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0628\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0634 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0689\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0688 - val_loss: 0.0683\n",
      "Epoch 391: Training Loss = 0.06813805550336838\n",
      "Epoch 392: Training Loss = 0.06859235465526581\n",
      "Epoch 393: Training Loss = 0.0685383751988411\n",
      "Epoch 394: Training Loss = 0.06812422722578049\n",
      "Epoch 395: Training Loss = 0.06804243475198746\n",
      "Epoch 396: Training Loss = 0.06723024696111679\n",
      "Epoch 397: Training Loss = 0.06682965159416199\n",
      "Epoch 398: Training Loss = 0.067847341299057\n",
      "Epoch 399: Training Loss = 0.0687573179602623\n",
      "Epoch 400: Training Loss = 0.06756532192230225\n",
      "Starting training for epoch set: 401 to 410\n",
      "Loading model: model_epoch_10_loss_0.0676.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0649\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0650 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0711\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0710.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0711 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0715\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0664.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0711 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0679\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0680 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0624\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0651.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0627 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0643\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0645 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0708\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0682.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0706 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0663\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0660.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0663 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0691\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0661.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0690 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0713\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0711 - val_loss: 0.0683\n",
      "Epoch 401: Training Loss = 0.06754538416862488\n",
      "Epoch 402: Training Loss = 0.07102452963590622\n",
      "Epoch 403: Training Loss = 0.06640387326478958\n",
      "Epoch 404: Training Loss = 0.068574920296669\n",
      "Epoch 405: Training Loss = 0.06509511917829514\n",
      "Epoch 406: Training Loss = 0.06809813529253006\n",
      "Epoch 407: Training Loss = 0.06820546835660934\n",
      "Epoch 408: Training Loss = 0.06596308946609497\n",
      "Epoch 409: Training Loss = 0.06607998162508011\n",
      "Epoch 410: Training Loss = 0.06679785251617432\n",
      "Starting training for epoch set: 411 to 420\n",
      "Loading model: model_epoch_10_loss_0.0668.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0709\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0706 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0705\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0685.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0703 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0692\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0671.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0691 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0685\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0683 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0706\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0703 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0716\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0713 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0728\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0723 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0669\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0669 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0697\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0686.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0696 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0687\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0687 - val_loss: 0.0683\n",
      "Epoch 411: Training Loss = 0.06839495897293091\n",
      "Epoch 412: Training Loss = 0.0684942975640297\n",
      "Epoch 413: Training Loss = 0.06712662428617477\n",
      "Epoch 414: Training Loss = 0.06692502647638321\n",
      "Epoch 415: Training Loss = 0.06704205274581909\n",
      "Epoch 416: Training Loss = 0.06772289425134659\n",
      "Epoch 417: Training Loss = 0.06796732544898987\n",
      "Epoch 418: Training Loss = 0.06855276226997375\n",
      "Epoch 419: Training Loss = 0.06862267106771469\n",
      "Epoch 420: Training Loss = 0.0680832639336586\n",
      "Starting training for epoch set: 421 to 430\n",
      "Loading model: model_epoch_10_loss_0.0681.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0679\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - loss: 0.0678 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0683\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0683 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0657\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0659 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0663\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0654.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0662 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0675\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0669.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0674 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0675\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0675 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0705\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0671.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0702 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0637\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0639 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0678\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0677 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0714\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0710 - val_loss: 0.0683\n",
      "Epoch 421: Training Loss = 0.06702448427677155\n",
      "Epoch 422: Training Loss = 0.06833593547344208\n",
      "Epoch 423: Training Loss = 0.06906863301992416\n",
      "Epoch 424: Training Loss = 0.06536512821912766\n",
      "Epoch 425: Training Loss = 0.06692194938659668\n",
      "Epoch 426: Training Loss = 0.06716448068618774\n",
      "Epoch 427: Training Loss = 0.06711471825838089\n",
      "Epoch 428: Training Loss = 0.06743471324443817\n",
      "Epoch 429: Training Loss = 0.06679020076990128\n",
      "Epoch 430: Training Loss = 0.06780049949884415\n",
      "Starting training for epoch set: 431 to 440\n",
      "Loading model: model_epoch_10_loss_0.0678.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0657\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0670.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0658 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0675\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0676 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0709\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0707 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0693\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0677.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0691 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0689\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0660.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0685 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0677\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0679.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0677 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0682\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0680.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0682 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0661\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0698.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0662 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0728\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0727 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0678\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0678.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.0678 - val_loss: 0.0683\n",
      "Epoch 431: Training Loss = 0.06697515398263931\n",
      "Epoch 432: Training Loss = 0.0682503953576088\n",
      "Epoch 433: Training Loss = 0.06842829287052155\n",
      "Epoch 434: Training Loss = 0.06765259057283401\n",
      "Epoch 435: Training Loss = 0.06599017232656479\n",
      "Epoch 436: Training Loss = 0.0679435282945633\n",
      "Epoch 437: Training Loss = 0.06804723292589188\n",
      "Epoch 438: Training Loss = 0.06982980668544769\n",
      "Epoch 439: Training Loss = 0.0682750791311264\n",
      "Epoch 440: Training Loss = 0.06780461966991425\n",
      "Starting training for epoch set: 441 to 450\n",
      "Loading model: model_epoch_10_loss_0.0678.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0731\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0717.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 0.0730 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m32/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0705\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0703 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m33/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0658\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0675.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0660 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0644\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0674.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0647 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0684\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0672.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0683 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0654\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0681.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - loss: 0.0655 - val_loss: 0.0683\n",
      "Epoch 7/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0702\n",
      "Epoch 7: saving model to saved_models\\model_epoch_07_loss_0.0700.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0702 - val_loss: 0.0683\n",
      "Epoch 8/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0648\n",
      "Epoch 8: saving model to saved_models\\model_epoch_08_loss_0.0668.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0650 - val_loss: 0.0683\n",
      "Epoch 9/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0639\n",
      "Epoch 9: saving model to saved_models\\model_epoch_09_loss_0.0683.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0641 - val_loss: 0.0683\n",
      "Epoch 10/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0679\n",
      "Epoch 10: saving model to saved_models\\model_epoch_10_loss_0.0684.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0679 - val_loss: 0.0683\n",
      "Epoch 441: Training Loss = 0.07174602895975113\n",
      "Epoch 442: Training Loss = 0.06911256164312363\n",
      "Epoch 443: Training Loss = 0.06751649081707001\n",
      "Epoch 444: Training Loss = 0.0674169585108757\n",
      "Epoch 445: Training Loss = 0.06716848164796829\n",
      "Epoch 446: Training Loss = 0.06814634799957275\n",
      "Epoch 447: Training Loss = 0.07002419233322144\n",
      "Epoch 448: Training Loss = 0.06677515804767609\n",
      "Epoch 449: Training Loss = 0.06828876584768295\n",
      "Epoch 450: Training Loss = 0.06839855015277863\n",
      "Starting training for epoch set: 451 to 460\n",
      "Loading model: model_epoch_10_loss_0.0684.keras\n",
      "Epoch 1/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0689\n",
      "Epoch 1: saving model to saved_models\\model_epoch_01_loss_0.0676.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - loss: 0.0688 - val_loss: 0.0683\n",
      "Epoch 2/10\n",
      "\u001b[1m35/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0697\n",
      "Epoch 2: saving model to saved_models\\model_epoch_02_loss_0.0690.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0697 - val_loss: 0.0683\n",
      "Epoch 3/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0704\n",
      "Epoch 3: saving model to saved_models\\model_epoch_03_loss_0.0696.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0703 - val_loss: 0.0683\n",
      "Epoch 4/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0704\n",
      "Epoch 4: saving model to saved_models\\model_epoch_04_loss_0.0688.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0703 - val_loss: 0.0683\n",
      "Epoch 5/10\n",
      "\u001b[1m36/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0681\n",
      "Epoch 5: saving model to saved_models\\model_epoch_05_loss_0.0691.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0681 - val_loss: 0.0683\n",
      "Epoch 6/10\n",
      "\u001b[1m34/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0640\n",
      "Epoch 6: saving model to saved_models\\model_epoch_06_loss_0.0666.keras\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0643 - val_loss: 0.0683\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdadelta\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model for a set number of epochs (e.g., 10)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Print the loss at the end of each epoch set\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch, loss \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\.conda\\envs\\time_series_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_count = 0\n",
    "while True:\n",
    "    print(f\"Starting training for epoch set: {epoch_count + 1} to {epoch_count + 10}\")\n",
    "    model = load_latest_checkpoint(save_dir)\n",
    "    if model is None:\n",
    "    # Build a new model if no checkpoint is found\n",
    "        model = build_model(optimizer='Adadelta')\n",
    "    # Train the model for a set number of epochs (e.g., 10)\n",
    "    history = model.fit(\n",
    "        trainX, trainY, \n",
    "        batch_size=16, \n",
    "        epochs=10, \n",
    "        verbose=1, \n",
    "        validation_data=(validX, validY),\n",
    "        callbacks=[checkpoint]\n",
    "    )\n",
    "\n",
    "    # Print the loss at the end of each epoch set\n",
    "    for epoch, loss in enumerate(history.history['loss'], start=1):\n",
    "        print(f\"Epoch {epoch_count + epoch}: Training Loss = {loss}\")\n",
    "\n",
    "    epoch_count += 10\n",
    "\n",
    "    # Keep only the 5 most recent model files\n",
    "    model_files = sorted(os.listdir(save_dir), key=lambda x: os.path.getmtime(os.path.join(save_dir, x)))\n",
    "    if len(model_files) > 5:\n",
    "        for old_file in model_files[:-5]:\n",
    "            os.remove(os.path.join(save_dir, old_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: model_epoch_06_loss_0.0666.keras\n"
     ]
    }
   ],
   "source": [
    "model_latest = load_latest_checkpoint(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_latest = model_latest.predict(validX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock_prediction(y_pred=y_pred_latest, validY=validY, scaler=scaler, numerical_cols=numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (time_series_env)",
   "language": "python",
   "name": "time_series_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
